# -*- coding: utf-8 -*-

import sys
import uuid

from fastapi import FastAPI, Request, Query
from fastapi.responses import JSONResponse, StreamingResponse

import aws_bedrock_runtime_mate.api as aws_bedrock_runtime_mate
from vercel_ai_sdk_mate.api import RequestBody

from sanhe_me.paths import path_enum
from sanhe_me.vendor.ai_sdk_adapter import (
    request_body_to_bedrock_converse_messages,
)
from sanhe_me.one.api import one

app = FastAPI()


def debug(s: str):
    """Print debug info to stdout (shows as [info] in Vercel logs)"""
    print(s, file=sys.stdout)


@app.get("/api/hello")
async def hello_world():
    """
    Hello World API endpoint - ç”¨äºæµ‹è¯• FastAPI é›†æˆ
    """
    return JSONResponse(
        content={
            "message": "Hello from FastAPI!",
            "status": "success"
        }
    )


@app.post("/api/chat")
async def handle_chat_data(request: Request, protocol: str = Query("data")):
    """
    AI SDK v5 ä½¿ç”¨ SSE (Server-Sent Events) æ ¼å¼çš„ Data Stream Protocol
    æ–‡æ¡£: https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol
    """
    import json
    import sys

    # è°ƒè¯•ï¼šæ‰“å°åŸå§‹è¯·æ±‚
    debug("====== Incoming request")
    debug("------ Request Headers")
    for key, value in request.headers.items():
        debug(f"{key}: {value}")
    debug("------ Request Body")
    request_body_data = await request.json()
    request_body_formatted = json.dumps(request_body_data, indent=2, ensure_ascii=False)
    debug(request_body_formatted)

    sys.stderr.flush()

    # è§£ææ¶ˆæ¯
    request_body = RequestBody(**request_body_data)

    default_converse_kwargs = aws_bedrock_runtime_mate.ConverseKwargs(
        # ä½¿ç”¨è·¨åŒºåŸŸ inference profileï¼Œè‡ªåŠ¨åˆ†å‘è¯·æ±‚åˆ°å¤šä¸ªåŒºåŸŸï¼Œæé«˜ååé‡
        # model_id="us.amazon.nova-micro-v1:0",
        # model_id="us.amazon.nova-lite-v1:0",
        model_id="us.amazon.nova-pro-v1:0",
        system=[
            {"text": path_enum.instruction_content},
            {"text": path_enum.knowledge_base_content},
            {"cachePoint": {"type": "default"}},
        ],
    )  # we can reuse this later

    chat_session = aws_bedrock_runtime_mate.patterns.ChatSession(
        client=one.bsm.bedrockruntime_client,
        converse_kwargs=default_converse_kwargs,
        verbose=False,
    )

    # chat_session._messages = [
    #     {
    #         "role": "user",
    #         "content": [
    #             {"text": path_enum.knowledge_base_content},
    #             {
    #                 "cachePoint": {"type": "default"},
    #             },
    #         ],
    #     },
    #     {
    #         "role": "assistant",
    #         "content": [
    #             {"text": "Iâ€™ve reviewed the knowledge base and Iâ€™m ready to answer questions based on it."},
    #         ],
    #     },
    # ]
    messages = request_body_to_bedrock_converse_messages(request_body)

    debug("------ Chat request")
    for message in messages:
        debug(message)

    # è°ƒç”¨ Bedrock å¤„ç†
    converse_stream_response = chat_session.converse_stream(messages)

    # AI SDK v5 ä½¿ç”¨ SSE æ ¼å¼ï¼Œæ¯è¡Œä»¥ "data: " å¼€å¤´
    # æ–‡æœ¬ä½¿ç”¨ start/delta/end ä¸‰é˜¶æ®µæ¨¡å¼
    def ai_sdk_v5_message_generator():
        id = str(uuid.uuid4())
        response_text_chunks = []
        usage = None

        # æ–‡æœ¬å¼€å§‹
        yield f'data: {json.dumps({"type": "text-start", "id": id})}\n\n'

        # æ–‡æœ¬å†…å®¹ï¼ˆå¯ä»¥åˆ†å¤šæ¬¡å‘é€ï¼‰
        for event in chat_session.iterate_events(converse_stream_response):
            debug(str(event))
            if event.is_messageStart():
                debug("ğŸš€ Message starting...")
            elif event.is_contentBlockDelta():
                if event.text:
                    response_text_chunks.append(event.text)
                    yield f'data: {json.dumps({"type": "text-delta", "id": id, "delta": event.text})}\n\n'
            elif event.is_messageStop():
                debug(f"\nâœ… Done! Stop reason: {event.messageStop.stopReason}")
            elif "metadata" in event.boto3_raw_data:
                usage = event.metadata.usage.boto3_raw_data

        debug("------ Chat response")
        output_text = "".join(response_text_chunks)
        debug(output_text)

        debug("------ Token Usage")
        debug(str(usage))

        sys.stderr.flush()

        # æ–‡æœ¬ç»“æŸ
        yield f'data: {json.dumps({"type": "text-end", "id": id})}\n\n'
        # æ¶ˆæ¯å®Œæˆæ ‡è®°
        yield f'data: {json.dumps({"type": "finish-message", "finishReason": "stop"})}\n\n'
        # SSE ç»“æŸæ ‡è®°
        yield "data: [DONE]\n\n"

    # StreamingResponse è¿”å› SSE æ ¼å¼çš„æµ
    # v5 ä½¿ç”¨ x-vercel-ai-ui-message-stream å¤´è€Œä¸æ˜¯ x-vercel-ai-data-stream
    response = StreamingResponse(
        ai_sdk_v5_message_generator(),
        media_type="text/event-stream",  # SSE çš„ MIME ç±»å‹
    )
    response.headers["x-vercel-ai-ui-message-stream"] = "v1"
    response.headers["Cache-Control"] = "no-cache"
    response.headers["Connection"] = "keep-alive"
    return response
