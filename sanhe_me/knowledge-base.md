This is the knowledge base:
<knowledge-base>
<document>
  <title>Sanhe Hu Resume - AI / Data Architect</title>
  <markdown_content>
# Sanhe Hu

## Senior AI Architect

Rockville, MD, USA, (202) 738-0085, [husanhe@gmail.com](mailto:husanhe@gmail.com)  
Professional Portfolio: [https://www.sanhe.me/](https://www.sanhe.me/)  
Chat with Sanhe’s AI Assistant to learn about his unique expertise and value: [https://www.sanhe.me/chat](https://www.sanhe.me/chat)

# **Summary**

**AI Expertise**: Enterprise AI architect specializing in Multi-Agent systems, RAG infrastructure, AI workflow orchestration, and observability platforms. Hands-on technical leader with proven track record of designing and implementing production-ready AI solutions—from strategic architecture through code-level implementation—while maintaining deep involvement in emerging AI technologies and rapid framework evolution.

**Entrepreneurship**: Founded and scaled boutique IT consulting firm (BunnymanTech) serving enterprise clients, delivering rapid time-to-value AI transformation solutions including multi-agent platforms, knowledge integration systems, and strategic consulting frameworks that compress decision cycles from months to weeks.

**Technical Depth:** Subject matter expert in AWS AI services (Bedrock, AgentCore, SageMaker) with particular depth in enterprise LLM applications, vector databases, agent orchestration frameworks, and Model Context Protocol (MCP) infrastructure. Active open-source contributor with [150+ solutions](https://pypi.org/user/machugwu/) reaching [10M monthly downloads](https://github.com/MacHu-GWU), including early MCP implementations 6 months ahead of official vendor releases.

**Leadership**: Demonstrated ability to architect enterprise-grade AI systems while managing comprehensive business operations—from technical delivery and stakeholder management to customer relationships and business development. Proven track record of staying ahead of rapidly evolving AI landscape through forward-thinking architectural decisions and rapid iteration cycles.

# **Professional Experiences**

## **BunnymanTech \- Product, Architect and Engineer**

Jan 2022 \- PRESENT, Rockville MD

**AI Agent Project @ Travelers** (BunnymanTech Client) *\- Enterprise AI Architect*

* **Led 2-year strategic evolution of enterprise Multi-Agent architecture** across four technology phases (AI Workflow → AI Agent → Multi Agents → Agent Platform), establishing platform-agnostic three-layer abstraction enabling seamless framework migrations without business logic rewrites. Maintained competitive advantage through forward-thinking architectural decisions and rapid iteration cycles in the rapidly evolving AI landscape where frameworks progressed from basic workflow automation to sophisticated autonomous systems.  
* **Phase 1 \- Workflow Orchestration**: Pioneered AI workflow infrastructure on Temporal Cloud using Command Pattern tool encapsulation, migrated legacy Marvin engine to modern orchestration, published platform-agnostic framework design patterns enabling rapid adoption across any AI orchestration tools with shared tool libraries.  
* **Phase 2 \- Autonomous Agents**: Transformed Lambda tools into Bedrock Agent Action Groups establishing Agent-as-tool model, built Agent Catalog for discovery/reuse, developed open-source "Open API Adapter for Bedrock Agent" enabling single deployment to serve Bedrock, ChatGPT, MCP, and REST APIs solving vendor lock-in   
* **Phase 3 \- Multi-Agent Collaboration**: Deployed Strand Agents framework on AWS EKS supporting six coordination patterns (Agent-as-tool, Swarm, Graph, Workflow), integrated Model Context Protocol (MCP) standardization, created Multi-Agent Design White Paper with reference implementations for all patterns published as open-source templates for Kubernetes-based agent systems  
* **Phase 4 \- Enterprise Platform**: Achieved production maturity through AWS AgentCore early access, integrated native observability/memory/prompt management/guardrails, transitioned to serverless reducing infrastructure costs 60%, established knowledge base integration patterns, published migration toolkit and enterprise architecture reference patterns while influencing AWS product roadmap through strategic feedback

**Enterprise MCP Infrastructure Project @ Travelers** (BunnymanTech Client) *\- Enterprise AI Architect*

* Built enterprise-grade Model Context Protocol infrastructure enabling secure AI integration across 20+ applications processing 500-600M monthly inference requests, reducing development cycles from 1 month to 1 day through standardized framework and automated deployment  
* Designed three-layer authentication architecture integrating Okta SSO and OAuth 2.0 with Atlassian JIRA and Confluence, implementing fine-grained permission controls for JIRA/Confluence access while maintaining compliance with enterprise security requirements  
* Developed open-source MCP servers for Atlassian (JIRA/Confluence), Google Drive/Gmail, establishing company as early adopter 6 months ahead of official vendor releases  
* Developed open-source Text to SQL MCP server ([LINK](https://github.com/MacHu-GWU/mcp_ohmy_sql-project)) supporting 8+ database engines (PostgreSQL, MySQL, SQLite, Oracle, SQL Server, AWS Redshift, AWS Athena, DynamoDB, MongoDB, ElasticSearch) with intelligent Text-to-SQL translation, query result caching, and transaction management—enabling AI agents to interact with enterprise databases through natural language while maintaining security and performance controls  
* Created MCP Registry platform and development framework using Python/FastAPI, standardizing discovery, access control, and deployment across Kubernetes infrastructure, reducing duplicate database access code by \~10,000 lines  
* Published Atlassian MCP Server ([LINK](https://sanhehu.atlassian.net/wiki/external/M2VhM2JhMjBkMGRiNGI4NDg0YzZjYjQxZGZmM2NmYjg)) and AI-powered JIRA visualization tools ([LINK](https://github.com/easyscalecloud/esc_ai_gallery-project/blob/main/docs/source/01-AI-JIRA-Visualizer-Automated-Ticket-Relationship-Diagrams/index.rst)), contributing to enterprise AI ecosystem

**Enterprise Knowledge Integration (RAG) Project @ Travelers** (BunnymanTech Client) *\- Enterprise AI Architect*

* Built RAG-based knowledge system aggregating 1000+ Confluence spaces, 3000+ GitHub repositories, and SharePoint documents, deployed 100+ knowledge bases processing 20K daily AI queries within first month  
* Designed two-tier retrieval architecture using pgvector with Contextual Retrieval, implementing multi-stage LLM processing (Haiku for filtering, Sonnet for reasoning) optimizing cost while maintaining quality  
* Created atlassian-doc-parser ([LINK](https://github.com/MacHu-GWU/atlas_doc_parser-project)) and docpack ([LINK](https://github.com/MacHu-GWU/docpack-project)) open-source libraries enabling gitignore-style selective crawling across non-traditional filesystems, establishing template system reducing knowledge base deployment from months to 1 hour  
* Engineered automated sync pipeline with hash-based incremental updates, integrated open-source embedding models over Bedrock Titan for cost optimization while maintaining performance  
* Reduced onboarding friction for enterprise, team, and project-level knowledge discovery, improved developer productivity through instant access to code documentation, business processes, and compliance procedures

**Enterprise AI Observability Project @ Travelers** (BunnymanTech Client) *\- Enterprise AI Architect*

* Designed hybrid observability architecture using Arize Phoenix SDK with dual-write pattern, capturing 500K monthly AI inference calls across 10 core applications representing 80% of company AI workload  
* Implemented data pipeline through CloudWatch, Kinesis Firehose, S3, and OpenSearch enabling long-term compliance auditing, cost tracking by application/team/department, and prompt quality analysis  
* Achieved 40% cost reduction through trace-level token analysis identifying optimization opportunities, built custom adapter extending Arize Phoenix SDK for MongoDB backward compatibility  
* Evaluated and compared three commercial platforms (Arize, Fiddler, Dynatrace) against open-source alternatives through systematic three-dimensional framework (data ingestion, capability matching, SDK integration complexity)  
* Developed POC on most complex application (Underwriting Business Research) validating architecture before progressive rollout to production applications

**AI Transformation Consulting Framework @ BunnymanTech** *\- Enterprise AI Architect*

* Designed AI Catalyst framework ([LINK](https://www.easyscalecloud.com/services/ai-catalyst-for-enterprise)) with seven transformation pathways covering enterprise AI adoption lifecycle, reducing client decision cycles from 2-3 months to 3 weeks and meeting frequency from 6 to 3 sessions  
* Created differentiated methodology positioning security compliance as foundational transformation pathway, addressing compliance requirements upfront rather than post-implementation  
* Developed demo applications, architecture templates, and standardized material packages (websites, presentations, deployment guides) enabling rapid proposal generation and technical evaluation  
* Established repeatable delivery framework with clear phase definitions, success criteria, and milestone tracking, building competitive moat through knowledge assets difficult to replicate

**Data Modernization & AI Transformation @ Thrivent Finance** (BunnymanTech Client) *\- Data / AI Architect*

* **OpenSearch Migration (Student Search Platform):** Led complete backend migration of mission-critical student recruitment platform (1,900 university clients, 5M student records, 1M daily queries) from DataStax to Amazon OpenSearch Service, architecting blue-green deployment via index alias strategy, divide-and-conquer parallel ETL framework, and bulk update optimization achieving 76% cost reduction ($25K to $6K monthly), 5-6x performance improvement (1-2 minutes to 1-10 seconds), 30x data freshness (monthly to daily), eliminating 3-person DevOps team while handling critical production incident resolution (10-minute diagnosis after 15-person team's full-day failure).  
* **Business Intelligence AI Agent:** Architected and delivered production-grade text-to-SQL system as College Board's first AI application, building custom ohmy-sql MCP engine supporting dual-datalake architecture (AWS Glue Catalog Athena \+ Redshift), AI-native data dictionary with 200+ tables, and Bedrock Agent orchestration with intelligent Parquet/DuckDB caching, reducing business intelligence query turnaround from 3-5 days to 10-20 seconds across 20-40 active users, directly securing second-year contract renewal through full-stack implementation (React frontend \+ serverless backend \+ AWS CDK infrastructure).

**DynamoDB Data Lake Solution @ Thrivent Finance** (BunnymanTech Client) *\- Data Architect*

* **DynamoDB Data Lake Solution:** Architected serverless platform using custom Rust-accelerated serialization engine (dynamodb\_json\_seder, 80x faster than boto3) and divide-and-conquer parallel processing (s3manifesto with 100+ concurrent Lambdas) to sync 50+ tables (500M records, 500GB) in 2 minutes versus hours with AWS Glue, reducing monthly costs from $100K+ to \~$1K while enabling real-time SQL analytics with 99%+ query cost savings.

**Gatorade Acquisition Data Integration @ PepsiCo** (BunnymanTech Client) *\- Data Engineer*

* **Cross-Cloud Data Integration:** Built Airflow-based ETL pipeline syncing 100M customer records from AWS S3 to Azure Blob Storage and Salesforce for Gatorade acquisition, coordinating cross-functional approvals across AWS/Azure teams and achieving 95% uptime with October on-time delivery.

**Power Plant KPI Data Platform @ EDF Energy** (BunnymanTech Client) *\- Data Architect / Engineer*

* **Power Plant KPI Engine:** Architected dynamic lazy-loading calculation system using Step Functions event loop pattern, Lambda dependency resolver (toposort), and DynamoDB state tracking to orchestrate 20+ interdependent PySpark Glue Jobs, achieving 10x faster execution and 60-80% cost reduction while mentoring team to independent delivery within 7 months.

**Music Royalty Data Platform @ SoundExchange** (BunnymanTech Client) *\- Data Architect / Engineer*

* **Third-Party Data Entity Resolution**: Designed and implemented AWS Glue ML FindMatch entity matching system to resolve 500K inconsistent artist records across 10+ streaming platforms, achieving 90% accuracy through custom sampling strategy and business team labeling workflow, unlocking $5-6M annual royalty payments to 100K+ musicians.  
* **Third-Party Data Ingestion Platform**: Architected extensible multi-source data ingestion platform using push/pull patterns and Command design pattern with EventBridge, Lambda, AWS Batch, and DynamoDB state tracking, leading 6-7 engineers to deliver 10 vendor integrations (SFTP, REST APIs, CSV/JSON/XML) into S3 data lake and Redshift within 15 months, reducing per-source development time from weeks to days.

**Federal Retirement Investment Project @ Accenture** (BunnymanTech Client) *\- Data Architect / Engineer*

* **Mutual Fund Price ETL Pipeline**: Engineered serverless ETL pipeline using AWS Lambda, EventBridge, and Step Functions with custom Python parser for proprietary file format, processing 10 years of historical mutual fund pricing data into S3 data lake with Athena pivot transformations, achieving zero-incident stability since September 2022\.  
* **AWS Cross-Cloud Data Synchronization**: Architected event-driven cross-partition S3 sync solution using Lambda, DynamoDB state tracking, and smart routing mechanism to transfer 700MB daily between AWS Gov Cloud and Commercial Cloud, achieving 1+ year zero-downtime operation.  
* **Login Log Auditing Data Pipeline**: Built custom Okta API crawler using Lambda, DynamoDB, and time-sliced ingestion (15-minute chunks) to extract 10 million login records in hours, replacing tens-of-thousands-monthly Informatica connector with automated daily pipeline storing partitioned JSON logs in S3 for fraud detection ML models.  
* **Fraud Detection Feature Engineering**: Refactored 500+ line nested SQL monolith into modular PySpark AWS Glue ETL job with staging table upsert mechanism and in-memory DataFrame caching, transforming S3 raw data into Redshift Feature Store while enabling rapid iteration for evolving fraud detection requirements.

## **Amazon Web Service (AWS) \- Senior Data Architect, Data Ops Evangelist**

July 2021 \- Apr 2023, Herndon VA

**MLOps Architecture and Best Practices**

* **Multi-Account MLOps Platform Architecture:** Designed enterprise multi-account and multi-environment ML platform architecture establishing cross-account IAM patterns, centralized artifact management, and environment-specific deployment strategies enabling secure model development and promotion across Dev/Staging/Production environments.  
* **End-to-End MLOps Pipeline with MLflow and SageMaker:** Established comprehensive MLOps best practices integrating MLflow for centralized model registry with SageMaker Pipeline for automated workflows encompassing model training, hyperparameter tuning, evaluation gates, conditional approval, model registration, and deployment—enabling production-ready ML lifecycle management with CI/CD automation and model versioning.

**Big Data and DataOps Best Practices**

* **NoSQL to Data Lake CDC Pipeline Patterns:** Created standardized migration architectures for DynamoDB and MongoDB to Data Lake using AWS DMS and Apache Hudi, establishing near-realtime CDC ingestion patterns, upsert mechanisms for change data capture, and data lake optimization strategies enabling scalable analytics on operational databases.  
* **Data Pipeline Visibility and Quality Framework:** Pioneered AWS DataOps best practices framework introducing advanced ETL job health monitoring, data traffic tracking, data quality metrics, automated failure detection with error tracing, and real-time alerting—establishing comprehensive observability patterns for production data pipelines that became foundational methodology across AWS data engineering teams.  
* **OpenSearch SME and Customer Enablement:** Served as AWS OpenSearch Subject Matter Expert during product launch, authored "Amazon OpenSearch Jump Start" workshop evangelizing migration best practices and management patterns, mentored multiple AWS customers in transitioning from Elasticsearch to OpenSearch, and pioneered anonymous entity matching solutions using OpenSearch enabling sub-second fraud detection and identity resolution use cases.

**Serverless Architecture and Lambda Best Practices**

* **Lambda Function Operations and CI/CD:** Defined comprehensive Lambda microservice development standards establishing CI/CD automation patterns with GitHub Actions and CodePipeline, created observability frameworks using Lambda Powertools with CloudWatch and OpenSearch for distributed tracing, developed Lambda layer versioning with hash-based publishing, and authored reusable deployment templates with API Gateway integration patterns—systematizing serverless operational excellence across enterprise deployments.  
* **Step Functions Orchestration Framework:** Developed early step\_function\_sdk enabling Airflow-like declarative workflow definitions predating AWS CDK Step Functions constructs, creating intuitive Python abstractions for complex serverless orchestrations including Job Poller patterns for SageMaker integration, coordinator-worker architectures, and event-driven workflow compositions—simplifying Step Functions adoption and establishing orchestration best practices for distributed serverless systems.

**Intelligent Document Processing**

* **Modular IDP Pipeline Architecture:** Pioneered production-ready IDP reference architecture in pre-AI era (2021-2022) establishing foundational AI service patterns, designing event-driven Lambda-based processing pipelines with S3 data lake integration, creating pluggable modular components for business-specific document workflows including OCR with AWS Textract, entity detection with Amazon Comprehend, and custom field extraction algorithms—enabling extensible document automation systems that became template for AI document processing solutions.  
* **Document Deduplication and Similarity Engine:** Invented intelligent document deduplication system using TF-IDF similarity models achieving O(N log N) computational complexity, developed Git-inspired difference analyzer with word-by-word comparison using difflib and fuzzywuzzy, and created decision tree classifier for duplicate detection—establishing reusable algorithms handling scan quality variations and enabling efficient document processing at scale.  
* **Human-in-the-Loop Validation Framework:** Designed comprehensive Augmented AI integration patterns with conditional triggering based on confidence thresholds and configurable sampling rates, created custom Liquid templates for SageMaker Ground Truth workforce UIs with local rendering tools, and developed cookiecutter template system enabling 95% code reuse across multiple document processing subsystems—establishing HIL best practices ensuring quality control for AI-powered document automation.

**Developer Experience and Open Source Innovation**

* **boto\_session\_manager \- Modern AWS Python SDK:** Authored AWS official Python library introducing type hints, auto-completion, and static analysis for boto3, eliminating common API usage errors and fundamentally improving AWS SDK developer experience with IntelliSense support validated across thousands of engineering teams globally.  
* **s3pathlib \- Intuitive S3 SDK:** Created widely-adopted S3 operations library providing file-system abstractions and pathlib-like interfaces for S3, dramatically reducing learning curve for S3 application development and simplifying complex operations into intuitive Python patterns—achieving significant adoption as AWS official SDK project.  
* **pynamodb\_mate \- Enhanced DynamoDB SDK:** Developed comprehensive DynamoDB SDK extension incorporating industry best practices for single-table design, versioning patterns, and query optimization, simplifying DynamoDB application development through high-level abstractions and type-safe operations.  
* **AWS Console Navigator with Full-Text Search:** Created afwf\_aws\_console interactive desktop application enabling full-text search across AWS Management Console with instant navigation, supporting both GUI and CLI interfaces, dramatically accelerating AWS resource discovery and console workflow efficiency for developers.  
* **AWS Resource Interactive Search Tool:** Developed aws\_resource\_search terminal application enabling interactive searches across AWS resources with fuzzy matching and real-time filtering, streamlining resource discovery and management workflows through intuitive command-line interface.

**Technical Evangelism and Knowledge Leadership**

* **Documentation as Code Methodology:** Pioneered "documentation as code" best practices collaborating with AWS ML, Datalake, and Database teams to establish maintainable internal knowledge bases generated directly from codebases, creating systematic approach to technical documentation that scaled across multiple AWS service teams.  
* **AWS Workshops and Training Programs:** Authored 10+ public AWS Workshop training programs including SFTP to S3 reliable ingestion patterns, AWS Glue Spark jobs local development and unit testing, Lambda Microservice CI/CD best practices, RDS/NoSQL to Data Lake migration using DMS, Amazon OpenSearch fundamentals and migration strategies, and AWS Textract Human-in-the-Loop workflows—collectively enabling rapid customer onboarding and knowledge transfer across AWS ecosystem.  
* **Publications and Technical Talks:** Delivered multiple AWS Tech Talks covering AWS Console Navigator, AWS Resource Search, Serverless CI/CD best practices, DynamoDB status tracking patterns, and Human-in-the-Loop creative solutions; authored AWS Blog posts on DynamoDB business-critical application patterns and versatile IDP solution architectures; presented at AWS re:Invent Financial Tech Field sessions—contributing to public AWS knowledge ecosystem and thought leadership.  
* **Customer Mentoring and Enablement:** Created DataOps Productionalization Training Workshop Program upskilling AWS solution architects in transitioning prototype solutions to production-ready environments, mentored 10+ enterprise customers in serverless adoption and data platform modernization, and contributed to public-facing open-source projects collectively achieving 2 million monthly downloads—fostering thriving AWS developer community and enabling systematic knowledge transfer.

**AWS Data Lab (Accelerated Engineering Program)** *\- Senior Data Architect*

* **Build Lab \- Hybrid CDC ETL Pipeline for Financial Services Company:** Designed and implemented hybrid EMR-Glue architecture processing 100TB of mainframe and SQL Server data, reducing incremental update cycles from 6+ hours to under 2 hours using Apache Hudi upsert patterns, while establishing Lake Formation tag-based access controls and MWAA orchestration for production-ready data lake deployment.  
* **Build Lab \- Enterprise Data Lake Platform for Management Consulting Firm:** Architected three-tier ETL pipeline using AWS Glue to transform 300+ datasets from mixed formats into optimized Parquet, migrated PostgreSQL configuration to DynamoDB, implemented Lake Formation TBAC for four personas, and delivered Step Functions orchestration enabling five-year scalable analytics platform supporting cross-enterprise business units.  
* **Build Lab \- Real-Time Entity Resolution for Foreign Exchange Platform:** Engineered AWS OpenSearch-based matching system achieving 300ms latency with 97% accuracy (exceeding 1000ms/95% targets), supporting 100 TPS without caching for 125M customer records, using novel determinative/non-determinative field classification with fuzzy matching and geo-location algorithms to enable sub-second fraud detection at scale.  
* **Design Lab \- ML Product Recommendation System for Biopharma Company:** Architected end-to-end ML pipeline using SageMaker for automated product matching across 6M SKUs, designed AWS Glue ingestion from Snowflake and unstructured sources, integrated Augmented AI for human-in-the-loop validation, and established event-driven architecture with Step Functions orchestration enabling transition from manual vendor-based process to automated in-house solution.  
* **Design Lab \- Healthcare Analytics Modernization for Cost Management Company:** Designed three-tier S3 data lake with dual-stage Glue ETL validating schemas from insurance providers, physicians, and patients, architected Lake Formation tag-based access controls, evaluated Amazon QuickSight as Tableau/Snowflake replacement, reducing data processing timeline from two months to under two weeks while enabling scalable analytics on production CRM system.  
* **Build Lab \- ML-Powered Product Recommendation Engine for Life Sciences Company:** Developed event-driven serverless architecture processing 7M products across 100 Match Groups, invented conditional probabilistic triggering for Augmented AI workflows, created custom task UI templates with local rendering tool, and achieved 13-minute end-to-end processing (5.5min ETL \+ 7min computation) enabling 15-minute recommendation updates versus quarterly—demonstrating 2,880x improvement in refresh capability.  
* **Design Lab \- Zero-Downtime Kafka Migration for Fortune 100 Insurance Company:** Architected hybrid cloud network using Transit Gateway and Direct Connect for migrating 158 Kafka topics across on-premises and multi-account AWS environments, designed authentication matrix for six application types (Mule API, Spring Boot, Debezium CDC, DynamoDB Streams, Spark Stream), and created divide-and-conquer migration strategy using MirrorMaker with synced offsets guaranteeing strong ordering and zero data loss.  
* **Design Lab \- Golden Record System for Automotive Services Provider:** Invented novel "Golden Tier" prefix-tree data structure enabling constant-time multi-tenant data isolation for 2B customer records, designed hybrid DynamoDB-OpenSearch architecture with CDC pipeline supporting dynamic fuzzy search patterns across dealership hierarchies, and reduced monthly costs from $17K to $800-1K while ensuring automatic query-time isolation preventing access to stale records during ownership transfers.  
* **Design Lab \- Data Lineage and Traffic Monitor for Educational Testing Organization:** Designed AWS Neptune graph database solution storing data catalog metadata and ETL transformations as nodes/edges, created novel data model inspired by Apache Spark operators capturing Map/Apply/Join/Split/Merge transformations, and developed Python automation using boto3/SQLAlchemy for batch-converting Glue Catalog tables and RDBMS schemas enabling bidirectional lineage traversal for legal compliance and operational transparency.  
* **Design Lab \- Enterprise Document Search for Investment Management Firm:** Architected serverless ingestion pipeline using Kinesis Data Streams for event aggregation, DynamoDB for status tracking, and Lambda coordinator-worker pattern achieving near-real-time (30-60s) searchability with exact-once delivery for 10M documents with 100M daily updates, integrated token-based ACL with 10K reader groups, conducted experimental validation of undocumented Kendra API behaviors, and delivered comprehensive Kendra vs. OpenSearch evaluation across eight dimensions demonstrating ML-powered relevance ranking benefits despite higher AWS resource costs.  
* **Design Lab \- Scalable ML Operations Platform for Marketing Automation Company:** Designed comprehensive data and ML pipeline architecture using AWS DMS for CDC capture, highly parameterized Glue jobs processing hundreds of customer accounts concurrently, SageMaker Pipelines with step caching for model versioning, demonstrated Step Functions orchestration advantages for data engineering teams, validated Batch Transform scalability with 250MB micro-batch processing, and created ML ops maturity roadmap from "Repeatable" phase to production-ready deployment.  
* **Build Lab \- ML Lead Scoring API for Reverse Mortgage Company:** Deployed scikit-learn logistic regression model to SageMaker Endpoint in VPC-Only mode by identifying undocumented VPC endpoint requirements (S3, SageMaker API/Runtime, STS, CloudWatch with Private DNS), designed serverless proxy using API Gateway and Lambda with custom authorizer for token-based authentication, achieved exact prediction accuracy match with local model, and delivered complete automation scripts with unit/integration/load tests enabling secure public internet access to private endpoints.  
* **Build Lab \- Intelligent Document Processing for Property Insurance Company:** Developed Python-based IDP solution achieving 100% classification accuracy and 100% field extraction (exceeding 95%/70% targets) for CMS1500 medical forms, engineered three custom algorithms for fuzzy-matching, geo-location-based table cell realignment, and bounded-box coordinate search to handle scan quality issues, designed event-driven architecture with Lambda/SNS/DynamoDB/S3, and validated solution against low-quality edge cases demonstrating production readiness for processing 3M annual documents.  
* **Build Lab \- Automated ML Pipeline for Marketing Platform Customer Churn:** Implemented five-step SageMaker Pipeline automating data retrieval, hyperparameter tuning, evaluation, and conditional model registration using Pipeline Session for proper context storage, designed hybrid orchestration with Step Functions "Job Poller" pattern for integrating SageMaker workflows with existing infrastructure, established production-ready MLOps framework with modularized Python library and CI/CD shell scripts, and achieved 90% accuracy/precision training on 5M production records with weekly automated retraining capability.  
* **Build Lab \- Radiology Follow-Up ML Infrastructure for Healthcare Technology Company:** Architected event-driven inference system using Lambda as ML proxy decoupling dependencies, integrated Amazon Comprehend real-time endpoint with dual-write to DynamoDB (via PynamoDB ORM) and S3 Data Lake, developed custom Python abstractions simplifying Comprehend's boto3 API for multi-class classifier training, created SageMaker Pipeline with "PendingApproval" gates, invented reusable modules for various data sources, and built unexpected A2I human-in-the-loop workflow demonstrating potential 80% reduction in manual review workload.  
* **Build Lab \- Document Deduplication for Insurance Risk Assessment Firm:** Engineered intelligent document processing pipeline with MD5-based file naming, TF-IDF similarity model using gensim achieving O(N log N) complexity (50x faster), Git-inspired difference analyzer with difflib/fuzzywuzzy for word-by-word comparison, and decision tree classifier achieving 94.33% accuracy with 97.02% F1 score detecting 98.2% duplicates across 421-page documents, while deploying Amazon Augmented AI workflow for generating labeled training datasets using SageMaker Ground Truth private workforce.  
* **Build Lab \- Production HIL Pipeline for Insurance Claims Automation:** Designed five-subsystem IDP platform using serverless architecture (S3, Lambda, SageMaker Ground Truth, Augmented AI, DynamoDB, SNS), built comprehensive DevOps framework with Python shell scripts/Make commands for config management using Parameter Store, deterministic dependency management via Poetry with private Nexus repository, implemented semantic Git branching strategy triggering conditional CI/CD workflows, created versioned Lambda layer publishing with hash comparison, developed conditional HIL trigger with configurable sampling rates and custom Liquid templates, and converted implementation into cookiecutter template enabling 95% code reuse across remaining subsystems.  
* **Design Lab \- Multi-Account ML Ops Platform for Beverage Distribution Company:** Architected enterprise ML platform using hybrid function/workload-oriented AWS Organization structure with Infrastructure/Networking OUs and separate Dev/Staging/Prod environments, designed cross-account IAM AssumeRole patterns for MWAA orchestration, established data sharing strategies via S3 bucket policies/Glue Catalog/Redshift Data Sharing, evaluated all four SageMaker inference options with API Gateway/Lambda proxy for Apigee integration, defined semantic branching strategy mapping git branches to environments, and delivered prioritized implementation roadmap with 22 action items grouped into five dependency-ordered phases.  
* **Build Lab \- Enhanced Deduplication with Custom Entity Recognition for Risk Assessment Firm:** Optimized document similarity thresholds using 550 labeled page pairs with weighted scoring balancing capture rate (weight 1\) and false positive rate (weight \-5), trained Amazon Comprehend custom entity recognizer for "Service Date" using 128 examples achieving 69.72 F1 score, integrated entity recognition into decision logic for borderline similarity cases, achieved 97.4% capture rate with 0% false positive rate (vs. previous 98.2%/66% or 43%/0%).

## **Enquizit \- Data Engineer and Data Architect**

Apr 2018 \- July 2021,  McLean VA

* Designed a **Data Lake/House architect** using S3 and Redshift to adapt dynamic analytics requirements for a Health  Tech Company. Built batch and stream hybrid ETL pipeline that uses **Spark** \+ **Glue** \+ **Kinesis** \+ **Lambda** to ingest and transform data from a variety of data sources including FTP, Salesforce, On-prem SQL DB, API, Kafka.  
* Built a decentralized **Data Mesh** \+ **Data Lake** solution to help a Financial Bank to adapt fast-growing data complexity and number of data applications, simplified the data sharing between internal teams and external users with grained data access control and PII data masking using **AWS Lake Formation**.  
* Developed a **high throughput** (25,000 TPS) **near real-time ETL** solution to sync the data from the application **Dynamodb** database to **ElasticSearch** for an IOT device company. Created a central metrics report **BI dashboard** using **AWS Athena** and **QuickSight** providing timely business insight for decision making.

## **EarthNetwork R\&D Group \- Data Scientist and Data Engineer**

Oct 2014 \- Apr 2018, Germantown MD

* Developed an energy consumption prediction model for **demand response**, reducing total energy consumption by 5% during peak times.  
* Created an HVAC schedule ranking algorithm for monthly scorecard, helping users to achieve higher energy efficiency.  
* Innovated a solution using AWS **Lambda** and **MongoDB** to estimate house square footage for connectivity-saving program.  
* Automated weekly report email generation and delivery process by integrating databases and **RESTful APIs**.

## **The Eagle Force Inc \- Data Scientist**

Feb 2014 \- Aug 2014, Herndon VA

* Conducted analysis of Electronic Health Records (**EHR**), disease, drug and activity data to build a monitoring and alert system for senior care facilities. The system identifies potential risks to prevent accidents and enhance patient safety.  
* Integrated app code (**Ruby**) with scientific computing code (**Python**), bridging the gap between software engineering and scientific research. Fostered effective collaboration with cross-functional teams.

## **Cybioms Corporation \- Data Scientist**

Aug 2013 \- Feb 2014, Rockville, MD

* Performed signal processing and data visualization on measurement data to demonstrate business insights.  
* Analyzed measurement and instrument log data to identify and resolve internal issues.  
* Developed **Python**\-based automated testing tools to efficiently manage and maintain hardware stacks.

# **Education**

## **The George Washington University, Master of Science in Electrical And Computer Engineering**

Aug 2011 \- May 2013, Washington DC

# **Skills**

* **Python**: expert, author of multiple popular frameworks and AWS official Python projects  
* **AI**: AWS Bedrock, OpenAI, Anthropic, Gemini, strand agents, LangChain, MCP   
* **Data Stream**: AWS Kinesis, Kafka  
* **Data Processing**: Spark, AWS EMR, AWS Glue, AWS Batch  
* **Data Lake/Warehouse**: AWS S3, Redshift, Hudi, SnowFlake  
* **Data Analytics**: AWS Athena, AWS OpenSearch, ElasticSearch  
* **Data Security**: AWS LakeFormation, Glue Catalog  
* **AWS**: EC2, VPC, S3, IAM, RDS, subject matter expert of most of the AWS services  
* **SQL Database**: Postgres, MySQL, AWS Aurora  
* **NoSQL Database**: AWS Dynamodb, MongoDB, Redis, AWS TimeStream DB  
* **Machine Learning**: AWS Sagemaker, Comprehend, Textract, Recognition, MLFlow  
* **Generative AI**:  LangChain, Amazon Bedrock, OpenAI, Anthropic, AI Agent, Prompt Engineering  
* **Search Technology**: ElasticSearch, OpenSearch, Amazon Kendra, Whoosh  
* **Microservice**: AWS Lambda, AWS API Gateway, AWS Chalice  
* **Container**: Docker, AWS ECR, ECS,  
* **Visibility, Monitoring and Dashboard**: AWS CloudWatch, AWS SNS, AWS QuickSight  
* **Infrastructure as Code**: CloudFormation, AWS CDK, Terraform  
* **Orchestration**: AirFlow, AWS MWAA, AWS StepFunction, SageMaker Pipeline  
* **DevOps**: Ansible, Packer  
* **CI/CD**: Jenkins, GitHub Action, CircleCI, AWS CodeBuild, AWS CodePipeline

# **Open Source Projects**

* **AWS Official S3 SDK s3pathlib**: [https://github.com/aws-samples/s3pathlib-project](https://github.com/aws-samples/s3pathlib-project)  
* **AWS Official Boto Session Manager**: [https://github.com/aws-samples/boto-session-manager-project](https://github.com/aws-samples/boto-session-manager-project)  
* Influential open-source projects, total monthly downloads are around **10 million/month**: [https://github.com/MacHu-GWU](https://github.com/MacHu-GWU)

# **Certificate**

* **AWS AI Practitioner,** Nov 2025  
* **AWS Certified Big Data Specialist,** June 2019  
* **AWS Certified Solutions Architect,** May 2019

# **Awards**

* **Cloud Guru Serverless Conf Hackathon, Best AWS Architect Winner**, Oct 2019, New York, NY  
* **AWS Public Sector DC Summit Hackathon For Good: Best App Winner,** June 2019, Washington, DC
  </markdown_content>
<document>
<document>
  <title>2024-06 to 2025-01 - Enterprise Knowledge Integration  System</title>
  <markdown_content>
# 2024-06 to 2025-01 - Enterprise Knowledge Integration  System



## Overview

I built a RAG-based knowledge integration platform that solves a problem most large enterprises face: critical information scattered across Confluence, GitHub, and SharePoint with no unified way to access it. My solution uses a multi-source architecture with templated deployment, reducing the time to create a production-ready knowledge base from months down to roughly an hour. The adoption numbers tell the story - over 100 projects went live in the first month, and the platform now processes 20,000 AI-powered queries every day. The measurable impact on onboarding efficiency and knowledge accessibility has been substantial.

## Background

### The Travelers Environment

Travelers Insurance operates a mature AWS infrastructure distributed across more than 50 accounts. The architecture follows standard enterprise patterns: EKS clusters host internal API services, S3 handles object storage, DocumentDB manages unstructured data, Aurora provides managed PostgreSQL, and Lambda powers the microservices layer.

I joined Travelers in September 2023 as an Enterprise Architect on the Enterprise AI Team. The broader architecture organization consists of about 20 architects, with three of us specifically focused on AI initiatives. I report to the VP of Architecture, who in turn reports to the SVP & CIO of Enterprise Tech Solutions. My direct team included 9-10 engineers responsible for implementation work.

My relationship with Travelers predates my full-time role. I spent several years at AWS before transitioning to consulting, where I completed three projects for Travelers, including an Intelligent Document Processing system built on OCR and AI entity extraction. Those successful engagements established the technical credibility that led to my permanent position on the AI team.

### The Knowledge Silo Problem

Enterprise knowledge at Travelers exists in three massive, disconnected repositories: over 1,000 Confluence spaces containing wikis and technical documentation, more than 3,000 GitHub repositories housing code and development docs, and SharePoint storing business documents, policies, and procedures. This fragmentation was creating serious productivity bottlenecks across the organization.

Unlike the tech giants - Google, Amazon, Microsoft - Travelers lacks systematic mentoring infrastructure and comprehensive onboarding documentation. The challenge isn't just creating one type of documentation. Enterprise onboarding requires understanding HR policies, finance workflows, and benefits systems. Team onboarding demands familiarity with specific working styles and toolchains. Project onboarding means getting up to speed on particular codebases and business logic. There's no universal document that addresses all these contexts, and maintaining separate, detailed documentation for each level represents a massive ongoing effort that rarely stays current.

The documentation problem goes deeper than just scattered files. When documentation does exist, new hires face a different challenge: they don't know where to start or what's relevant. Nothing is systematically organized or curated for their specific needs. A new engineer typically needs two full weeks just to achieve basic familiarity with a codebase, and throughout the following month or two, they're constantly interrupting senior engineers for clarification. Approval workflows are even worse - employees can spend two weeks just understanding the process, another week or two obtaining necessary credentials, and potentially another two weeks navigating escalations when things don't go smoothly.

This inefficiency manifests as a measurable productivity gap. An experienced engineer completes a project in roughly one month. That same project, when handed off to someone less experienced, takes three months - the extra two months represent pure knowledge transfer overhead. In an AI-driven era where product iteration cycles are accelerating rapidly, this knowledge transfer bottleneck prevents the organization from keeping pace with technological change.

### Why Commercial Tools Don't Work

Each platform - Confluence, SharePoint, and GitHub - offers AI-powered search capabilities, but none of them address the fundamental problem Travelers faces. The economics alone make Confluence's AI features prohibitive: at a per-employee pricing model, enabling AI functionality would cost tens of thousands of dollars monthly, approaching a million dollars annually. SharePoint's Copilot struggles with comprehensive directory-level content search. GitHub doesn't provide AI-powered knowledge base search at all.

The bigger issue is architectural. Information doesn't live in isolated platforms. A typical project spans 4-5 GitHub repositories, multiple Confluence folder hierarchies, and various policy documents stored in SharePoint. Single-platform AI tools can't synthesize knowledge across these boundaries, which is exactly what employees need most.

### The Strategic Opening

In June 2024, Travelers made a strategic commitment to AI adoption at the executive level, with secure knowledge management identified as a core initiative. The timing was ideal from a technology maturity standpoint. RAG architectures had evolved significantly over the previous year, and the vector database ecosystem had stabilized enough for production deployment. I had the right technology, the right organizational commitment, and the right timing. My mandate was to identify high-value use cases, validate technical approaches, and deliver a solution that would demonstrate measurable business impact quickly.

## Situation

### Three Core Pain Points from Knowledge Silos

**Onboarding efficiency represents the first critical pain point.** New engineers require approximately two weeks to develop basic familiarity with a codebase, but even after that initial period, they can't work independently. They're constantly pulling senior engineers away from high-value work for clarification and guidance. This dependency continues for another month or two. The organization lacks the systematic mentoring infrastructure and layered documentation architecture that large tech companies have invested in building over many years.

The documentation that does exist presents three fundamental challenges. First, creating well-structured technical documentation is genuinely difficult and time-consuming. Second, keeping documentation synchronized with rapidly changing systems requires continuous effort that rarely happens in practice. Third, relevant documentation is distributed across GitHub repositories, SharePoint directories, and Confluence spaces with no unified discovery mechanism. The end result: incomplete documentation, overwhelming volume without clear navigation, and new employees who can't effectively bootstrap their own learning.

**Approval process opacity is the second major pain point.** Employees frequently encounter situations requiring specific credentials or workflow approvals, but they lack visibility into who owns these processes. The typical pattern involves asking colleagues, locating someone who has successfully navigated the process before, searching through historical documentation, and often scheduling meetings to extract institutional knowledge. This cycle can easily consume one to two weeks. The problem compounds over time because the people who successfully navigate these processes have little incentive to document their experience comprehensively, so organizational knowledge never accumulates systematically.

A real example illustrates the cost: a project reached deployment readiness but required a specific certificate. Obtaining that certificate took two weeks. Understanding the approval workflow took another two weeks. Potential escalations added another two weeks. A month and a half of elapsed time attributable entirely to process navigation overhead.

**Knowledge transfer cost represents the most critical pain point.** An experienced engineer can complete a project independently in approximately one month. Delegating that same project to someone less experienced extends the timeline to three months. That two-month delta represents pure knowledge transfer overhead - context sharing, clarification, guidance, and error correction.

The problem intensifies when offshore resources enter the equation. Transition periods can consume an entire month before the offshore team achieves basic productivity. Throughout this entire process, senior engineers are investing significant time in knowledge transfer rather than high-impact technical work. In an AI-driven market where product iteration cycles are measured in weeks rather than months, slow information flow becomes an existential competitive disadvantage. The organization simply can't keep pace with the rate of technological evolution.

### Technical Complexity

**Knowledge source scale and complexity presented the first architectural challenge.** Travelers maintains over 1,000 Confluence spaces, but the relationship between projects and spaces isn't one-to-one. Many projects correspond to individual folders or span multiple folder hierarchies within larger spaces. GitHub hosts more than 3,000 repositories, with typical projects touching 4-5 repos. SharePoint contains extensive policy documentation, templates, procedures, and business documents distributed across numerous directory structures.

Simply indexing everything wasn't viable. I needed granular control over what content gets included in each knowledge base.

**Selective integration requirements formed the second challenge.** Creating one massive, monolithic knowledge base would have introduced severe problems: retrieval noise from irrelevant content, context confusion from mixing unrelated domains, complex permission management, and maintenance nightmares. The solution required supporting numerous small, focused knowledge bases - each one targeting a specific project, team, or system. This approach maintains context consistency and improves retrieval precision.

The technical challenge was enabling flexible filtering at the file and folder level. I needed to support GitHub file selection, Confluence folder hierarchies, and SharePoint directory structures. The complication: Confluence and SharePoint don't behave like traditional file systems. I needed an intuitive way for users to specify inclusion and exclusion rules without requiring them to understand complex internal data models.

**Deployment scale presented the third challenge.** Knowledge bases are needed at every organizational level: corporate, departmental, team (hundreds of teams), and project (thousands of projects). If standing up a knowledge base requires months of effort, scaling to thousands of instances becomes impossible. I needed a lightweight, structured approach that enables users to deploy production-ready knowledge base applications in minimal time.

**Enterprise environment constraints formed the fourth challenge.** Travelers enforces rigorous network security policies and approval processes. OpenSearch exists in the infrastructure but primarily serves log analysis workloads running on clusters rather than serverless deployments, raising both approval complexity and cost concerns. Cloud-hosted vector databases like Pinecone offer compelling features but require extensive security review and carry significant licensing costs. The solution needed to work within the existing technology ecosystem and approval framework.

## Task

As Enterprise Architect, I owned the complete system architecture, technology selection decisions, and resolution of critical technical challenges. The project timeline spanned seven months from June 2024 through January 2025. The implementation team consisted of two engineers focused on data pipeline infrastructure, two engineers building the RAG AI application backend, and one engineer developing the frontend interface.

The primary objective was delivering an enterprise-grade RAG knowledge integration platform with four core capabilities:

**Multi-source knowledge integration with fine-grained control.** The system needed to aggregate content from Confluence, GitHub, and SharePoint with flexible, file-level filtering capabilities. This wasn't about bulk indexing entire spaces or repositories - I needed precise, selective integration.

**Distributed knowledge base architecture.** Rather than building a monolithic knowledge base, the platform would support creating numerous small, purpose-built knowledge bases. Each instance would focus on a specific project, team, or system, maintaining strong context consistency and retrieval accuracy.

**Templated rapid deployment capability.** The time required to create and deploy a knowledge base application needed to drop from months to hours. Users shouldn't need to write code - configuration files should drive the entire deployment process, from knowledge extraction through vectorization to production AI application deployment.

**Operation within existing enterprise constraints.** The solution had to work within the current technology stack to avoid lengthy approval cycles. Vector database selection needed to balance cost, capability, and deployment velocity.

Success would be measured through three key indicators: rapid adoption across multiple projects post-launch, measurable improvement in employee onboarding experience and knowledge access efficiency, and reduced time investment by senior engineers in knowledge transfer activities.

## Action

### Initial Research & Technical Foundation (2 months)

The project benefited significantly from technical assets I'd already developed in the open source community. Before this initiative even began, I had created and was actively maintaining two libraries that would become foundational to my solution.

**atlassian-doc-parser** is a Python library that converts Confluence's complex JSON format into lightweight, normalized document representations. It also handles document extraction from GitHub and SharePoint. Anyone who's worked with Confluence at enterprise scale knows its data structure is exceptionally complex - direct JSON manipulation creates enormous parsing and formatting overhead. This library encapsulates that complexity behind a clean abstraction layer.

**docpack** is an information scraping library built around a key insight: even though Confluence and SharePoint aren't traditional file systems, users think about content selection in file system terms. The library uses glob patterns and gitignore-style configuration to let users specify inclusion and exclusion rules through declarative config files rather than imperative code. This directly addressed the selective integration challenge.

These weren't tools I built for this project - they were artifacts of my ongoing technical work in the open source community. Having them available when the project launched meant I'd already solved the hardest problems around document parsing and selective content aggregation. I could focus on RAG architecture and systematic deployment rather than reinventing fundamental infrastructure.

The first two months focused on research, prototype validation, and refining these libraries to handle enterprise-specific requirements that weren't present in my open source work.

### Architecture Design & Technology Selection

**Vector database selection was the first critical architectural decision.** OpenSearch existed in the infrastructure but served primarily as a log analysis platform running on traditional clusters rather than serverless configurations. Between approval complexity, cost implications, and my specific requirements, OpenSearch wasn't the optimal path forward. Cloud-native vector databases like Pinecone offer excellent capabilities but come with two significant barriers: extensive security review processes and substantial licensing costs.

The solution I selected was **pgvector** - the vector search extension for PostgreSQL. This decision offered three compelling advantages. First, Travelers already runs Aurora PostgreSQL extensively, so pgvector required no new service introduction and faced zero approval friction. Second, as a mature PostgreSQL extension, pgvector provides sufficient capability for my RAG use cases without the complexity of standalone vector databases. Third, the cost structure is extremely favorable - I'm leveraging existing database infrastructure rather than adding new licensing expenses.

Beyond the immediate technical fit, I designed the architecture with **abstraction and future flexibility in mind**. The data pipeline's sink layer uses a pluggable architecture - once content is processed and vectorized, it can sink to any vector database through a standardized interface. This means if I need to migrate to OpenSearch, Pinecone, or another solution in the future, the migration cost is minimal. This is a fundamental architectural principle I follow: **always leave room for evolution without requiring complete rewrites.**

**Embedding model selection balanced capability against cost.** AWS Bedrock's Titan embedding model provides a fully managed solution but carries significant per-token costs at scale. I opted for mature open source embedding models from the community. This choice maintained quality while dramatically reducing operational expenses.

### RAG Technical Implementation

**The chunking and embedding strategy follows standard patterns with key enhancements.** Documents get split into chunks, each chunk receives an embedding, and retrieval operates at the chunk level. However, when a query matches a chunk, I don't just return that fragment in isolation. I reverse-traverse to retrieve the complete source document and provide the entire document as context to the LLM. This preserves the full semantic context around each retrieved fragment.

The significant innovation is **Contextual Retrieval**. During chunk generation, I don't just extract raw text fragments. For each chunk, I generate a concise contextual summary describing where this content sits within the larger document and what role it plays in the overall narrative. When these chunks get embedded and stored, they carry both their intrinsic content and their relationship to the broader document structure. This dramatically improves both recall accuracy and relevance of retrieved results.

**I implemented a multi-stage processing pipeline to optimize both cost and quality.** The first stage uses vector similarity search to identify relevant chunks and retrieve their associated source documents. The second stage employs a cost-effective model (Claude Haiku) to summarize and filter the retrieved content based on the user's query. The third stage passes this refined, high-signal context to a more capable model (Claude Sonnet) for deep reasoning and response generation. This tiered approach maintains answer quality while keeping inference costs manageable at scale.

### Data Pipeline Implementation

**The crawler and synchronization architecture prioritizes efficiency and reliability.** Rather than fetching documents sequentially, I use paginator-based batch operations to maximize throughput. The synchronization logic implements an intelligent differential update mechanism: I first extract metadata and compute content hashes for all source files, then compare against hashes stored in my database. Only files with changed hashes trigger actual content retrieval and ingestion. This makes incremental synchronization extremely efficient - I'm not reprocessing unchanged content.

Synchronization frequency is configurable per knowledge source, ranging from hourly to daily based on content volatility. The entire pipeline follows an event-driven architecture, with file change detection triggering appropriate ingestion workflows automatically.

From my perspective, this pipeline design is relatively straightforward - conceptually, every file in GitHub is a key-value pair, every Confluence page is a key-value pair, every SharePoint document follows the same pattern. With my background as an AWS Data Architect, designing extraction, ingestion, chunking, and embedding pipelines for key-value data is familiar territory. However, I recognize this represents significant engineering complexity for teams without extensive data pipeline experience.

### Templated System Design (3 months implementation)

**The core value proposition centers on templated, rapid deployment capability.** I invested three months working with the engineering team to build and deploy the complete system infrastructure.

The user experience is deliberately simple: creating a new knowledge base application requires only uploading a JSON configuration file. This configuration specifies:

- Source GitHub repositories with glob patterns defining file inclusion/exclusion rules
- Target Confluence spaces and folders with appropriate filtering criteria
- Relevant SharePoint directories with file type specifications

Once the system receives this configuration, it orchestrates the complete deployment automatically:

1. Extracts documents from the three source platforms according to configuration rules
2. Processes documents through atlassian-doc-parser for normalization
3. Executes chunking with contextual retrieval enhancement
4. Stores resulting vectors in pgvector
5. Deploys a complete AI Q&A application including both backend API and frontend UI

The target I achieved: **a single person can create and deploy a production knowledge base application in approximately one hour**. This represents a reduction from months to hours - exactly what I designed for.

### User Onboarding & Promotion (1 month)

Following system deployment, I dedicated a month to internal customer onboarding and enablement. This included comprehensive usage documentation, hands-on workshops, and one-on-one technical support for early adopters.

Adoption accelerated rapidly because the value proposition was clear and the barriers were minimal. At extremely low incremental cost with demonstrable efficiency benefits, I encountered no stakeholder resistance. Teams that saw demonstration sessions immediately recognized how the system addressed their specific pain points and proactively requested access.

## Result

### Rapid Scale Adoption

Within one month of launch, **over 100 projects had created their own knowledge bases** on the platform. Given that Travelers operates roughly 1,000 projects, this represents a 10% adoption rate in the first 30 days - strong validation of both the system's utility and its ease of deployment.

Usage metrics demonstrated sustained growth. Across all deployed knowledge bases, the platform processes **approximately 20,000 AI-powered queries daily**. More significantly, this volume has been growing by roughly 3,000 queries per month, indicating that employees are progressively integrating the system into their standard workflows rather than treating it as an occasional resource.

### Significantly Improved Onboarding Experience

**New employee onboarding quality improved substantially**, particularly for enterprise-level knowledge domains. Questions about benefits, policies, training programs, and internal processes now get resolved through the AI assistant rather than requiring human intervention. New hires no longer spend days searching through documentation or waiting for responses from busy colleagues - they access accurate information on demand.

The three core business units - Underwriting, Claims, and Business Intelligence - have comprehensive business process knowledge organized through their respective knowledge bases. This is particularly valuable for engineers without insurance industry background. Insurance involves extensive specialized terminology, and database schemas reflect complex business logic specific to the industry. Through the knowledge base system, new employees can rapidly understand company context, decode industry-specific terminology, and comprehend data models, significantly compressing the learning curve.

While I didn't establish precise metrics for onboarding time reduction, qualitative evidence is clear: unnecessary knowledge transfer meetings have decreased substantially, ad hoc mentoring sessions have dropped in frequency, and senior engineers report spending significantly less time on basic knowledge transfer. Their time has been freed for higher-value technical work.

### Improved Knowledge Flow Efficiency

Project-specific knowledge bases transformed **codebase comprehension and technical documentation access**. When developers need to understand API usage patterns, service architecture decisions, or module evolution history, they query the AI assistant rather than searching through fragmented GitHub documentation or locating and interrupting the original authors. The friction in accessing technical knowledge dropped dramatically.

Approval workflow transparency improved measurably. When employees need credentials or must navigate approval processes, the AI assistant provides concrete guidance: who to contact, what documentation to prepare, and realistic timeline expectations. While I didn't quantify this impact through formal metrics, subjective feedback consistently indicates that back-and-forth communication cycles and time waste from process opacity have decreased noticeably.

### Technical Metrics

From a system architecture perspective, I achieved my core technical objectives:

**Rapid deployment capability:** Knowledge base application creation time dropped from months to approximately one hour, enabling the platform to scale across hundreds of projects without creating deployment bottlenecks.

**Cost optimization:** Selecting pgvector over commercial vector databases, using open source embedding models instead of managed services like Bedrock Titan, and implementing multi-stage LLM processing (cost-effective models for summarization, premium models for reasoning) kept operational costs extremely low even as usage scaled.

**Architecture extensibility:** The data pipeline's abstraction layer means future vector database migrations or new knowledge source integrations require minimal modification. This design establishes a foundation for long-term platform evolution without costly rewrites.

### Personal Technical Impact

A significant outcome of this project is that [**atlassian-doc-parser**](https://github.com/MacHu-GWU/atlas_doc_parser-project)** and **[**docpack**](https://github.com/MacHu-GWU/docpack-project)** - my open source libraries - became formally integrated into Travelers' enterprise infrastructure**. As the maintainer of these libraries, the organization now depends on my technical contributions in a foundational way. This has substantially strengthened my technical influence within the company, with other teams actively monitoring my open source work and several expressing interest in leveraging these libraries for their own initiatives.

From an enterprise AI strategy perspective, this Knowledge Integration System represents a meaningful milestone in Travelers' AI adoption journey. It validated RAG technology's practical applicability in regulated enterprise environments and established both technical patterns and operational infrastructure that subsequent AI initiatives can build upon.
  </markdown_content>
</document>
<document>
  <title>2025-01-06 to 2025-12-31 at Collegeboard</title>
  <markdown_content>
# 2025-01-06 to 2025-12-31 at Collegeboard



## About Collegeboard

College Board is a mission-driven not-for-profit organization founded in 1900 that connects students to college success and opportunity. With over 6,000 member institutions including universities, colleges, and educational organizations, College Board reaches more than 7 million students annually through programs like the SAT, Advanced Placement (AP), and BigFuture. Beyond assessments, College Board operates critical enrollment management platforms that enable higher education institutions to identify, recruit, and engage prospective students. Their technology infrastructure supports massive-scale data operations, serving both the student journey from high school to college and the business needs of universities seeking to build their incoming classes.

## **Why I Was Invited to the Project**

In January 2025, BunnymanTech Data/AI Consulting secured a contract with College Board to serve as Data/AI Architect on two core business platforms: **Modernized Student Search Service (MSSS)** and the **Higher Education Reporting Portal**. MSSS is College Board's flagship student recruitment platform—a sophisticated backend system that enables over 1,900 colleges and universities to identify and engage with prospective students from a pool of more than 5 million high school students annually. Universities use MSSS to filter students based on academic profiles, geographic locations, intended majors, and other criteria, then send targeted recruitment communications to build their prospect pipelines. This platform represents a highly profitable business line for College Board and requires a robust, scalable search infrastructure to deliver millisecond-response queries across millions of student records while maintaining strict privacy controls and data security standards.

**The first reason** for my engagement was College Board's critical need for OpenSearch expertise during MSSS's backend database migration. The platform was transitioning to Amazon OpenSearch Service, and the organization lacked internal architects with deep OpenSearch knowledge. When AWS initially launched Amazon OpenSearch Service, only five subject matter experts existed within Amazon, and I was one of them—recognized as an expert among experts in this technology. My proven track record includes leading OpenSearch implementations on high-stakes projects such as the Xe Currency Exchange entity matching system and Cox Automotive's entity resolution platform during my tenure at AWS Data Lab. College Board needed an architect-level OpenSearch specialist to lead their migration strategy, optimize query performance at scale, ensure data integrity during the transition, and establish best practices for their production environment.

**The second catalyst** for my engagement was College Board's strategic initiative to embrace Generative AI across their technology stack, where they had no personnel with hands-on AI implementation experience. My expertise spans the entire AI application lifecycle—from AI workflow orchestration and RAG (Retrieval-Augmented Generation) applications to AI agents, multi-agent systems, AI infrastructure, and AI operations. 

Given that College Board operates as an AWS-centric organization and my deep technical proficiency across AWS services, they brought me on board to architect their AI innovation roadmap and lead implementation of production-grade AI capabilities. This dual expertise in both Amazon OpenSearch Service and enterprise AI solutions, combined with my proven ability to deliver scalable architectures in complex AWS environments, positioned me as the ideal technical leader to drive College Board's data modernization and AI transformation initiatives.
  </markdown_content>
</document>
<document>
  <title>2022-01-31 - Design Lab - A Fortune 500 global provider of mission-critical products and services to the biopharma</title>
  <markdown_content>
# 2022-01-31 - Design Lab - A Fortune 500 global provider of mission-critical products and services to the biopharma

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

A Fortune 500 global provider of mission-critical products and services to the biopharma, healthcare, and advanced technologies industries was seeking to modernize their alternative product recommendation system for their B2B eCommerce platform. With over 6 million product SKUs worldwide serving 225,000+ customer locations, the company relied on an external vendor to manually perform product matching and generate "matchcodes" (alternate product suggestions) when items were out of stock, discontinued, or on allocation. This manual process involved extensive web crawling and attribute matching, creating delays and limiting scalability. The company wanted to bring this capability in-house using AWS services and implement an ML-based automated solution with human-in-the-loop review capabilities for low-confidence predictions.

## Task

I was assigned as the Data Lab Architect for a two-day Design Lab engagement to architect a scalable, end-to-end machine learning pipeline that would automate product grouping and generate alternate product recommendations. My responsibility was to design a comprehensive solution that could ingest data from multiple heterogeneous sources (structured and unstructured), perform ML-based product matching, integrate human annotation workflows for quality assurance, create dashboards for reporting and search capabilities, and orchestrate the entire pipeline with automated alerts and notifications. The architecture needed to support millions of product records with daily batch inference and quarterly model retraining cycles.

## Action

I designed a multi-phase solution architecture divided into three main components.

**Data Pipeline:** I architected a centralized data ingestion and transformation pipeline using AWS Glue to extract data from the company's Snowflake Data Warehouse and unstructured sources, apply business rules through ETL logic, and persist transformed data in Amazon S3 as the single source of truth for downstream ML pipelines.

**Machine Learning Pipeline:** I designed an ML workflow using Amazon SageMaker with Jupyter notebooks for feature engineering and model development, leveraging SageMaker's AutoML capabilities to identify optimal algorithms for product grouping (starting with KNN classification), and implementing SageMaker Batch Transform jobs for daily inference on new product data. I integrated the SageMaker Model Registry to track model metrics and artifacts across training cycles.

**Human-in-the-Loop Integration:** I incorporated Amazon Augmented AI (A2I) to route low-confidence ML predictions to private workforce reviewers for human annotation, with configurable confidence thresholds determining the routing logic. I designed an event-driven architecture using S3 event notifications and AWS Lambda functions to process human annotations in near real-time and feed them back into the training dataset for continuous model improvement. I orchestrated the entire pipeline using AWS Step Functions to provide flexibility for conditional branching, and integrated CloudWatch with Amazon SNS for comprehensive monitoring and alerting. For the recommendation engine (Phase 2 stretch goal), I designed a Glue-based pipeline to integrate product, sales, pricing, and match group data, then execute custom Python business logic to generate ranked product recommendations. I cataloged all datasets in AWS Glue Data Catalog and enabled ad-hoc SQL querying through Amazon Athena for business intelligence and reporting. Throughout the design session, I collaborated with the customer's data scientists, business analysts, and engineering teams to validate architectural decisions and ensure alignment with their operational requirements and Snowflake-centric data strategy.

## Results

I successfully delivered a comprehensive architecture design that met all stated objectives for building a scalable ML-based product recommendation system. The proposed solution provided the customer with a clear technical roadmap for replacing their manual vendor-based process with an automated, in-house AWS solution capable of processing millions of product SKUs daily. The design included specific recommendations for Build Lab preparation, including customer immersion sessions with SageMaker and Glue, and defined clear next steps for prototype development in the customer's development environment.
  </markdown_content>
</document>
<document>
  <title>2022-06-06 - Design Lab - American nonprofit educational organization</title>
  <markdown_content>
# 2022-06-06 - Design Lab - American nonprofit educational organization

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

I worked with a major American nonprofit educational organization that administers standardized college entrance examinations and advanced placement programs. As a cloud-first organization leveraging AWS serverless technologies, they operated a complex data infrastructure using DynamoDB, RDS, Redshift, AWS Lambda, and AWS Glue for data storage and ETL pipelines. Their data catalogs and ETL pipelines evolved rapidly as they scaled their operations, creating a critical challenge: they lacked visibility into data flows and lineage across their distributed systems. This created legal and reputational risks, as the organization couldn't readily explain why specific data wasn't flowing into downstream reporting systems or trace how metrics were calculated.

## Task

I was responsible for designing a comprehensive Data Lineage and Data Traffic Monitor system during a two-day Design Lab engagement. The deliverable needed to enable the organization to store and analyze both traditional data catalog metadata and data transition metadata in a query-friendly format, allowing them to efficiently traverse data flows bidirectionally and programmatically analyze data traffic. The system needed to support two critical use cases: tracing backward from downstream data sources to understand metric calculations, and tracing forward from upstream sources to analyze the impact of data changes.

## Action

I designed a graph database-based solution using AWS Neptune that stored data lineage as nodes and data transitions as edges, enabling efficient bidirectional traversal of complex data flows.

**Architecture Approach:** I evaluated three competing approaches (external system, catalog-level metadata, and record-level metadata) and recommended the external system approach using graph database technology, as it provided maximum flexibility for their rapidly evolving infrastructure without requiring changes to existing pipelines.

**Data Model Design:** I created a novel data model inspired by Apache Spark's transformation operators that captured rich metadata including data frame identifiers, transformation types (Map, Apply, Join, Split, Merge), user-defined functions, and sequence order. This model stored both classic data catalog information and dynamic ETL transformation logic in a unified, queryable format.

**Implementation Strategy:** I designed Python-based automation scripts using boto3 and SQLAlchemy to batch-convert existing AWS Glue Catalog tables and RDBMS tables into graph data objects, enabling efficient initial population of the graph database from their heterogeneous data sources.

**Synchronization Mechanism:** I architected AWS Lambda functions to maintain graph data currency by scanning nodes against DDL metadata and comparing edge timestamps with ETL job updates, with SNS notifications triggering when the graph became stale.

**Application Framework:** I provided pseudocode examples demonstrating how to leverage Neptune's graph query capabilities to trace upstream ancestors and downstream descendants, enabling engineers to build custom analysis scripts for lineage investigation and impact analysis.

## Results

I delivered a complete architectural design with implementation roadmap that the customer adopted for internal development. The solution provided a flexible, automation-friendly approach that didn't require changes to existing data catalogs or ETL code, positioned them to build custom applications for business-specific lineage analysis, and established a foundation for addressing their legal compliance and operational transparency requirements.
  </markdown_content>
</document>
<document>
  <title>2021-09-13 - Build Lab - A leading U.S. financial services company</title>
  <markdown_content>
# 2021-09-13 - Build Lab - A leading U.S. financial services company

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

A leading U.S. financial services company specializing in life insurance, retirement plans, and investment products managed 100TB of customer data across DB2 mainframe and on-premises SQL Server systems. The company used Attunity to extract mainframe data in JSON format and AWS Data Migration Service (DMS) to extract SQL Server data in Parquet format, storing everything in Amazon S3. Their existing EMR-based ETL pipeline could only perform full data reloads taking 6 hours to 1 day, preventing timely analytics. Additionally, they lacked data access controls in AWS and wanted to implement Apache Airflow orchestration without managing infrastructure, but had limited expertise in Airflow scripting for ETL workflows.

## Task

I was assigned as the Data Lab Architect to design and build a production-ready ETL solution that could process change data capture (CDC) information incrementally in under 2 hours, implement database/table/column-level access controls, and orchestrate the pipeline using managed Airflow. Working on-site with the customer's data engineering team over 5 days, I needed to deliver a working proof-of-concept while simultaneously training their engineers to replicate and extend the solution in their AWS environment, navigating their Cloud Custodian compliance rules and network restrictions.

## Action

I designed and implemented a hybrid ETL architecture combining EMR and AWS Glue to separate initial load processing from incremental CDC processing. **Architecture Design:** I configured EMR clusters to use AWS Glue Catalog as the metadata store (rather than default Hive metastore) to enable seamless integration between EMR and Glue jobs, then built PySpark code in EMR Notebook to process the initial Attunity JSON data and DMS Parquet data. **CDC Implementation:** I developed AWS Glue jobs using Apache Hudi to create an "upsert zone" that maintained the most recent data snapshot by processing only CDC files, reducing processing time from 6+ hours to under 2 hours. I solved Glue Crawler include/exclude pattern issues by discovering the correct wildcard syntax for separating initial load from CDC data in the same S3 folders. **Data Access Control:** I implemented AWS Lake Formation with tag-based access control (LBAC), configuring IAM roles and testing database/table/column-level permissions through assumed roles to verify proper security isolation. **Orchestration:** I provisioned an Amazon MWAA cluster and created Airflow DAG scripts demonstrating ETL pipeline orchestration best practices, teaching the team how to coordinate EMR jobs, Glue jobs, and Glue crawlers. **Monitoring:** I integrated AWS CloudWatch and Amazon SNS for pipeline monitoring and failure notifications. Throughout the engagement, I recommended cost optimizations including using Spot instances for ad-hoc EMR jobs and consolidating workloads on shared clusters to reduce their significant EMR expenses from 20+ long-running on-demand clusters.

## Results

The customer team successfully built and deployed a hybrid ETL pipeline processing 100TB of data with 2-hour incremental updates, enabling timely analytics for business decision-making. They gained hands-on capability in Lake Formation access controls, MWAA orchestration, and Apache Hudi for managing data snapshots, with follow-up sessions scheduled for data quality validation strategies and scaling their Lake Formation permissions model.
  </markdown_content>
</document>
<document>
  <title>2022-10-17 - Build Lab - publicly-traded financial services holding company</title>
  <markdown_content>
# 2022-10-17 - Build Lab - publicly-traded financial services holding company

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

I was assigned to a publicly-traded financial services holding company specializing in reverse mortgages and home equity-based retirement solutions. The company's Reverse Mortgage division needed to optimize their lead distribution process by deploying a machine learning model that could score leads and route them to appropriate loan officers within 5 minutes to improve closing rates. The company was new to machine learning and had built a proof-of-concept logistic regression model on a local laptop but required a production-ready, low-maintenance ML service that could integrate with their HubSpot CRM system. The engagement faced significant environmental constraints: the customer's security policy required "VPC Only" mode with no public internet access, production data restrictions necessitated private subnet deployment, and the customer lacked a dedicated operations team for long-term ML service maintenance.

## Task

I was responsible for migrating the customer's existing lead scoring model from a local development environment to Amazon SageMaker, deploying it as a scalable API service, and creating a secure integration layer for their HubSpot CRM system to consume real-time predictions. Working on-site with the customer's analytics team over four days, I needed to validate that the deployed model's prediction accuracy matched the local model within 5% tolerance, establish token-based authentication for secure public internet access, and simultaneously train the customer's engineers to replicate and maintain the solution. The Build Lab format required me to develop the solution collaboratively while teaching the customer team the underlying AWS architecture patterns and best practices for production ML deployment in highly restricted network environments.

## Action

**Environment Configuration:** I first tackled critical infrastructure challenges by researching and identifying undocumented VPC endpoint requirements for SageMaker Studio in "VPC Only" mode. After discovering that the official AWS documentation was incomplete, I systematically debugged connectivity issues by creating VPC endpoints for Amazon S3, SageMaker API, SageMaker Runtime, AWS STS, and CloudWatch, ensuring Private DNS Domain names were enabled—requirements not mentioned in the standard documentation. I collaborated with the customer's security team and AWS support to resolve network configuration blockers that prevented SageMaker Studio IDE access and remote training job execution.

**Model Migration and Deployment:** I migrated the customer's scikit-learn logistic regression model to SageMaker Studio, refactoring the experimentation code into production-ready training scripts organized into four modular Python functions (data ingestion from Amazon S3, preprocessing and feature engineering, model training, and artifact serialization). I ran a comparative tutorial using a public Kaggle Titanic dataset with similar binary classification characteristics to demonstrate three ML training options: built-in algorithms, Script Mode with custom models, and bring-your-own-container approaches. After resolving dependency conflicts with the statsmodels library by manually building binary packages, I successfully executed a remote SageMaker training job and deployed the trained model to a SageMaker Endpoint, validating that prediction accuracy exactly matched the local model's confusion matrix results using the test dataset.

**Integration Architecture:** I designed and demonstrated a serverless proxy architecture using AWS API Gateway and AWS Lambda Functions with custom Lambda authorizers for token-based authentication, enabling secure public internet access to the private SageMaker Endpoint. I created a Python function using boto3 API that accepted raw lead records and returned predictions, then conducted a live 30-minute demo deploying the complete proxy server from scratch in an AWS sandbox account, showcasing automation-ready deployment with unit tests, integration tests, load tests, and CI/CD scripts. When the customer pivoted on day four to explore low-code solutions like SageMaker Canvas, I provided architectural guidance on how the proxy server approach would remain necessary regardless of the final ML deployment method, ensuring the knowledge transfer addressed both immediate and future implementation paths.

## Results

I successfully deployed the lead scoring model as a production-ready SageMaker Endpoint with exact prediction accuracy matching the local model, meeting all technical success criteria. The customer's engineers gained hands-on experience with SageMaker Studio development workflows, remote training job orchestration, and serverless API integration patterns using AWS API Gateway and Lambda Functions. I identified and documented critical gaps in AWS's official VPC configuration documentation for SageMaker, providing feedback to improve future implementations, and delivered a reusable reference architecture with complete automation scripts that the customer could adapt for additional ML use cases across other business divisions.
  </markdown_content>
</document>
<document>
  <title>Sanhe's Skill Point</title>
  <markdown_content>
# Sanhe's Skill Point



## Python

I have been working with Python since 2010, accumulating over 15 years of hands-on experience. My expertise spans the full spectrum of Python development, including advanced design patterns, language features, and virtually all mainstream third-party libraries. I consider myself a Python master with deep technical knowledge that goes well beyond typical senior developer capabilities.

As an active contributor to the Python open source community, I have authored and published over 160 open source libraries as of 2025, collectively generating more than 10 million downloads per month. Among these contributions, several flagship projects have been adopted as part of Amazon Web Services' official SDK ecosystem, where I serve as the primary author. My open source portfolio covers diverse categories including utilities, frameworks, and implementations of various architectural patterns.

I am a member of the Python Software Foundation, regularly participating in feature discussions and contributing to the language's evolution. My commitment to the Python ecosystem is evident in my GitHub activity, which shows nearly 10 consecutive years of daily contributions with over 10 commits per day, 365 days a year. This sustained contribution record demonstrates not just technical skill but dedication to continuous improvement and knowledge sharing.

My public GitHub repositories document over 1 million lines of Python code, all verifiable and accessible. This body of work establishes me not simply as an excellent Python developer, but as an influential figure within the Python community. My expertise encompasses both exceptional breadth across the Python ecosystem and significant depth in specialized areas, positioning me well beyond the typical Senior Python Developer level.

## AWS - Analytics

### AWS Athena

AWS Athena is one of my most frequently utilized AWS services, serving as a powerful query optimization tool for S3-based data lakes. I extensively leverage Athena's serverless architecture to query large volumes of Parquet files stored in S3, taking advantage of its cost-effectiveness and scalability. A key technique I employ is configuring Athena to output query results directly in Parquet format to S3, which enables efficient downstream processing. I've built custom SDK wrappers around Athena's API to transform it into a programmable query engine, allowing Lambda functions to access query results without exceeding memory constraints—since Lambda cannot directly scan large S3 datasets due to memory limitations.

My implementation approach treats Athena as an asynchronous query execution layer, where results are automatically persisted to S3 in optimized formats for subsequent programmatic access. I developed a custom Python library that abstracts Athena's native asynchronous API into a synchronous interface, enabling developers to use Athena queries as seamlessly as traditional database calls from any Python environment. This library handles the complexity of query execution polling, automatic Parquet result storage in S3, and efficient data retrieval, making Athena accessible for use cases that require synchronous workflows. The pattern has proven particularly valuable for serverless architectures where compute resources are constrained. While DuckDB has emerged as a viable alternative for smaller datasets since 2022 (handling approximately 90% of typical use cases with in-memory processing), Athena remains my first choice for extremely large datasets and scenarios requiring zero memory footprint within Lambda functions. I continue to architect solutions around Athena for petabyte-scale analytics where its distributed query engine and pay-per-query pricing model deliver superior performance and cost efficiency compared to memory-intensive alternatives.

### AWS Glue

**Glue Catalog**: AWS Glue Catalog is one of my most frequently leveraged metadata management services, serving as the centralized hub for organizing data across heterogeneous systems. I extensively use it to federate external data sources, enabling seamless cross-system querying and unified metadata governance. For dynamic schema management, I employ Glue Crawlers with statistical algorithms to automatically detect and update schema changes, ensuring catalog freshness in evolving data landscapes. For stable schemas, I optimize by performing one-time crawls or manual catalog generation to reduce operational overhead. To enhance catalog management capabilities, I developed an open-source Python library that treats Glue Catalog tables as programmable objects, providing a more agile alternative to Infrastructure-as-Code approaches like CDK for scenarios requiring rapid, ad-hoc table creation. My deep expertise spans DDL syntax, native and external data store integration, federated catalog configurations, and the critical role Glue Catalog plays as the foundation for Lake Formation's fine-grained access control—positioning it as the cornerstone of AWS's data governance ecosystem.

**Glue Job and DataOps Innovation**: During my tenure at AWS, I pioneered a DataOps framework that solved a critical challenge in the Glue ecosystem: the inability to perform quality control and unit testing on Spark jobs. I architected an original testing framework and delivered workshops to AWS customers, demonstrating how to implement unit tests using mock data within local development environments and integrate them into CI/CD pipelines. My approach leverages Glue containers in GitHub Actions to enable comprehensive testing workflows. I designed a three-phase architectural pattern that separates Glue jobs into Read, Transform, and Write operations, isolating transformation logic as the focus of unit testing while treating I/O operations as integration tests. By packaging transformation logic into modular Python libraries, I enable developers to mock DataFrames and validate expected outputs independently of data sources—a methodology that fundamentally transforms Glue job development practices. Beyond this innovation, I maintain expert-level proficiency in PySpark, Glue's native syntax, data manipulation, and SQL operations, consistently delivering production-grade Spark implementations. I extensively utilize Glue's serverless Spark capabilities for cost-effective, scalable data processing and leverage its seamless integration with modern lakehouse engines including Apache Hudi, Apache Iceberg, and Delta Lake to enable efficient upsert operations on data lakes.

**DataBrew for Business Users**: I have implemented AWS DataBrew solutions for non-technical business analysts in traditional industries, particularly healthcare organizations, where low-code interfaces are essential. I've designed and deployed exploratory data analysis environments and visualization dashboards on top of existing Glue infrastructures, enabling business teams to perform self-service analytics without requiring deep data engineering expertise. This experience complements my technical Glue capabilities by demonstrating my ability to bridge the gap between engineering teams and business stakeholders through appropriate tooling.

### AWS Kinesis

I treat AWS Kinesis Stream as a serverless alternative to Apache Kafka, with nearly identical architectural concepts: streams map to Kafka topics, shard iterators to offsets, and shards to partitions. I actively manage throughput scaling using shard split and merge operations, and design my applications to rely on Kinesis's FIFO ordering guarantees within individual shards while accounting for the lack of ordering across shards—exactly like Kafka's partition behavior. For high-frequency data ingestion scenarios, I implement batching and aggregation patterns inspired by AWS's Java Producer Library, including checkpointing and failure recovery to prevent data loss during batch assembly.

Since AWS only provides the advanced Producer and Consumer libraries in Java, I built open-source Python implementations that replicate the full functionality—giving Python developers access to enterprise features like automatic aggregation, checkpointing, and durable buffering. This has become a key part of my toolkit for building Python-based streaming applications. I routinely use Kinesis as middleware in ETL pipelines, particularly for change data capture where I stream database change events for real-time processing or route them through Data Firehose for batch analytics.

My production experience spans diverse streaming architectures: subscribing to DynamoDB streams for near-real-time data replication, processing relational database CDC logs for downstream analytics, and ingesting IoT sensor data where HVAC systems aggregate thousands of time-series readings per minute before streaming to Kinesis. These implementations demonstrate my ability to design high-volume, mission-critical data pipelines that leverage Kinesis's serverless scalability while maintaining proper ordering semantics and exactly-once processing where required.

### AWS Data Firehose

AWS Data Firehose serves as a fully managed data delivery service that eliminates the need to build and maintain custom streaming consumers. Rather than writing Lambda functions to consume from Kinesis streams or manually managing consumer state and error handling, Firehose provides pre-built delivery pipelines that handle all operational complexity. I primarily use Firehose to stream CloudWatch Logs to S3 for long-term archival or to OpenSearch for real-time log analytics, leveraging its native integration with these AWS services to build resilient data pipelines with minimal code.

My most common pattern involves streaming data from Kinesis to S3, where Firehose manages the complete delivery lifecycle—buffering, batching, compression, encryption, and retry logic—while allowing me to inject custom Lambda transformations for data enrichment or filtering when needed. This approach has proven invaluable for log aggregation and streaming analytics workloads, where Firehose's automatic scaling and serverless architecture deliver significant operational simplicity compared to managing dedicated consumer infrastructure. The pay-per-use pricing model and zero-maintenance overhead make it my default choice for reliable data delivery pipelines.

### AWS Lake Formation

AWS Lake Formation serves as the comprehensive data governance layer for the AWS data ecosystem, enabling fine-grained and row-level access control for any data cataloged in Glue. Once data is defined in Glue Catalog, Lake Formation can enforce granular permissions on all access patterns that flow through the catalog—making it essential for enterprise-scale data governance. I leverage Lake Formation extensively for cross-account and cross-organization data sharing scenarios, where precise permission management becomes critical for maintaining security boundaries while enabling collaboration.

Between 2022 and 2023, I delivered multiple Lake Formation implementations for large enterprises managing thousands of Glue Catalog tables across hundreds of AWS accounts. My work focused on establishing clear data governance frameworks that include comprehensive audit logging, access tracking, and data ownership models—ensuring that every dataset has a designated owner responsible for approving schema changes and access requests. During my time supporting Capital One and other enterprise clients as a subject matter expert, I provided technical guidance on Lake Formation adoption, helping teams navigate complex governance scenarios and optimize their permission strategies.

To streamline Lake Formation adoption, I developed an innovative "Data Access Control as Code" framework using a declarative approach. This solution wraps CDK functionality to enable users to define fine-grained access control and tag-based access control policies in a simplified format, which are then automatically deployed through infrastructure-as-code pipelines. This abstraction layer significantly reduces the complexity of onboarding Lake Formation, making enterprise data governance more accessible and maintainable for organizations transitioning to centralized access control models.

### AWS EMR (Elastic Map Reduce)

## AWS - Application Integration

### AWS EventBridge

### AWS SNS (Simple Notification Service)

### AWS SQS (Simple Queue Service)

### AWS StepFunction

## AWS - Business Application

### AWS Pinpoint

### AWS Simple Email Services

### AWS WorkDocs

### AWS WorkMail

## AWS - Compute

### AWS EC2

### AWS ECS (Elastic Container Service)

### AWS ECS Fargate

### AWS Batch

### AWS Lambda

## AWS - Container

### AWS ECR (Elastic Container Registry)

### AWS EKS (Elastic Kubernetes Service)

## AWS - Database

### AWS RDS (Relational Database Services)

### AWS Aurora

### AWS DynamoDB

### AWS Redshift

### AWS OpenSearch Service

### AWS DocumentDB

### AWS MemoryDB & ElastiCache

### AWS Neptune

### AWS Timestream

## AWS - Developer Tools

### AWS CloudShell

### AWS CodeCommit

### AWS CodeBuild

### AWS CodePipeline

### AWS CodeArtifact

## AWS - Machine Learning

### AWS Bedrock

### AWS Bedrock Agent

### AWS Bedrock AgentCore

### AWS SageMaker

### AWS Textract

### AWS Comprehend

### AWS Rekognition

### AWS Augmented AI

### AWS Kendra

### AWS Polly

## AWS - Management & Governance

### AWS CloudFormation

### AWS CloudTrail

### AWS CloudWatch

### AWS Organizations

### AWS Service Catalog

### AWS System Manager

## AWS - Migration & Transfer

### AWS DMS (Database Migration Service)

### AWS Transfer Family

## AWS - Networking & Content Delivery

### AWS VPC

### AWS Transit Gateway

### AWS Route53

### AWS Cloudfront

### AWS API Gateway

### AWS Direct Connect

## AWS - Security, Identity & Compliance

### AWS IAM

### AWS KMS (Key Management Service)

### AWS Secret Manager

### AWS Directory Service

### AWS Certificate Manager

## AWS - Storage

### AWS S3

### AWS EBS

### AWS EFS

### AWS FSx

  </markdown_content>
</document>
<document>
  <title>2022-06-27 - Design Lab - global institutional investment management firm</title>
  <markdown_content>
# 2022-06-27 - Design Lab - global institutional investment management firm

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

I was assigned to a global institutional investment management firm specializing in hedge funds and asset management for pension funds, endowments, and central banks. The firm needed to modernize their document search capabilities for 10 million internally-generated analytics documents, with each document averaging 10 updates per day (100M updates daily). Their existing architecture relied on self-managed OpenSearch and Redis infrastructure, but they sought a fully-managed AWS solution to reduce operational overhead. The engagement faced typical Design Lab constraints: a two-day timeframe to architect a comprehensive solution without code implementation, requiring deep dives into Amazon Kendra's capabilities, capacity limits, and integration patterns with their existing Access Control List (ACL) system.

## Task

As the assigned Data Lab Architect, I was responsible for designing a complete serverless data architecture that would ingest S3-stored documents into Amazon Kendra, enable ACL-based search capabilities integrated with the firm's internal authorization system, and support near-real-time searchability (30-60 seconds) with exact-once delivery guarantees. I needed to evaluate Amazon Kendra against OpenSearch for this enterprise use case, resolve mission-critical questions not covered in AWS documentation through hands-on experimentation, and deliver a production-ready architectural blueprint that the customer's engineering team could implement independently. Additionally, I needed to design mechanisms for syncing 10,000 reader groups across their internal ACL system with Kendra's token-based access control, and architect solutions for both initial bulk loading and incremental updates.

## Action

I designed a comprehensive serverless data ingestion pipeline using **Amazon Kendra as the core search engine, AWS Lambda for ETL and orchestration, Amazon Kinesis Data Streams for event aggregation and batching, Amazon DynamoDB as a metadata store for tracking document indexing status, and Amazon S3 for document storage**. **Architecture Design:** I implemented a coordinator-worker pattern where S3 Put Events triggered Lambda functions that forwarded events to Kinesis Data Streams. The stream aggregated events into batches to maximize write throughput using Kendra's `batch_put_document` API, then triggered concurrent Lambda workers that created DynamoDB tracker entries, sent asynchronous batch indexing requests to Kendra, and updated status based on responses. I designed a scheduled error-handling system with a coordinator Lambda (running every minute) that queried DynamoDB for pending/failed documents and delegated recovery tasks to worker Lambdas.

**ACL Integration:** I architected a token-based access control solution where document metadata included ACL groups that were passed to Kendra's `batch_put_document` API via the UserContext parameter, while the search application retrieved user ACL groups from the internal system and passed them to Kendra's query API, enabling hierarchical permission inheritance across the organizational structure.

**Sync Mechanism:** I designed an initial sync process using batch ETL to map documents to ACL groups stored either in S3 metadata files or DynamoDB, then architected an incremental sync where document ownership changes trigger new document versions with updated ACL metadata that flow automatically through the ingestion pipeline.

**Critical Research:** I conducted experiments to resolve undocumented API behaviors, proving that `batch_put_document` returns partial failures rather than rejecting entire batches when size limits are exceeded, and verified that error messages only expose document IDs without leaking content to CloudWatch Logs.

**Comparative Analysis:** I delivered a comprehensive Kendra vs. OpenSearch evaluation across eight dimensions (capacity limits, search capabilities, operational effort, data ingestion, security, and cost), demonstrating that while Kendra had higher AWS resource costs, it eliminated the need for three senior engineers to build and maintain infrastructure, supported ML-powered relevance ranking and NLP query interpretation, and provided out-of-the-box connectors for enterprise document management systems like SharePoint. I documented capacity planning showing Kendra could scale to 10M documents with 100M daily updates using 100 capacity units, and architected a catch-up mechanism for bulk historical data using Kinesis `put_records` API.

## Results

I successfully delivered a production-ready architectural blueprint that met all requirements for near-real-time document search with exact-once delivery, ACL integration, and sync mechanisms. The customer gained clarity on Kendra's undocumented behaviors through my experimental validation, received a detailed cost-benefit analysis that informed their technology stack decision, and obtained comprehensive Build Lab preparation guidance including specific AWS service configurations and integration patterns ready for POC implementation.
  </markdown_content>
</document>
<document>
  <title>2023-09 to 2024-06 - Enterprise AI Observability Platform</title>
  <markdown_content>
# 2023-09 to 2024-06 - Enterprise AI Observability Platform



## Overview

Travelers Insurance needed enterprise-grade observability for its rapidly expanding AI applications, but faced a difficult landscape: monitoring tools were still immature, and our highly regulated environment imposed severe constraints. After a systematic evaluation of commercial platforms, open-source tools, and custom-built options, we implemented a hybrid solution built on Arize Phoenix SDK. The result: complete visibility into 5 million monthly AI inference calls, comprehensive auditing and cost tracking across 10 production applications (covering 80% of our AI workload), and a 40% reduction in inference costs through data-driven optimization.

## Background

### Travelers Insurance's Enterprise Environment

Travelers Insurance operates as one of the nation's leading insurance carriers, with cloud infrastructure spanning more than 50 AWS accounts. Our technology foundation includes Amazon EKS for internal API services, S3 for data storage, DocumentDB for unstructured data, Aurora PostgreSQL for relational workloads, and Lambda for microservices. We selected AWS Bedrock as our AI platform specifically because it keeps all customer data within our own AWS environment—nothing ever leaves our enterprise perimeter.

The ChatGPT launch in November 2022 sparked industry-wide recognition of generative AI's transformative potential. By mid-2023, Travelers launched a comprehensive AI transformation initiative aimed at automating business processes and driving operational efficiency. We stood up an Enterprise Architect Team of over 20 architects, three dedicated to AI architecture. I joined this team on September 4, 2023 as an Enterprise Architect, reporting to our VP of Architecture, who reported directly to the SVP & CIO of Enterprise Tech Solutions.

My background at AWS as a Data/AI Solution Architect gave me deep expertise in AWS services. During that time, I worked with Travelers on three consulting engagements, including an Intelligent Document Processing solution leveraging OCR and AI entity extraction. These collaborations led to Travelers inviting me to join their enterprise AI team full-time.

**The Urgent Need for AI Observability**

AI adoption accelerated quickly across the organization. Three business units—Underwriting, Business Intelligence, and Claims—had more than 20 application teams integrating AI capabilities. Underwriting and Claims drove the highest call volumes, supporting mission-critical workflows. Underwriting alone ran multiple AI-powered applications: a Business Research tool for web-based company intelligence, systems pulling historical policy data for risk analysis, and more. In insurance, compliance isn't optional—it's existential. Even without explicit regulatory mandates for AI audit trails, we needed to get ahead of this. More immediately, we had a burning operational need: visibility into AI resource consumption. How many tokens were we burning through? What were our actual costs? Which applications were heaviest users? And critically, we needed access to inference logs to evaluate prompt quality and measure AI effectiveness.

## Situation

When I took ownership of this initiative, Travelers faced a fundamental problem: AI applications were proliferating rapidly, but we had zero visibility into—and therefore zero control over—AI usage.

### Complete Lack of Visibility

Our 20+ AI applications were generating millions of monthly inference calls, but we had almost no instrumentation around these calls. Development teams took wildly different approaches—some used LangChain, others called Bedrock APIs directly, still others employed different frameworks entirely. At best, when an inference happened, a developer might log a line to the application logs. No structured data capture. No systematic storage. The result: we couldn't answer basic questions. Token consumption per application? Unknown. Cost attribution by team or department? Impossible. A centralized repository for querying inference inputs and outputs? Didn't exist. Prompt effectiveness evaluation? No way to do it. And if regulators walked in tomorrow demanding an AI usage audit? We had nothing to show them.

### The Risk of Runaway Costs

Even more concerning was the cost trajectory. AI spend kept climbing, but Finance couldn't get meaningful breakdowns. Money was going out the door, but we couldn't answer fundamental questions: Which department? Which application? Which team? Was growth driven by increased business volume, or were we wasting tokens on poorly-designed prompts? At enterprise scale, operating blind like this wasn't sustainable.

### Strict Technical Environment Constraints

Travelers operates in a highly regulated environment with stringent security requirements across 50+ AWS accounts. Our network architecture and security policies are extremely strict—deploying any new service means satisfying enterprise security standards and network isolation requirements. We also had no standardized approach to AI development across the company. Different teams used different frameworks and integration patterns. Any solution we built had to accommodate this heterogeneous landscape.

On top of technical constraints, procurement posed another hurdle. Bringing in any third-party tool required vetting by Security, Legal, and Procurement—a 1-3 month process. We had to get solution selection right the first time. There was no room for trial and error.

### Immature Market Tools

The AI observability market was just emerging in late 2023. Commercial options like LangSmith, Arize, and Fiddler, along with open-source alternatives like LangFuse and OpenLLMetry—none had reached 1.0. For a large, risk-averse insurance enterprise, tool maturity and reliability were non-negotiable. We needed systematic evaluation to determine what would actually work in our environment.

### The Architect's Mission

My job as Enterprise Architect was to cut through this complexity—systematically evaluate everything (commercial platforms, open-source tools, custom builds, hybrid approaches)—and design a solution that met immediate needs while positioning us for future growth. This wasn't purely a technical challenge. It required balancing competing constraints, managing stakeholder expectations, and delivering value fast.

## Task

My mandate: design and deploy an enterprise-grade AI observability platform for Travelers. Timeline: September 2023 through June 2024—nine months total. I'd be leading a 9-10 person engineering team, building a complete solution from the ground up in a tightly constrained environment, delivering audit compliance, cost transparency, and quality analytics.

### Core Objectives

We needed to satisfy three distinct requirement layers:

**Audit Compliance**: Build a system ready for future regulatory scrutiny. Capture every AI call's essential metadata—timestamp, user identity, model used, input prompt, output response. Store it securely with fast query and export capabilities.

**Cost Visibility**: This was the burning platform. Track every single inference call: token consumption, cost per call, aggregated across every dimension that mattered—application, team, department, time period. Finance needed clear cost attribution. Business units needed to understand their AI spend.

**Quality Analysis**: Preserve complete inference logs so business teams could analyze prompt-response quality retroactively, evaluate AI effectiveness, refine prompt design, and spot anomalies. For applications using templated prompts, we needed full chat history visibility to validate prompt performance.

### Roles and Responsibilities

I owned the entire delivery chain from architecture through production deployment:

- **Systematic Evaluation**: Survey commercial platforms, open-source tools, and build-it-yourself options. Build a structured evaluation framework for head-to-head comparison.
- **Solution Design**: Based on enterprise constraints and evaluation findings, architect the optimal technical approach and implementation path.
- **POC Validation**: Write prototype code to validate critical technical decisions before committing the team.
- **Team Leadership**: Lead 9-10 engineers through development and deployment. Write template code and build team capability.
- **Cross-departmental Coordination**: Work with Security, Legal, Procurement, and business teams to drive approvals and integration.

### Constraints

**Technical Environment**: Build on Travelers' existing AWS infrastructure. Comply with strict network isolation and security policies. All data stays within our AWS account boundaries.

**Integration Complexity**: Support 20+ application teams with wildly different tech stacks—LangChain users, raw Bedrock API callers, various other frameworks. Some legacy apps already logging to MongoDB needed smooth compatibility.

**Time Pressure**: Complete evaluation, design, and production launch within 9 months. Fast decisions. Efficient execution.

**Approval Process**: Any third-party tool introduction meant 1-3 months of rigorous vetting. Needed thorough documentation, POC demos, and sandbox environments ready upfront.

Success looked like this: By project end, all core AI applications automatically instrumented. Finance getting accurate cost reports. Business teams accessing and analyzing inference logs. System running reliably with capacity for future expansion.

## Action

I structured the 9-month timeline into three phases: two months for systematic evaluation, one month for solution design and approvals, and six months for phased implementation.

### Establishing a Three-Dimensional Evaluation Framework

Before comparing vendors, I built a structured evaluation framework. Three core dimensions:

**Data Ingestion Capability**: What data points can each solution capture? We needed prompt, response, token count, latency, model name, application name—all the critical metadata. Vendors varied widely in coverage. I also needed to understand the capture mechanism: SDK decorators? API interception? Agent-based collection? How intrusive was each approach to existing application code?

**Capability Matching**: Did each solution's features align with our requirements? We had two priorities: long-term archiving with flexible querying (think data lake for inference logs), and robust support for prompt optimization plus chat history analysis. Real-time monitoring and alerting weren't priorities—historical analysis power was what mattered.

**SDK Integration Complexity**: How hard would it be to integrate with our 20+ existing AI applications? I needed to actually write integration code, test extensibility, evaluate framework compatibility, and measure code change requirements. Critical questions: Was the SDK open for customization? What's the vendor lock-in risk? Could we integrate with our AWS services—CloudWatch, S3, OpenSearch?

### Systematic Research and Horizontal Comparison

With the framework established, I spent two months on systematic research. Evaluation targets: three commercial platforms (Arize, Fiddler, and our existing microservices monitoring vendor Dynatrace), plus open-source and AWS-native options (AWS Kinesis + CloudWatch, OpenSearch + Kibana, OpenLLMetry).

I engaged each vendor in multiple deep-dive sessions. Rather than passively watching sales pitches, I brought actual code—samples mimicking our real application scenarios—and asked vendors to demo against these. This approach caught my own assumptions early. Sometimes what I'd inferred from documentation turned out wrong, and vendors would show me better implementation patterns during live demos.

What the comparison revealed:

**Arize** delivered the most balanced performance across all three dimensions. They offered bring-your-own-cloud deployment, letting us keep data in our AWS environment while using their SaaS UI for visualization. More importantly, Arize's open-source Phoenix SDK was highly extensible—we could customize and modify it freely. Functionally, Arize excelled at prompt optimization and request-response chat history analysis, hitting exactly what we needed. Integration-wise, Phoenix SDK was lightweight with minimal vendor lock-in risk.

**Fiddler and Dynatrace** had strong capabilities but heavier lock-in. Their SDKs were relatively closed with limited customization options. While Dynatrace was already our microservices monitoring vendor, its AI observability features weren't mature enough for our needs.

**Open-source and AWS-native solutions** offered maximum flexibility but required building everything from scratch—data collection, storage, query, visualization—the full stack. Given our 9-month deadline and team size, pure custom build carried too much risk.

Beyond my technical evaluation, I organized hands-on trials with stakeholder engineers, collecting feedback on usability. Arize came out on top—engineers found it easiest to operate and understand.

### Designing a Hybrid Architecture

Based on the evaluation, I chose Arize Phoenix SDK as the foundation—but designed a hybrid architecture to address our unique requirements. The architecture had to solve three problems: bidirectional data flow, legacy system compatibility, and AWS-native service integration.

Here's how data flows: All our AI applications run on Python. I customized Phoenix SDK to implement dual-write—capturing data and sending it to two destinations simultaneously. In application code, we use Python decorators to mark AI functions for monitoring. When these functions execute, Phoenix SDK captures all critical data points (prompt, response, token count, latency, model name, application name), then writes to both targets:

First target: Arize platform. Phoenix SDK sends data to Arize's SaaS UI, leveraging their powerful visualization and prompt optimization capabilities.

Second target: Our AWS data lake. Data first lands in CloudWatch Logs for short-term storage and fast queries (with a short retention period). From there, Kinesis Firehose streams everything to S3 for long-term archiving—meeting compliance and legal hold requirements. S3 data then flows into OpenSearch, our long-term query and analysis engine.

For legacy compatibility, I built a backward-compatible solution. One or two legacy Underwriting applications were already logging AI data to MongoDB. Rather than forcing immediate migration, I added a MongoDB adapter layer in Phoenix SDK so we could query legacy data alongside new data uniformly. New applications use OpenSearch exclusively.

### POC Validation and Template Code

With the design complete, I didn't roll out broadly—I chose the highest-risk, most complex application for proof-of-concept validation. Target: Underwriting's Business Research application. This app does public web research, handles high call volumes, runs complex business logic, and uses multiple AI models. If we could nail this integration, everything else would be easier.

I wrote all the POC code myself. Deep customizations to Phoenix SDK: extended data ingestion to support dual-write, modified data formats for seamless CloudWatch and S3 integration, added adapter layers for different AI frameworks (LangChain, raw Bedrock API). Phoenix SDK's openness gave me the extension points I needed—moderate difficulty, totally doable.

After successful POC, I packaged the code as a template—a reference implementation for other application integrations. The template clearly demonstrated: how to use decorators to mark AI functions, how to configure dual-write destinations, how to handle different data formats, and how to implement error handling and performance optimization.

### Progressive Rollout and Team Enablement

The final six months: phased rollout with my 9-10 person engineering team. Backend engineers handled SDK and telemetry integration in applications. DevOps engineers built data pipelines and dashboards.

My rollout strategy started with simpler applications—but with one deliberate exception. Early on, I tackled those legacy MongoDB-based applications even though they were highest risk. Why? If we could prove the solution worked in the hardest scenarios, team confidence would skyrocket.

I personally completed the first three integrations, including the complex Underwriting Business Research app. This accomplished two things: validated solution viability and trained the team. I ran detailed code reviews, explaining architecture rationale, implementation details, and gotchas to watch for. Gradually, other engineers developed the ability to understand the codebase architecture, integrate new applications independently, and review each other's code.

As architect, I needed to focus on higher-leverage work. I progressively delegated daily integration and code review to the team, concentrating instead on solving gnarly technical problems, communicating with Tech Leads across teams, and driving the approval process.

### Driving Cross-Team Collaboration

AI observability integration was a company mandate—technically required. But that doesn't make rollout automatic when you're dealing with 20+ application teams. My approach: **focus on Tech Leads, not entire teams**. I'd have in-depth technical conversations with each Tech Lead, walk through template code, explain integration value and simplicity, and help them get started. Once a Tech Lead bought in, they'd drive execution within their team.

Resistance came from two places: **code compatibility issues** (some apps had unusual structures needing adapter work) and **internal approval complexity**. For code issues, my team provided hands-on support solving specific integration problems. For approvals, I prepared comprehensive packages: POC code, demo environments, sandbox AWS accounts, security assessment reports—everything needed to sail through Security, Legal, and Procurement review in one shot.

The rollout followed a progressive adoption pattern: high-traffic, high-value applications first, then expand to others. Each step deliberate and solid.

## Result

Nine months later, we'd established enterprise-grade AI observability capabilities at Travelers.

### Covering Core Business Traffic

By June 2024, **10 of 20 AI applications had integrated with the observability platform. That's 50% by count, but 80% by workload**—all core Underwriting and Claims applications included. **The platform now captures roughly 5 million AI inference calls every month**, with complete records: prompts, responses, token usage, latency, model information—the full metadata set.

### Establishing Cost Visibility

Finance finally got the cost transparency they needed. Through trace ID tracking, we can calculate token consumption and cost for every step in every AI workflow. Example: for a four-step AI workflow, we can show exactly which application ran each step, which model it called, token consumption, and cost generated. Data aggregates across any dimension that matters—application, team, department, time period. Finance has clear cost attribution and trend analysis. Finally.

### Enabling Data-Driven Cost Optimization

With visibility came optimization opportunities. Analyzing token usage patterns revealed clear inefficiencies: **simple tasks like summarization and entity extraction were hitting expensive premium models when cheaper options (Claude Haiku) would work fine. Complex tasks like strategic planning and deep research kept using premium models—appropriate for the quality requirements**.

I worked with business teams on model downgrade experiments. Results: for simple tasks, cheaper models performed nearly identically but cost dramatically less. **Selectively downgrading models across applications drove a 40% reduction in overall AI inference costs**. This optimization only became possible with complete cost visibility.

### Supporting Auditing and Quality Analysis

Complete inference logs—preserved, structured, queryable. All AI calls captured: prompts, responses, timestamps, user identities, model information. Exportable anytime. Regulatory requirements will come eventually; we're ready now.

Business teams can now retrospectively analyze prompt quality. View complete chat histories for specific applications. Evaluate prompt effectiveness. Spot anomalies. Refine prompt design. Data-driven quality improvement—now possible.

## Learnings

### What Went Well

**The evaluation framework paid off**. Two months felt slow during evaluation, but it ensured we made the right call. Rushing into a solution without thorough vetting could have forced an expensive mid-project pivot. The three-dimensional framework kept evaluations objective—vendor presentations didn't sway us.

**The hybrid architecture hit the sweet spot**. Phoenix SDK plus our AWS data lake combined the best of commercial platforms and custom builds. We got Arize's powerful analytics UI while keeping data sovereignty and control. If Arize gets too expensive or unstable, we can build our own UI—the data's already in our environment. This architecture delivered fast time-to-value without vendor lock-in.

**POC-first reduced risk**. Tackling the most complex application (Underwriting Business Research) first validated feasibility in the hardest scenario. Later integrations hit simpler problems, team confidence stayed high. Starting with easy applications might have hidden critical issues until it was too late.

**Template code accelerated adoption**. After writing the POC, I packaged it as template code. This dramatically lowered the barrier for other engineers integrating new applications. Pattern-based examples beat lengthy documentation—engineers could reference working code directly.

**Tech Lead engagement worked**. Rather than convincing entire teams or negotiating with middle managers, I focused on Tech Leads. Once they understood integration value and simplicity, they drove execution within their teams. Far more efficient than broad evangelization.

### What Could Have Been Better

**Should have secured executive sponsorship earlier**. The project had company backing, but a VP-level champion from day one would have smoothed many obstacles. I figured this out mid-project and started reporting progress directly to the VP—which helped, but earlier would have been better.

**Did too much hands-on work initially**. First two months, I wrote all POC code and handled most integrations myself. Quality stayed high, but I became a bottleneck. Earlier team training would have enabled more parallel work, possibly completing more application integrations.

**Success criteria needed clearer alignment**. Early on, I assumed 100% application coverage was the goal. When we hit 10 of 20, I felt incomplete. In retrospect, 80% business value coverage with 50% of applications was a huge win. Should have aligned with leadership earlier on what "success" actually meant.

**MongoDB compatibility created technical debt**. To get legacy apps online fast, I built a MongoDB adapter layer. Worked short-term but left debt. Ideally those apps would fully migrate to the new architecture, but I chose backward compatibility for speed. That debt eventually needs paying.

### What I Didn't Complete

- Wanted all 20 applications integrated; completed 10. The remaining 10 were mostly low-traffic edge apps with lower priority. Some were mid-refactoring—better to integrate post-refactoring anyway.
- Planned real-time alerting (cost anomalies, error rates). Ran out of time. Deferred to future roadmap.
- MongoDB compatibility is technical debt. Those legacy apps should eventually migrate fully. Chose backward compatibility for speed—debt to repay later.

**Personal Reflections**:

- Enterprise environments rarely favor pure technical solutions. Hybrid approaches may lack elegance, but they balance technology, time, cost, and team capabilities. That's pragmatism.
- I over-indexed on technical details early, underweighting communication and adoption. Even perfect technical solutions fail without buy-in. Later focus on stakeholder management yielded better results.
- Would establish executive sponsorship from day one next time. Company strategy backing helps, but a high-level champion opens doors. Realized this mid-project—VP support made things easier.

**Impact on Travelers**:

- My internal visibility jumped. Other teams started consulting me on AI architecture. Gradually became the company's go-to AI architect.
- Project success enabled subsequent AI initiatives. With observability infrastructure in place, the company could advance AI applications more confidently—everything's under control.
- Finance was thrilled. Could finally explain AI ROI clearly to the CFO. Critical for proving AI value and securing future budget.
  </markdown_content>
</document>
<document>
  <title>2022-05-02 to 2024-03-08 at SoundExchange</title>
  <markdown_content>
# 2022-05-02 to 2024-03-08 at SoundExchange

In May 2022, I joined **SoundExchange (SX)** on a contract. SX is a **music rights organization** responsible for collecting and distributing royalties from streaming platforms, live performances, and other music-related activities.

For example, **Spotify** sends monthly music usage data to SX, which then analyzes the data and distributes royalties to the rightful owners—typically **artists**, **labels**, and **rights holders**.

I was hired as a **Data Engineer**, focusing on building **ETL pipelines**, **data lakes**, and **data analytics solutions** to support these processes.
  </markdown_content>
</document>
<document>
  <title>2023-09 to 2025-10 - Multi-Agent Solution Evolution</title>
  <markdown_content>
# 2023-09 to 2025-10 - Multi-Agent Solution Evolution



## Overview

Over 18 months, I built Travelers Insurance's enterprise Multi-Agent infrastructure through four major technology shifts. This wasn't about picking the "right" framework - it was about staying ahead of rapidly evolving AI technology while protecting our investments along the way.

Early on, I designed a three-layer abstraction that became the key to everything. When tech stacks changed, we swapped out the framework layer but kept the business logic and tools intact. We built up real assets - open source libraries, SDKs, design patterns - and created standardized approaches for Agent development that work across the company.

## Background

Travelers Insurance runs one of the largest insurance operations in the US, with 50+ AWS accounts powering everything. We're talking EKS for all internal APIs, S3 for storage, DocumentDB for unstructured data, Aurora PostgreSQL, and Lambda supporting our microservices architecture.

I joined as an Enterprise Architect in September 2023, one of three architects focused on AI in our 20+ person architecture team. I reported to our VP of Architecture, and managed about 10 engineers handling implementation. Before joining, I'd spent years at AWS and had already done some successful consulting work for Travelers - I knew both the tech and the organization.

### Why Multi-Agent Systems Matter

Large language models evolved fast. AI Agents went from basic tool-calling to sophisticated Multi-Agent systems that make autonomous decisions and work together. For complex enterprise processes, you need this kind of setup. One Agent extracts data, another analyzes it, a third generates reports. Breaking tasks down this way makes everything more maintainable and scalable.

Travelers had dozens of perfect use cases: claims processing, risk assessment, customer service, business automation. But Multi-Agent tech moved incredibly fast between 2023 and 2025. We went through LangChain's workflow orchestration, then cloud-managed Agent services, then new open-source frameworks, and finally AWS Agent Core's completely redesigned architecture. The entire tech landscape shifted in two years.

Here's the real problem: how do you build a solid Multi-Agent strategy when the technology won't sit still? You can't just pick a tech stack and call it done. You need an architecture that can evolve.

## Phase 1: LangChain + Temporal Cloud - Building the Foundation with Three-Layer Architecture

### Situation

Late 2023 was the wild west of AI Agents. ChatGPT had just launched, and even LangChain wasn't mature yet. We were exploring Tool Use but hitting some serious walls.

Our AI Workflows ran on a homegrown Java engine called Marvin. We'd built it because the company had deep Java expertise and tons of orchestration infrastructure already written in Java. Sounded good at the time. But as AI evolved, Marvin became a nightmare - expensive to maintain, hard to extend, and the developer who built it had left the company. We'd lost all the tribal knowledge.

The bigger issue? Everyone could see AI tech wasn't going to slow down. Whatever Workflow framework we picked would eventually need replacing. If we coupled business logic tightly to the framework, every upgrade would mean rewriting everything from scratch.

The Underwriter Policy Team was our main stakeholder. Their AI apps were already running on Marvin and desperately needed migration. They wanted modern tech but were worried about downtime and migration costs.

### Task

I needed to migrate all our AI Workflows from Marvin to a modern orchestration platform. But that was just table stakes. The real challenge was designing an architecture that wouldn't force us to rewrite everything when the next tech shift came around.

Here's what success looked like:

- Pick a better Workflow engine and execute the migration
- Design patterns that decouple business logic from orchestration frameworks
- Build SDKs and tooling that make migration and development easier for teams
- Create reusable templates and best practices so new projects can spin up fast

Getting this migration done was important, but setting up an architecture that could handle multiple future evolutions - that was the real goal.

### Action

**Choosing Temporal Cloud**

I picked Temporal Cloud to replace Marvin. Think of it like AWS Step Functions or Apache Airflow, but more advanced and flexible - especially for complex state management and long-running Workflows. After evaluating multiple orchestration tools, Temporal came out on top for enterprise reliability, developer experience, and extensibility.

**The Three-Layer Architecture**

This was the breakthrough. AI tech changes too fast - we needed an architecture where Workflow frameworks, orchestration logic, and underlying Tools could all change independently.

**Layer 1: Python Tool Library (Command Pattern)**

The foundation - completely business-agnostic utility functions. I wrapped each Tool as an independent Python package using Command Pattern:

- Input: Parameters bundled in a class
- Main: Core logic in its own method
- Output: Standardized structure

This made extension trivial. Inherit, override the Main method, insert Mixins through hooks to modify behavior. The Input/Output structure works with any orchestration framework - these Tools can move between frameworks without changing.

Database operations became ORM tools, document processing became standalone functions. Each Tool handles 90% of generic use cases while staying extensible. Everything versioned and distributed through our Nexus Repository.

**Layer 2: Orchestration Logic (Pure Business Logic)**

The middle layer - just orchestration logic. Function calls and conditionals. Take the previous step's output, feed it to the next step, use if-else for flow control. Since Layer 1 provides complete capability, this stays clean - just logic, no implementation details.

**Layer 3: Framework Adapter**

The top layer - lightweight wrappers for specific frameworks. Temporal, Step Functions, LangChain, whatever. This layer injects the framework's mechanics into the orchestration logic. It's thin by design. When frameworks change, you only touch this layer.

**Why This Works**

Complete separation means:

- Tool changes? Only Layer 1
- Flow adjustments? Only Layer 2
- Framework upgrades? Only Layer 3

Migrations stay smooth, and we're ready for whatever comes next.

**Building the SDK and Toolchain**

I turned the three-layer architecture into a complete SDK - the "AI Workflow Orchestration Framework." Not just the abstractions, but automated CI/CD pipelines that publish Python packages to Nexus, development templates for quick starts, and detailed docs with code examples.

**Migrating the Underwriter Policy Team**

We used this architecture and tooling to migrate the Underwriter Policy Team's AI apps from Marvin to Temporal. Most business logic stayed intact - we focused on rewriting the adaptation layer and optimizing flows.

### Result

We completed the migration from Marvin to Temporal Cloud smoothly. The Underwriter Policy Team's AI apps ran stable on the new platform. After each phase, 3-5 new applications adopted the latest framework - continuous momentum.

The three-layer architecture became the foundation for everything that followed. Over the next three phases, no matter how tech stacks changed, the Tool Library and orchestration logic stayed intact. Migration work dropped dramatically each time.

Key technical assets:

- AI Workflow Orchestration Framework (three-layer architecture SDK)
- Temporal Cloud integration toolchain
- CI/CD pipeline for Python package management
- Reusable development templates and documentation

The real win wasn't shipping apps - it was establishing an architecture philosophy. In fast-moving tech environments, abstraction and decoupling beat specific tech choices. This philosophy guided every decision through the next three phases.

## Phase 2: Bedrock Agent + MCP - Agent Reuse and Open Integration

### Situation

By mid-2024, AI Agents were getting smarter. We'd moved past simple Workflow orchestration into autonomous decision-making territory. New challenges emerged.

Phase 1 solved deployment and migration, but had limits. Each deployed app worked in isolation - business logic tied to that specific app, Agent capabilities couldn't be shared. Teams kept building the same things over and over.

Worse, Agents weren't actually that intelligent. They could execute predefined Workflows but couldn't plan autonomously or adjust based on context. Still basically upgraded Workflows, not real Agents.

Amazon Bedrock Agent changed the game. Fully managed deployment, standardized Runtime API interfaces, stronger autonomous planning. Time to rethink the architecture.

### Task

I needed to upgrade our AI infrastructure to Agent-as-a-Service. Make Agent capabilities standardized and reusable. Here's what that meant:

- Convert our existing Tool Library into Action Groups that Bedrock Agent could call
- Build an Agent Catalog so Agents became discoverable, reusable resources
- Design Agent-as-a-Tool architecture where Agents could call each other
- Solve integration between Bedrock Agent and other ecosystems like ChatGPT and MCP

The deeper goal? Figure out how to make Agents developed once work everywhere - multiple apps, multiple client types.

### Action

**From Python Library to Lambda Functions**

The Command Pattern from Phase 1 made this easy. Each Input Object mapped directly to Lambda Event, Main method became Handler logic, Output structure stayed the same. We wrapped all our existing Tools as Lambda Functions and configured them as Bedrock Agent Action Groups.

**Building the Agent Catalog**

I designed registration and discovery for Agents:

- Standardized GitHub repo structure - each Agent got its own repository
- Agent Catalog page on Confluence with all available Agents in table format
- Complete metadata for each Agent: what it does, API interface, usage examples, owning team

The Catalog wasn't just asset management - it drove reuse. Teams building new apps would check the Catalog first instead of rebuilding things.

**Agent-as-a-Tool Architecture**

Bedrock Agent's Runtime API meant Agents could be called like regular Tools. I used this to design inter-Agent calling. Complex business processes could be handled by multiple specialized Agents working together, each focused on what it does best. Early Multi-Agent thinking in action.

**Open API Adapter for Bedrock Agent: Solving Integration**

I hit a critical pain point in practice. Bedrock Agent Lambda Functions used a specific Payload format - they only worked with calls from Bedrock Runtime API. Want ChatGPT's Tool Use or other frameworks calling the same Lambda? Payload formats didn't match. No reuse possible.

Our Agents got locked into the Bedrock ecosystem, couldn't integrate with other AI tools. This violated the whole "develop once, use everywhere" principle.

I built an innovative open-source library - "Open API Adapter for Bedrock Agent" - that solved this:

- Modified the Open API Spec Generator to keep interface definitions clean and generic
- Added smart Adapter logic to Lambda Functions that recognizes where requests come from: Bedrock Agent Runtime, ChatGPT Tool Use, MCP protocol, other frameworks
- Automatically converts Payload format based on source, bidirectionally adapting between Bedrock's nested format and generic API format

With this Adapter, we deploy Lambda once and it works everywhere:

- Exposed as standard API through API Gateway or Function URL
- Called by ChatGPT's Tool Use
- Integrated by MCP clients
- Used by Bedrock Agents

True "develop once, use everywhere." I released this as open source - solved our problem and helped the broader community.

**Early MCP Adoption**

I started introducing MCP (Model Context Protocol) standard to unify tool integration interfaces. MCP wasn't mainstream yet, but I could see where things were heading. Built MCP integration points into the architecture early.

### Result

We established enterprise-level Agent reuse. Agents in the Catalog became shared technical assets across teams. After each phase, 3-5 new apps adopted this architecture - standardized Agent development patterns taking hold.

Key technical assets:

- Agent Catalog (GitHub repo architecture + Confluence registry)
- Open API Adapter for Bedrock Agent (open-source library for cross-framework integration)
- Lambda Function wrappers and deployment tools
- Agent-as-a-Tool design patterns and examples

The Open API Adapter library was the big breakthrough. Not just solving Bedrock's ecosystem lock-in, but embodying my architecture philosophy - don't get locked to one vendor, keep the tech stack open and flexible. The library gained community recognition and boosted Travelers' reputation in AI tech.

This phase validated Phase 1's three-layer architecture. Migrating from Temporal to Bedrock Agent, the core Tool Library and business logic barely changed. Only the adaptation layer needed work. The forward-looking design proved itself again.

## Phase 3: Strand Agents + EKS - Entering the Multi-Agent Era

### Situation

Early 2025 brought another major breakthrough. A new framework called Strand Agents launched, marking the shift from "Agent-as-a-Tool" to true Multi-Agent collaboration.

The first two phases got us Agent deployment and reuse, but still essentially single-Agent orchestration. Complex business scenarios needed multiple Agents working together more flexibly - not just sequential calls but parallel cooperation, dynamic decisions, role division, more complex patterns. Bedrock Agent couldn't really support true Multi-Agent scenarios.

Strand Agents offered multiple Multi-Agent patterns:

- Agent-as-a-Tool: Basic inter-calling
- Swarm mode: Agent group collaboration
- Graph mode: Complex dependencies on graph structures
- Workflow mode: More flexible flow orchestration
- Agent-to-Agent mode: Direct Agent communication

These patterns dramatically boosted Agent capability. We evolved from upgraded Workflows to highly intelligent systems approaching Claude Code's autonomous coordination abilities. The enterprise needed this for complex business scenarios.

### Task

I needed to lead the team from Bedrock Agent into the Multi-Agent era. Build infrastructure supporting complex Agent collaboration. Here's what that meant:

- Evaluate Strand Agents framework, design EKS-based deployment architecture
- Keep Bedrock Agents running while building new Multi-Agent deployment capability
- Develop best practices and code examples for different Multi-Agent patterns
- Use existing Agent Catalog, re-wrap current Agents with Strand Agents

The challenge? Introduce new tech without impacting existing apps. We needed dual-track: old Bedrock Agents keep serving existing apps, new complex scenarios use Strand Agents.

### Action

**EKS Deployment Architecture**

I designed Strand Agents deployment on Amazon EKS. Each Multi-Agent app deploys as containerized service on EKS. We use Kubernetes orchestration for Agent lifecycle management, scaling, high availability. Compared to Bedrock's fully managed service, this gives us more flexibility and control but means handling operational complexity ourselves.

**Reusing and Extending Agent Catalog**

Thanks to Phase 2's Agent Catalog, we had a clear Agent asset inventory. I led the team re-wrapping these Agents with Strand Agents so they could participate in Multi-Agent collaboration. Since underlying Tool Library and business logic stayed unchanged (three-layer architecture paying off again), the wrapping work mostly focused on adapting to Strand Agents' interface specs.

**Best Practices for Multiple Multi-Agent Patterns**

The most important work this phase - establishing standardized practice guides for different Multi-Agent architecture patterns. I personally wrote extensive code examples and design docs:

- **Agent-as-a-Tool pattern**: Basic inter-calling for linear task decomposition
- **Agent-to-Agent pattern**: Direct Agent communication for real-time negotiation scenarios
- **Swarm pattern**: Multiple Agents processing in parallel for large-scale data or distributed decisions
- **Graph pattern**: Dependencies on directed acyclic graphs for complex task dependencies
- **Workflow pattern**: Flexible flow orchestration combining traditional Workflow and Agent intelligence

Each pattern got complete code examples, deployment configs, debugging methods, applicable scenario descriptions.

**Deep MCP Integration**

By this phase, MCP standard had matured. I made MCP integration a standard Strand Agents architecture component. All Tools exposed through MCP protocol so Agents could access capabilities uniformly.

**Multi-Agent Design White Paper**

I wrote an enterprise Multi-Agent design white paper covering:

- Technical characteristics and applicable scenarios for different Multi-Agent patterns
- Enterprise deployment architecture best practices
- Standardized solutions for security, monitoring, debugging
- Migration paths from single Agent to Multi-Agent

This became authoritative technical documentation internally, guiding Multi-Agent development across teams.

**GitHub Repos and Tooling**

Built a series of GitHub Repositories with reference implementations for different Multi-Agent patterns. Not just code but detailed documentation, deployment scripts, test cases - complete technical asset library.

### Result

We upgraded the enterprise AI infrastructure from single-Agent to Multi-Agent era. Complex business scenarios started using Multi-Agent architecture, fully leveraging Agent collaboration advantages. After each phase, 3-5 new apps adopted the latest Multi-Agent patterns.

Key technical assets:

- EKS-based Strand Agents deployment architecture
- Complete design patterns and code examples for six Multi-Agent patterns
- Multi-Agent Design White Paper for Enterprise
- Series of GitHub Repos with reference implementations for different patterns
- Toolchain deeply integrated with MCP

This phase marked a qualitative leap in enterprise AI capability. From Agents executing predefined tasks to intelligent systems capable of autonomous collaboration and dynamic decision-making. The three-layer architecture worked its magic again - reusing bottom Tools and business logic meant Multi-Agent migration costs came in way lower than expected.

## Phase 4: AWS Agent Core - Maturing the Enterprise Multi-Agent Platform

### Situation

Early 2025, AWS launched the completely redesigned Agent Core service. This was a comprehensive solution incorporating all the requirements and pain points from the first three phases.

Phase 3 got us Multi-Agent capability, but clear enterprise gaps remained:

- **Insufficient Observability**: Agents on EKS lacked unified monitoring and tracing. Hard to understand Agent decision processes, tool calling chains, failure reasons
- **Missing Memory Management**: Agents couldn't maintain context across sessions. Each call was fresh conversation, couldn't accumulate knowledge or remember user preferences
- **Scattered MCP Integration**: Already using MCP but integration needed manual handling, no out-of-box support
- **Difficult Prompt Management**: Agent Prompts scattered in code, hard to manage uniformly, version control, A/B test
- **Resource Waste**: EKS-deployed Agents had to keep running even when unused, consuming compute resources with high costs

AWS Agent Core came at the perfect time:

- Native MCP support - all tool integration unified using MCP protocol
- Built-in Visibility tracking every Agent decision step and tool call
- Unified Memory management supporting cross-session context
- Integrated Prompt Management
- Pay-as-you-go billing based on actual usage

As a major Travelers customer and former AWS employee, we got early access before Agent Core GA (October 20, 2025).

### Task

I needed to evaluate Agent Core, complete migration planning from Strand Agents to Agent Core, establish complete enterprise Multi-Agent platform. Here's what that meant:

- Deep evaluation of Agent Core capabilities and applicable scenarios
- Develop Prototypes validating Agent Core's Multi-Agent scenario support
- Design migration path from Strand Agents to Agent Core
- Establish complete supporting facilities: Observability, Memory, Prompt Management, MCP integration
- Set new Agent development and deployment standards

This phase wasn't just tech migration - it marked the enterprise Multi-Agent infrastructure reaching full maturity. From "it works" to "it works well," from "toy" to "production-grade platform."

### Action

**Early Access and Deep Evaluation**

Through Travelers' deep AWS partnership and my former employee connections, we became early Agent Core testers. I kept close communication with the AWS Agent Core team - many were former colleagues. This let me understand the product roadmap deeply and provide enterprise perspective feedback.

I led comprehensive technical evaluation focusing on:

- Whether Agent Core's Multi-Agent support covered all Strand Agents scenarios
- Whether Observability depth met enterprise debugging and monitoring needs
- Whether Memory management flexibility fit different business scenarios
- Integration complexity with existing toolchains and infrastructure

**Prototype Development**

I led development of multiple Agent Core Prototypes validating key scenarios:

- Can complex Multi-Agent collaboration processes work on Agent Core
- How existing Tool Library adapts to Agent Core's MCP interface
- Observability and Memory performance in actual scenarios
- Technical feasibility and workload of migrating from Strand Agents

During Prototype development, I fully used the three-layer architecture from Phase 1. Bottom Tool Library and middle orchestration logic barely changed - just rewrite the top Framework Adapter layer. The original architecture design's forward-thinking validated again.

**Complete Supporting Facilities Integration**

Agent Core provides out-of-box enterprise features but needed deep integration with Travelers' existing infrastructure. I completed:

- **Visibility Integration**: Connected Agent Core's tracing data to enterprise unified monitoring platform, set up Alerts and Dashboards
- **Memory Configuration**: Configured Memory strategies for different business scenarios, balancing context retention and privacy compliance
- **Prompt Management**: Established Prompt version control and approval processes, aligning with enterprise compliance
- **MCP Ecosystem Integration**: Migrated all existing Tools to standard MCP services, fully leveraging Agent Core's native MCP support

**Migration Planning**

I developed detailed migration plans using progressive strategy:

- Phase 1: New projects prioritize Agent Core
- Phase 2: Migrate some Strand Agents apps to Agent Core, validate stability
- Phase 3: Evaluate whether to keep Strand Agents for special scenarios or fully switch to Agent Core

During migration, fully leverage three-layer architecture advantages. Most code gets reused, migration costs stay within acceptable range.

**Best Practices and Example Repos**

I developed extensive Agent Core example code and best practice documentation:

- How to migrate different Multi-Agent patterns from Strand Agents to Agent Core
- Agent Core Observability best practices
- Scenario-based Memory configuration guides
- Prompt Engineering for Agent Core
- Integration patterns with MCP tool ecosystem

These assets formed new technical knowledge base, helping teams quickly master Agent Core development.

**Bidirectional Collaboration with AWS Team**

As early customers, we not only used Agent Core but provided feedback for product improvement. My direct communication channels with the AWS team meant Travelers' actual needs could influence the product roadmap. Some feature improvement suggestions we proposed got adopted, which made Agent Core better meet enterprise needs.

### Result

We completed Agent Core Prototype development and Migration Planning, marking enterprise Multi-Agent infrastructure entering maturity phase. Some apps already started migrating to Agent Core, validating technical solution feasibility.

Key technical assets:

- Complete Agent Core Prototypes and reference implementations
- Migration Planning documentation and tools
- Extensive Example Repos covering different Multi-Agent pattern implementations on Agent Core
- Best Practice documentation covering Observability, Memory, Prompt Management, other enterprise features
- Deep cooperation relationship with AWS team influencing product roadmap

Agent Core adoption marks an important milestone in the four-phase evolution. Enterprise AI infrastructure isn't a cobbled-together toolset anymore - it's complete, mature, extensible platform. All enterprise requirements (observability, memory management, cost optimization, compliance) have out-of-box solutions while maintaining high flexibility and customization capability.

From pay-as-you-go cost perspective, compared to EKS's continuous running costs, Agent Core significantly reduced resource waste, especially for Agent apps with uneven access patterns.

## Cross-Phase Impact and Architectural Philosophy

### Core Value of Evolutionary Architecture

This 18-month project's most important outcome wasn't successfully adopting any specific technology. It was building the ability to continuously make correct architectural decisions amid rapid change.

**Why Evolution Instead of "Waiting for Maturity"?**

AI tech changed faster from 2023-2025 than traditional IT ever has. If we'd chosen to "wait until the tech stack stabilizes," we'd have missed critical windows:

- Competitors already accumulating AI application experience
- Business units' automation needs going unmet
- Tech teams unable to keep pace with AI development

Our dual-track strategy solved this:

- Architecture team (engineers I led) continuously tracking latest, optimal technical solutions
- Each milestone phase forming stable best practices, new projects onboarding to latest framework
- Old apps not forced to upgrade, keeping independent operation, avoiding business disruption
- Bottom Tool abstraction design making migration costs controllable

This strategy let the enterprise stay agile amid rapid tech change without descending into chaos from frequent migrations.

### Three-Layer Architecture's Continuous Impact

The three-layer abstraction from Phase 1 persisted through the entire evolution:

**Layer 1 (Tool Library)**: Stayed highly stable across all four phases. Using Command Pattern encapsulation, these Tools adapted to LangChain, Bedrock Agent Lambda, Strand Agents, Agent Core, other frameworks. Core enterprise business capabilities settled as reusable Libraries, decoupled from specific Agent frameworks.

**Layer 2 (Orchestration Logic)**: Adjusted based on business needs across phases but stayed pure logical expression, not coupled to frameworks. This made business logic migration costs extremely low - mainly reorganizing code structure, not rewriting logic.

**Layer 3 (Framework Adapter)**: The main work layer for each migration. But because this layer was thin enough, migration workload stayed within reasonable bounds.

This architecture design's forward-thinking got validated with every migration. Without this abstraction layer, each tech stack change would've been catastrophic rewrites. The enterprise couldn't sustain such high-frequency migration costs.

### Continuous Accumulation of Technical Assets

Through four phases, tools, libraries, documentation I developed formed rich technical assets:

**Open Source Libraries**:

- Open API Adapter for Bedrock Agent: Solved cross-framework integration, gained community recognition

**Internal SDKs and Frameworks**:

- AI Workflow Orchestration Framework: Three-layer architecture implementation
- Temporal Cloud toolchain
- Agent deployment and management tools

**Knowledge Base**:

- Multi-Agent Design White Paper: Authoritative enterprise Multi-Agent guide
- Extensive Example Repos: Code examples for different patterns and phases
- Best Practice documentation: Covering development, deployment, monitoring, debugging

**Organizational Mechanisms**:

- Agent Catalog: Agent asset registration and discovery platform
- GitHub Repo structure standards: Unified code organization
- CI/CD Pipeline: Automated build and release workflows

These assets serve not just the present but foundation for continuous future evolution. Each new phase built on previous accumulation, creating compound effects.

### Strategic AWS Partnership

As former AWS employee and major customer representative, the bidirectional collaboration model I established brought unique advantages to Travelers:

- **Early Access**: Getting new tech earlier than competitors, building first-mover advantage
- **Product Influence**: Enterprise needs influencing AWS product roadmap, making products better fit actual scenarios
- **Technical Support**: Direct communication channels, quickly solving technical issues and getting best practice guidance

This relationship isn't just personal connections - it's enterprise strategic assets.

### Impact on Team and Enterprise

- **Team Capability Growth**: My 10 engineers experienced four tech stack evolutions over 18 months, deeply participating in the complete journey from Workflow to Multi-Agent. They mastered not just specific technologies but how to make architectural decisions amid rapid change, how to design evolvable systems.
- **Qualitative Change in Enterprise AI Capability**: From no standardized Agent infrastructure to owning mature Multi-Agent platform, Travelers' AI capability achieved qualitative transformation. Each phase had 3-5 new apps adopting latest framework - continuous tech adoption momentum.
- **Technical Influence**: Through open-source contributions and technical sharing, Travelers established industry influence in AI architecture - rare technical brand equity for traditional insurance company.

### Summary of Architectural Philosophy

This project validated several important architectural principles:

1. **Abstraction Over Specifics**: In rapidly changing domains, abstraction and decoupling matter more than specific tech choices. Three-layer architecture's success proves this.

2. **Evolution Over Perfection**: Don't chase one-time "perfect solutions." Build continuous evolution capability. Make optimal choice for each phase while preparing for future changes.

3. **Open Over Closed**: Developing Open API Adapter for Bedrock Agent embodied this. Avoid single vendor lock-in, keep tech stacks open and flexible.

4. **Accumulation Over Speed**: Each phase settles technical assets (tools, documentation, best practices) creating compound effects, not just completing migration and stopping.

5. **Dual-Track Balance**: Architecture team pursues latest and best while not forcing old app upgrades, finding balance between innovation and stability.

These principles apply not just to Multi-Agent infrastructure but universal methodology for handling any rapidly evolving technology. This is the project's core value.
  </markdown_content>
</document>
<document>
  <title>2022-04 to 2022-08 - AWS Cross-Cloud Data Synchronization Solution</title>
  <markdown_content>
# 2022-04 to 2022-08 - AWS Cross-Cloud Data Synchronization Solution

> As a Data Architect / Engineer Consultant at Accenture Federal Service, I led the design and implementation of critical data infrastructure for the Federal Retirement Thrift Investment Board's pension asset management system, which serves all federal government employees. My work spanned the entire data engineering lifecycle, encompassing **data architecture**, **ETL pipeline development**, **workflow orchestration**, **data analytics**, and **PySpark job optimization**.

## Overview

I built an S3 sync system for the Federal Retirement Thrift Investment Board that moves data between AWS Gov Cloud and Commercial Cloud. These environments are totally walled off from each other - none of AWS's normal sync tools work. The system pushed 700MB daily using event-driven architecture and smart routing. Ran in prod for over a year with zero downtime.

## Background

FRTIB manages retirement accounts for every federal employee. They're stuck in AWS Gov Cloud for compliance reasons, but they want to use newer analytics tools that only exist in Commercial Cloud (Gov Cloud is always behind on features).

The catch: Gov Cloud and Commercial Cloud might as well be on different planets. They're in separate AWS partitions, so nothing talks to each other. Data Pipeline? Doesn't work. S3 Sync? Nope. But the business still needs to pull non-sensitive data from their Gov Cloud data lake every day and dump it into Commercial Cloud for analytics.

## Situation

The big problem is AWS's partition wall:

- **Partition isolation**: Two different AWS partitions mean the APIs don't talk to each other. All the normal sync tools are dead in the water
- **Data volume**: About 700MB per day, but file sizes are all over the place (90% under 128MB, then random 2-3GB files show up)
- **Reliability**: Can't lose data, period. Need exactly-once delivery
- **Timing**: Weekly refresh cycle, has to finish by Friday. The weekend gives us breathing room if something goes wrong

## Task

Build a cross-partition S3 sync that:

- Moves files from Gov Cloud to Commercial Cloud automatically
- Never loses data - exactly-once delivery
- Handles everything from tiny files to multi-GB blobs
- Doesn't burn money on Lambda
- Keeps credentials secure and compliant

The constraints: AWS APIs won't cross partitions, Lambda has memory and storage caps, and compliance rules are non-negotiable.

## Action

### Event-Driven Architecture

EventBridge watches the S3 bucket in Gov Cloud. File shows up, Lambda fires. Way better than polling - you get instant response without burning cycles checking for updates.

### State Tracking and Idempotency Guarantee

DynamoDB tracks everything. Each sync job creates a record - S3 key is the primary key, stores the path and status. Source and destination paths match exactly, so no weird mapping logic.

Lambda gets an event, checks DynamoDB first. Record exists and succeeded? Skip it. Record failed? Retry. That's how we get exactly-once delivery. A cron job scans for failed records and kicks off retries automatically.

S3 operations are atomic, so no partial failures to worry about. Makes error handling way simpler.

### Cross-Cloud Credential Management

Commercial Cloud creds live in Gov Cloud's Parameter Store, encrypted with KMS. Only admins and the Lambda can touch them.

Lambda grabs the creds, then hits the S3 SDK directly - GetObject from Gov Cloud, PutObject to Commercial Cloud. Pretty straightforward.

### Smart Router Mechanism

Different Lambda configs for different file sizes:

- **Small files**: 90% of files (under 128MB) hit a standard Lambda with smaller memory. Just read and go
- **Large files**: The other 10% (up to 2-3GB) get routed to a beefier Lambda with 10GB memory for multipart uploads

Lambda checks the S3 metadata for file size. Under 128MB? Handle it. Over that? Forward the event to the big-file handler. Keeps costs down - you're not running 10GB Lambdas for tiny files.

### Iterative Improvement

First version just had one Lambda with 1GB memory. Launched in June 2022. Two months later in August, we started hitting failures on files over 1GB. Built the Smart Router right away and deployed a 10GB Lambda specifically for the big files.

## Result

The system went live in June 2022, got the Smart Router upgrade in August, then ran production for over a year with zero failures. It solved the technical challenge of AWS cross-partition data sync and gave the client a reliable data pipeline for their analytics system. The event-driven architecture plus state tracking guaranteed zero data loss, while smart routing kept Lambda costs reasonable.

## Personal Note

This project ran from April to June 2022, three-month timeline. I handled it sololy as a Data Engineer at Accenture - design, implementation, and dev environment deployment were all me. DevOps team handled the production deploy.

Two months after the August launch, we hit one issue: the original design only had a single Lambda function (1GB memory), which couldn't handle large files over 1GB. That failure drove me to design the Smart Router mechanism, adding the 10GB Lambda for large file scenarios. After that improvement, the system ran stable - zero failures for over a year until I left.

Multiple subprojects were running in parallel, and this sync system was relatively simple but critical. The client was extremely happy because it solved their long-standing bottleneck for cross-cloud data flow.
  </markdown_content>
</document>
<document>
  <title>About Me (Sanhe Hu)</title>
  <markdown_content>
# About Me (Sanhe Hu)



## About Me

My name is **Sanhe Hu**. I came to the United States in **August 2011** to pursue my **Master’s degree at George Washington University** in Washington, D.C., which I completed in **May 2013**. Since then, I’ve been living and working in the **DMV area** (Washington D.C., Maryland, Virginia). I’m currently a **U.S. Permanent Resident** that doesn’t need H1B sponsor ship.

## **Career Origins: Machine Learning Foundations (2014–2018)**

My professional journey began in **February 2014**, when I joined **Earth Networks** as a **Data Scientist**. The company focused on weather data and smart HVAC systems for enterprise energy efficiency solutions. This role gave me my core foundation in **machine learning**, **data analysis**, and **model-driven applications**. I spent four years designing algorithms and building ML applications that made a measurable difference in how organizations managed environmental and energy data.

## **Shifting into Software and Cloud: The Consulting Phase (2018–2021)**

By **mid-2016**, I found myself increasingly curious about software engineering, web development, and cloud computing. That curiosity turned into action when I joined **Enquizit on July 2018**, an IT consulting firm that provides solution implementation services. I served as a **Data Engineer and later a Data Architect**, working on cloud-based enterprise solutions primarily built on AWS. During these years, I developed a strong foundation in **software engineering**, **cloud architecture**, and the **consulting lifecycle**—everything from customer engagement to solution delivery.

## **AWS Data Lab: Deep-Dive Architecture with Enterprise Clients (2021–2023)**

In **July 2021**, I joined **Amazon Web Services (AWS)** as a **Data & Solutions Architect** within the **AWS Data Lab** program. My work was both technical and consultative—helping customers design and build AWS-based data solutions in real time, using their own environments and data. Every month, I worked with a different customer, guiding their engineering teams and working directly with **C-level executives and technical leads**. Over the span of two years, I completed **24 high-touch engagements**, gaining invaluable insight into real-world architectural challenges across industries. More importantly, I developed strong skills in **executive communication**, **technical persuasion**, and **sales enablement**—frequently helping customers align on my recommendations and **close on final architectures or decisions**.

## **Solo Consulting with AI: Building Bunnymantech LLC (2022–Present)**

While still at AWS, I decided to test my own ideas and created **Bunnymantech LLC** in **January 2022**, an **AI-assisted solo IT consulting firm** registered in Virginia and operated out of my home office in Maryland. My model was simple but bold: do everything myself—with the help of AI.

From architecture and software development to legal contracts, client demos, sales calls, marketing, and even website maintenance - I handled it all, assisted by a growing toolkit of AI-based solutions. My primary source of leads has been **professional networking platforms like LinkedIn**, where I engage potential customers, run demos, secure contracts, and deliver production-ready software solutions.

## **Open Source & Community Contributions**

Beyond my professional work, I have a deep passion for open source. I actively maintain **around 150 Python libraries**, including **three official AWS libraries** I created during my time at Amazon. These projects collectively see **10 to 20 million monthly downloads**.

I also maintain **500–600 public GitHub repositories** and have published over **2,000 technical blog posts** that showcase my projects, share insights, and document my learning journey. My GitHub activity is consistent and prolific—I average around **10 commits per day**, every day of the year.
  </markdown_content>
</document>
<document>
  <title>2023-06 to 2023-12 Cross-Cloud Data Pipeline Project</title>
  <markdown_content>
# 2023-06 to 2023-12 Cross-Cloud Data Pipeline Project



## Overview

I spent six months as a contractor at PepsiCo building a pipeline to move Gatorade's data from AWS over to Azure. PepsiCo had just bought Gatorade, and all their data was sitting in AWS RDS while PepsiCo's analytics lived on Azure. My job was simple: get 100 million retail customer records flowing from AWS into Azure Blob Storage and then into Salesforce so the business teams could actually see what was happening with their new acquisition.

The code itself wasn't complicated. The hard part was dealing with a massive company that runs workloads across AWS, Azure, and GCP - tracking down who owned what, getting permissions, and herding everyone through the approval process.

## Background

PepsiCo bought Gatorade in June 2023. Gatorade's infrastructure was all AWS, PepsiCo's BI was all Azure. And PepsiCo? They're basically cloud-agnostic - different business units scattered across all three major cloud providers depending on who set things up and when.

Gatorade's team was swamped dealing with the acquisition. Rather than pull internal resources, PepsiCo brought in contractors to handle the data integration work.

## Personal Note

Six-month contract. Nothing fancy technically, but I learned a lot about how multi-cloud actually works at enterprise scale.

Honestly, the biggest pain point wasn't writing the ETL code - it was the coordination game. Every single piece of the pipeline had someone who owned it, and I had to track them all down and get sign-off. In a big company, cross-cloud data sync is just hitting APIs, but you can't hit any APIs until you've talked to like eight different teams.

My approach: system design first, break everything into chunks, figure out who owns each chunk, throw it all in a checklist, knock them out one by one. Delivered end of October. Only ran into one issue in November and December. Uptime was around 95%.

## Situation

Here's what we were dealing with:

- Gatorade data completely isolated from PepsiCo's systems
- PepsiCo couldn't see Gatorade's retail numbers or operational metrics
- The BI team's dashboards on Azure had zero visibility into the new business
- Hard to make strategic calls when you can't see what's actually happening

Source was AWS RDS - about 100 million retail customer activity records, already exported to S3 via Database Export. About 800MB total. Goal: sync that over to Azure Blob Storage, then let another team push it through Informatica into Salesforce for the business folks.

## Task

Build the AWS-to-Azure sync pipeline. Constraints:

- Six months, June through December 2023
- Use Airflow running on Azure for orchestration
- Full snapshot replacement, run once a day at end-of-day
- Coordinate with AWS data owners, Azure Cloud Support, business teams, and the engineers using the data downstream

Reported to the Engineering Manager. This was an execution role - take the design, build it, ship it.

## Action

### System Design and Task Breakdown

Started with a design doc. Broke the whole flow into pieces: reading from S3, writing to Blob Storage, networking between clouds, auth setup, the Airflow DAG. Made a checklist for each piece with the technical work and who I needed to talk to.

### Cross-Team Collaboration and Approval Management

The real work was people, not code:

- Figured out who owned each AWS account, reached out for access
- Worked with AWS data owners to get S3 credentials and understand the RDS export setup
- Coordinated with Azure Cloud Support to provision the Airflow service account and get write access to Blob Storage
- Kept the business teams and downstream engineers in the loop on data format and delivery schedule

Just stayed organized. Every dependency got a name attached to it. Tracked everything in my checklist. Pushed things forward one at a time.

### ETL Pipeline Implementation

Built the Airflow DAG:

- Pull data files from AWS S3 (already exported from RDS via Database Export)
- Configure S3 endpoints so Airflow on Azure could talk to AWS
- Implement the full snapshot replacement logic - blow away yesterday's data, write today's
- Add error handling and monitoring

Main technical headache was getting the networking and auth right between clouds. Once I had S3 endpoints configured and IAM credentials sorted, it was smooth.

The piece from Azure to Salesforce was someone else's problem - they were using Informatica Connector. I just had to make sure data landed in Blob Storage in the right format.

## Result

Shipped end of October:

- Daily ETL running, syncing ~100 million records
- BI dashboards went from "no Gatorade data" to "live metrics on the new business"
- Hit one issue total between November and December - about 95% uptime
- PepsiCo got visibility into the acquisition, could actually track what was happening

Wrapped the six-month contract on time. Proved out that you can actually do data integration across multiple clouds at enterprise scale. Main thing I learned: how to navigate corporate bureaucracy and keep a multi-cloud project moving forward. The tech was never the bottleneck.
  </markdown_content>
</document>
<document>
  <title>2022-11-14 - Buidl Lab - casualty insurance company</title>
  <markdown_content>
# 2022-11-14 - Buidl Lab - casualty insurance company

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

A Fortune 500 property and casualty insurance company's Claims department received over three million medical documents annually for processing. Each document was manually reviewed by agents who extracted data from CMS1500 forms (professional claim forms used in the insurance industry) and uploaded it to application systems. This labor-intensive process, supported by an aging on-premises Kofax system with inadequate field extraction accuracy, created significant operational bottlenecks that slowed claims processing and negatively impacted customer experience. The customer's AI Accelerator team sought to modernize this workflow by building an intelligent document processing (IDP) solution on AWS, but faced constraints including restricted network access due to PII regulations and limited availability of production data for development.

## Task

I was assigned as the Data Lab Architect to design and build proof-of-concept machine learning solutions for automated document classification and data extraction while working on-site with the customer's data engineers. My responsibility was to deliver working code that achieved 95% classification accuracy across document types and successfully extracted 70% of 54 critical fields from CMS1500 forms, then transfer this capability to the customer team so they could replicate the solution in their AWS environment. The engagement required navigating network restrictions (no public internet access due to PII data), configuring VPC endpoints for AWS service access, and building the solution collaboratively on a SageMaker Notebook Instance while simultaneously teaching the customer engineers the implementation approach.

## Action

I designed and implemented a multi-component IDP solution using Python, Amazon Textract, Amazon Comprehend, and Amazon SageMaker.

**Pre-lab preparation:** I worked with the customer team to configure VPC endpoints for S3, Textract, and Comprehend to enable API access without public internet, set up a Nexus Repository for Python package management, and provisioned a SageMaker Notebook Instance with Python 3.8 for development.

**Document classification implementation:** I built a pipeline that split multi-page PDFs into single-page documents using PyPDF2, extracted text using Textract's document analysis API, manually labeled 350 pages across two document types (CMS1500 and Notes) with the customer team, trained a custom document classifier using Amazon Comprehend with a 70/30 train-test split, and validated predictions against test data.

**Data extraction implementation:** I developed Python programs to parse Textract's JSON output containing form key-value pairs and table cells, then engineered three custom algorithms to resolve field extraction mismatches: (1) a fuzzy-matching algorithm using the fuzzywuzzy library to identify and rename sub-region fields based on parent region coordinates, (2) a geo-location-based table cell realignment function to handle documents with invisible table grids, and (3) a bounded-box coordinate search to extract data from large unpredictable text areas. Throughout the four-day engagement, I demonstrated Amazon Augmented AI for building human-in-the-loop validation workflows and designed a comprehensive event-driven architecture diagram for the production pipeline using Lambda, SNS, DynamoDB, and S3.

## Results

I successfully delivered a complete IDP solution that achieved 100% document classification accuracy (exceeding the 95% target) and 100% extraction of all 54 required fields from CMS1500 forms (exceeding the 70% target). The customer team validated the solution against low-quality scanned documents with edge cases, and I resolved two complex edge cases with minor code changes in 15 minutes during live testing. Following the lab, the customer invited me to present the solution to executive leadership and immediately requested booking a follow-up Build Lab to implement the full production pipeline, citing this as a high-value capability that would significantly reduce manual effort and enhance customer experience.
  </markdown_content>
</document>
<document>
  <title>2024-04 to 2024-12 DynamoDB Data Lake Solution</title>
  <markdown_content>
# 2024-04 to 2024-12 DynamoDB Data Lake Solution



## Overview

I designed and delivered an innovative DynamoDB data lake solution for Thrivent Financial that's 10x faster than traditional Glue-based approaches and costs just a tenth as much. The secret? A custom high-performance serialization engine combined with a divide-and-conquer architecture that turned what would be hours of processing into minutes.

While the solution directly addressed Thrivent's challenge of analyzing 50+ tables with deeply nested structures, it also tackled a much broader problem plaguing the AWS ecosystem: how to make DynamoDB data analytics actually affordable. The result: costs dropped from potentially hundreds of thousands per month to just a few thousand, while data latency fell from a week to a single day.

## Background

Thrivent Financial is a nonprofit that provides affordable—often free—financial planning to low and middle-income families across the southern United States. Since they're a funded charity that reinvests every dollar rather than distributing profits, cost control isn't just good practice; it's essential to their mission.

When I joined as Data Architect in April 2024, my first task was to solve their core analytics problem. All of Thrivent's business data lived in 50+ DynamoDB tables. The largest held around 500 million records, with others ranging from smaller but still substantial. The data had all the hallmarks of real-world business systems: deeply nested JSON structures, constant schema evolution, and plenty of updates (definitely not append-only).

Here's the thing: Thrivent had zero ability to analyze any of this data. And they weren't alone. DynamoDB is fantastic as an OLTP database, but it's basically useless for OLAP workloads. This is a universal pain point—tens of thousands of companies across the AWS ecosystem store their business data in DynamoDB but have no efficient way to analyze it or build business intelligence on top.

## Personal Note

Here's what made this project particularly interesting: I didn't start from zero. During my time at AWS, I'd created two open-source libraries—`dynamodb_json_seder` and `s3manifesto`—that solved exactly these problems. For this project, I essentially packaged them into an enterprise-grade product. Thrivent got to benefit from years of my previous work, which is really what open source is all about.

Working as a contractor instead of a full-time employee gave me something invaluable: complete technical autonomy. No lengthy architecture review cycles, no committee of stakeholders to convince. I could simply pick the best technical approach and run with it. That freedom is what let me ship a complete enterprise product in just five months.

The biggest compromise I made was choosing full snapshots over incremental sync. I absolutely could have built a CDC (Change Data Capture) solution using DynamoDB Streams. But three factors made me choose differently:

1. Current performance was already plenty fast (minute-level sync)
2. Storage costs far less than compute
3. Full snapshots are simpler and more reliable

I went with "good enough" over "technically perfect." Nobody has questioned that decision since.

The real lesson here wasn't about the technology—it was about validating something I've long believed: the most valuable innovations don't solve one customer's problem. They identify widespread industry pain points and build general solutions. DynamoDB analytics is a massively underserved market, and I might be the first to attack it with this kind of aggressive architecture combining serverless with Rust-accelerated parsing.

## Situation

### The Core Challenge

Thrivent was wrestling with a problem that plagues the entire AWS ecosystem but has surprisingly few elegant solutions: how do you efficiently analyze complex business data sitting in DynamoDB?

Here's what made it hard:

**Data Structure Complexity**: DynamoDB uses an adaptive JSON encoding where every field carries a type identifier (`"S"` for string, `"N"` for number), with all values encoded as strings. When you export this data, you get deeply nested JSON that's murder to query in Athena—every query turns into a full table scan.

**Scale and Complexity**: We're talking 50+ tables, each with its own evolving schema. The largest table alone had 500 million records (roughly 500GB), while others ranged from tens of thousands to millions of records. And the data wasn't just append-only—constant updates meant we needed to keep everything in sync with the latest state.

**Limitations of Existing Solutions**: The traditional playbook calls for AWS Glue Jobs to load everything into PySpark, where you manually write complex unnesting logic before converting to Parquet. This approach has some serious problems: Glue Jobs are expensive to run, processing is painfully slow (50 tables could take hours), every table needs its own custom unnesting code (maintenance nightmare), and PySpark just chokes on deeply nested structures.

**Market Alternatives**: Sure, tools like Fivetran and Airbyte support DynamoDB, but they price based on data volume. For 500GB+ of data, you're looking at hundreds of thousands of dollars annually—completely out of reach for a nonprofit.

**The Analytics Gap**: Without a data lake, Thrivent's financial advisors and management were flying blind. No visibility into customer behavior trends, no way to analyze service effectiveness, no ability to optimize resource allocation. For an organization dedicated to serving underserved communities, this gap between mission and capability was stark.

### A Universal Problem

But here's the thing: this isn't just Thrivent's problem. Tens of thousands of companies in the AWS ecosystem store their business data in DynamoDB, and almost all of them hit the exact same wall when it comes to analytics. The market desperately needs a solution that's both high-performance and low-cost—it just doesn't exist yet.

That's what made this project valuable beyond a single customer engagement. It wasn't a one-off custom solution. It was product-grade architecture tackling an industry-wide pain point.

## Task

### Project Goals

As the solo Data Architect, I needed to design and deliver a complete DynamoDB data lake solution that would unlock Thrivent's ability to analyze all their business data.

**Quantifiable Goals**:

- Sync 50+ DynamoDB tables into an S3 data lake supporting efficient SQL queries
- Maintain daily data freshness (T+1 latency)
- Slash query costs by 100x compared to querying raw JSON
- Keep monthly operating costs in the thousands, not hundreds of thousands
- Full automation—no custom code per table

**Technical Requirements**:

- Handle DynamoDB's special encoding format
- Convert deeply nested JSON into Athena-friendly Parquet
- Support continuous updates (not just append-only), maintaining current state
- Build scalable architecture that easily accommodates new tables

**Constraints**:

- Tight budget: nonprofits can't absorb expensive SaaS subscriptions
- AWS tech stack only (aligned with Thrivent's cloud strategy)
- Delivery deadline: working solution by end of 2024
- Solo execution: while team members provided business context and data structure insights, I owned architecture, code, deployment, and operations

**Success Criteria**:

- Analysts query all business data using standard SQL in Athena
- Query response times measured in seconds, not minutes
- Overall cost at least 90% lower than commercial ELT tools
- Solution has product potential for broader AWS customer base

## Action

### Architecture Design Philosophy

I didn't try to incrementally improve what existed. I started over. Traditional solutions hit two fundamental bottlenecks: inefficient DynamoDB JSON parsing and lack of effective parallel processing frameworks. Break through on both fronts, and you completely change the game.

Fortunately, I'd already built the foundation during my AWS days: two open-source libraries called `dynamodb_json_seder` (serialization engine) and `s3manifesto` (divide-and-conquer engine). The real work here was wrapping these low-level tools into an enterprise-grade data pipeline.

### Core Innovation One: Rust-Accelerated Serialization Engine

**The Problem**: AWS's boto3 library handles DynamoDB JSON parsing, but it's built on Python for-loop recursion. When you throw complex nested structures at it, performance craters. I needed something an order of magnitude faster.

**The Solution**: I optimized my open-source `dynamodb_json_seder` library, which leverages Polars—a Rust-based DataFrame library—and its expression API to turn parsing logic into vectorized operations. Rust's memory management and parallel processing deliver 20-100x speedups over boto3, with deeply nested structures sometimes seeing improvements beyond 100x.

**Real-World Results**: On Thrivent's actual data—which features genuinely complex nesting—I measured 80x+ performance gains. Tasks that would take hours now complete in minutes.

**Technical Details**: DynamoDB JSON looks like `{"key": {"S": "value"}}`. I need to transform it into standard `{"key": "value"}` format while handling type conversion (string `"123"` becomes number `123`). Polars' expression API lets me do all this in a single pass, eliminating the function call overhead that recursion introduces.

### Core Innovation Two: Divide-and-Conquer Parallel Processing Engine

**The Problem**: DynamoDB exports produce hundreds or thousands of small files, each with varying sizes. How do you orchestrate processing efficiently? How do you handle errors and retries across parallel execution? How do you maintain data consistency?

**The Solution**: I used my `s3manifesto` library, purpose-built for batch file processing on S3. The approach:

1. **Metadata Scan**: Survey all files under an S3 path, capturing size and modification times
2. **Smart Grouping**: Package files by total size so each Lambda handles roughly equivalent workloads (avoiding scenarios where some Lambdas finish quickly while others drag)
3. **Parallel Scheduling**: Each Lambda processes only its assigned file group—no interference
4. **Error Recovery**: Track processing status per file; failed files can retry independently

**Implementation**: I built a fully serverless architecture using AWS Lambda, S3, and EventBridge. DynamoDB export triggers a Lambda that reads the manifest, groups files by size, then launches 100+ Lambda instances in parallel. Each Lambda runs on ARM architecture (lower cost), calls `dynamodb_json_seder` to parse JSON, converts to DataFrame, then writes to S3 partitioned by user-specified key (typically time).

**Performance Tuning**: By adjusting how many files each Lambda processes (usually 10-20 per group), I hit optimal parallelism. In practice, processing the largest 500GB table takes just 2 minutes, and all 50 tables running in parallel complete in roughly ten minutes total.

### End-to-End Data Pipeline

**Export Flow**: I use DynamoDB's Export to S3 feature to dump table data as raw JSON. This is AWS-native functionality—dirt cheap and rock solid.

**Transformation Flow**:

1. S3 event triggers a coordinator Lambda
2. Coordinator scans all exported files and generates a manifest
3. Based on the manifest, it spins up 100+ worker Lambdas in parallel
4. Each worker uses `dynamodb_json_seder` to parse its assigned files
5. Workers complete unnesting and transformation in memory
6. Workers write Parquet files to the data lake, time-partitioned

**Data Lake Structure**: The final data lake follows standard partition structure: `s3://bucket/table_name/year=2024/month=11/day=08/*.parquet`. Athena leverages partition pruning to dramatically cut query costs.

**Scheduling Strategy**: EventBridge rules trigger full sync automatically every night. Yes, it's full snapshot rather than incremental, but given the extreme performance, this simpler strategy is actually more reliable.

### Observability and Auditing

After shipping the first version in September 2024, I kept iterating—particularly around observability:

- **Detailed Reports**: Every sync generates a complete audit trail: which Lambdas ran, their runtimes, how much data they processed
- **Metadata Management**: Schema change history for each table, so users can track data evolution over time
- **Cost Tracking**: Real-time AWS cost calculation per sync, giving users complete spending visibility

These features make the solution not just fast, but transparent and controllable.

### Tech Stack Summary

- **Core Engine**: dynamodb_json_seder (Rust/Polars), s3manifesto (Python)
- **Compute Layer**: AWS Lambda (ARM architecture, Python 3.11 runtime)
- **Storage Layer**: S3 (data lake), DynamoDB (metadata)
- **Orchestration Layer**: EventBridge (scheduling), Step Functions (complex workflows)
- **Query Layer**: Athena (SQL analytics), Glue Catalog (metadata management)
- **Observability**: CloudWatch Logs, custom audit reports

## Result

### Quantitative Outcomes

**Performance Breakthrough**:

- Processing 500GB of data: from theoretical hours down to **2 minutes**
- 50 tables syncing in parallel: **10-15 minutes** total
- Versus traditional Glue Jobs: **10x+ faster**
- DynamoDB JSON parsing: **80x+ speedup** (measured on complex nested structures)

**Cost Revolution**:

- Monthly operating costs: **$hundreds to ~$1,000**, a **100x savings** versus querying raw JSON
- Versus SaaS tools like Fivetran and Airbyte: **1/10th the cost** (hundreds of thousands yearly → thousands)
- Query costs: Parquet format + partitioning strategy dropped Athena costs **99%+**
- Fully serverless: **zero infrastructure overhead**, no clusters to maintain

**Concurrency at Scale**:

- Single sync task: **100+ parallel Lambda instances**
- Handles unlimited tables without manual orchestration
- ARM Lambda: **20% cheaper** than x86

**Availability Transformation**:

- From "can't analyze anything" to "second-level SQL responses"
- Data sync latency: from N/A to **T+1** (daily updates)
- Full support for complex joins, aggregations, window functions—all advanced analytics

### Business Impact

**Value to Thrivent**:

- Financial advisors now analyze customer behavior patterns in real-time, optimizing service strategies on the fly
- Management gained data-driven decision capabilities for evaluating regional and program effectiveness
- As a nonprofit, dramatically lower costs free up resources for their actual mission instead of IT infrastructure
- Enhanced analytics directly support their social mission: serving underserved communities more efficiently

**Industry Significance**:

- Demonstrated serverless architecture viability and cost advantages in data engineering
- Challenged traditional ELT tool pricing through technical innovation
- Provided affordable analytics for tens of thousands of AWS DynamoDB users
- Open-source libraries (`dynamodb_json_seder`, `s3manifesto`) gained broader community traction

### Technical Recognition

**Architectural Innovation**:

- Proved Rust-accelerated data processing libraries integrate seamlessly with Python ecosystems
- Divide-and-conquer architecture became a reference pattern for large-scale batch file processing
- Full snapshot strategy validated as pragmatic choice in the era of cheap storage

**Personal Impact**:

- Became my signature case study as an Enterprise Architect—proof I can ship complete enterprise products from zero to one
- Strengthened my technical influence in the AWS ecosystem, setting up later AI infrastructure work at Travelers
- Validated "evolution over perfection": architectures that adapt to change beat technically perfect but rigid designs

**Sustainability**:

- Fully automated processes run long-term without constant human intervention
- Detailed audit reports and metadata management keep everything transparent and optimizable
- Product packaging creates potential for future commercialization

### Limitations and Future Directions

**Current Limitations**:

- Full snapshots rather than incremental sync—may not be optimal for ultra-large (TB-scale) tables
- Relies on DynamoDB Export to S3, constrained by AWS export frequency limits

**Evolution Plans**:

- DynamoDB Streams-based CDC (Change Data Capture) for true real-time sync
- Enhanced schema evolution and inference support
- Extension to other NoSQL databases (MongoDB, Cassandra)

That said, the current solution fully meets Thrivent's needs and works for the vast majority of AWS customers. Sometimes "good enough" really is good enough—not every problem demands the most complex solution.
  </markdown_content>
</document>
<document>
  <title>2022-02-14 - Design Lab - national healthcare cost management company</title>
  <markdown_content>
# 2022-02-14 - Design Lab - national healthcare cost management company

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

I was assigned to a national healthcare cost management company specializing in radiology services and diagnostic imaging networks for employers, health plans, and municipalities. The customer operated a multi-source data ecosystem where eligibility data from insurance companies, authorization data from physicians, and insurance claim data from patients flowed through an existing Boomi ETL pipeline into Salesforce, which served dual purposes as both their CRM system and analytics data warehouse. The customer faced significant data quality challenges due to inconsistent schemas from external data providers, a labor-intensive ETL process requiring up to two months of manual intervention, and performance concerns about running heavy analytics workloads on their production CRM system.

## Task

I was responsible for designing a comprehensive cloud-based data architecture during a two-day Design Lab engagement that would accelerate their data processing timeline from two months to under two weeks, enable scalable analytics capabilities, and evaluate cost-effective alternatives to their existing tooling. The deliverables included designing an end-to-end data ingestion pipeline to handle multiple data sources (direct uploads, FTP servers, Windows File Servers, API requests, and Salesforce objects), architecting a central data lake with proper data governance, implementing fine-grained access controls, and evaluating Amazon QuickSight as a potential replacement for Tableau and Snowflake to reduce software licensing costs.

## Situation

I designed a three-tier data lake architecture using Amazon S3 with raw, staging, and curated buckets to preserve ground truth data, validate and cleanse incoming data, and transform data into analytics-friendly Apache Parquet format.

**Data Pipeline Implementation:** I architected a dual-stage AWS Glue ETL process where the first job performed schema validation, data cleansing, and type checking to create trusted data, while the second job handled transformation, deduplication, and denormalization to produce analytics-ready datasets partitioned by business logic time.

**Integration Strategy:** I designed integration points with their existing Boomi ETL system by inserting data dump operations before processing logic, implemented scheduled AWS Lambda functions using SOQL to extract Salesforce data, and incorporated AWS Connect for contact center data ingestion.

**Data Governance Architecture:** I designed a scalable access control solution using AWS Lake Formation's tag-based access model (LF-TBAC) to provide database, table, column, and row-level security without the management overhead of traditional resource-based policies, with AWS Glue Crawler automating metadata catalog generation and AWS Glue Data Catalog serving as the unified metadata store.

**Analytics and Visualization:** I architected a serverless analytics layer using Amazon Athena for ad-hoc SQL queries with optional Amazon Redshift clusters for complex JOIN operations and feature engineering, integrated with Amazon QuickSight for KPI dashboards and self-service business intelligence.

**Operational Excellence:** I incorporated AWS CloudWatch for ETL monitoring, Amazon SNS for alerting, AWS Step Functions for workflow orchestration, and Amazon DynamoDB to maintain runtime mappings between raw and curated data for lineage tracking.

## Results

I successfully delivered a comprehensive cloud-native architecture that addressed all three use cases and positioned the customer to reduce their data processing timeline by 75% while eliminating infrastructure management overhead. I provided detailed recommendations for follow-up activities including AWS Glue ETL immersion day training, Lake Formation access control workshops, and QuickSight comparative analysis sessions to ensure successful implementation during their subsequent Build Lab engagement.
  </markdown_content>
</document>
<document>
  <title>2022-04-20 - Design Lab - Fortune 100 mutual life insurance company</title>
  <markdown_content>
# 2022-04-20 - Design Lab - Fortune 100 mutual life insurance company

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

A Fortune 100 mutual life insurance company and one of the largest life insurers in the United States was building an enterprise data platform using AWS native analytics services and had recently selected Amazon Managed Streaming for Apache Kafka (MSK) over a competitor solution. The company faced critical challenges in migrating their existing production workload—running across an on-premises Kafka cluster and a self-managed cloud Kafka cluster with 158 topics, 82 producers, and 121 consumers—to AWS MSK. Key constraints included maintaining secure connectivity across hybrid cloud and multi-account environments, managing permissions for numerous applications running on different platforms (Mule API, Spring Boot, LogStash, Debezium CDC, DynamoDB Streams, and Spark Stream), and minimizing service downtime while ensuring zero data loss, strong ordering, and no double-consumption during migration.

## Task

I was responsible for architecting a comprehensive migration solution during a two-day AWS Data Lab Design Session that would enable the customer to transition from their existing Kafka infrastructure to AWS MSK with minimal disruption. The engagement required designing a hybrid cloud network architecture connecting on-premises systems with multiple AWS accounts, defining appropriate authentication and authorization methods for six distinct application types, developing a detailed migration plan with near-zero downtime, and establishing a scalable, multi-tenant Data Stream Hub architecture for future growth. The deliverables included architecture diagrams, step-by-step migration workflows, authorization mapping for each application type, and recommendations for Build Lab preparation to validate the design through hands-on implementation.

## Action

I designed a **hybrid cloud network architecture** using AWS Transit Gateway as a central hub connecting the on-premises data center, multiple AWS accounts, and VPCs, with AWS Direct Connect providing secure, high-performance connectivity that bypassed the public internet. For **authentication and authorization**, I evaluated each application's technical capabilities and recommended tailored solutions: Kerberos + SASL for Mule API and LogStash (leveraging native support), IAM authentication for Java applications using the aws-msk-iam-auth library, MSK Connector with IAM for Debezium CDC pipelines, and Kerberos + SASL for Spark Stream processors. I created a **zero-downtime migration strategy** using a "divide-and-conquer" methodology that broke the complex migration into atomic operations: preparing parallel MSK infrastructure with temporary MirrorMaker instances, sequentially stopping producers, waiting for complete message consumption, switching consumers to MSK using synced offsets to prevent double-consumption, and restarting producers with new MirrorMaker configurations—resulting in downtime limited to 1-3 seconds per partition. The solution guaranteed strong message ordering by ensuring all messages in the old cluster were consumed before starting consumption from MSK, and I recommended Python automation scripts to orchestrate the timing of these operations. For the **Data Stream Hub architecture**, I designed a scalable solution leveraging MSK Connectors to simplify data ingestion and consumption wherever possible, with Kafka client APIs for unsupported runtimes, and proposed using AWS CDK for infrastructure-as-code to enable replication across business units, AWS CloudWatch for monitoring with automated scaling triggers via SNS, and MSK Storage auto-scaling to ensure adequate capacity.

## Results

I successfully delivered a comprehensive, production-ready architecture that addressed all three use cases with detailed design documentation, authorization mapping for six application types, and a step-by-step migration workflow. The customer team validated the approach and agreed to proceed with a Build Lab focused on executing the migration operations with dummy MSK clusters to practice the orchestration. I provided specific recommendations for MSK sizing, permission management automation, and rehearsal procedures to de-risk production migration.
  </markdown_content>
</document>
<document>
  <title>2024-11 to 2025-10 - Enterprise MCP Infrastructure</title>
  <markdown_content>
# 2024-11 to 2025-10 - Enterprise MCP Infrastructure



## Overview

In November 2024, Anthropic released the Model Context Protocol (MCP) standard. I saw an opportunity and led the buildout of enterprise-grade MCP infrastructure at Travelers Insurance. The challenge? The standard was barely a month old and evolving rapidly. We tackled this by taking an incremental approach - delivering working solutions fast while keeping the architecture flexible enough to adapt as the standard matured.

The results speak for themselves. We deployed a complete MCP Server development framework and Registry platform. Development time for a single MCP Server dropped from one month to one day - a 30x improvement. Today, this infrastructure powers the 5-6 million AI inference requests our company makes every month.

## Background

### Travelers' AI Strategy and Tech Environment

Travelers Insurance is one of the major players in US insurance. Our cloud footprint spans more than 50 AWS accounts. The tech stack is pretty standard for enterprises our size - EKS runs all our internal APIs, S3 handles storage, we use DocumentDB for unstructured data and Aurora PostgreSQL for relational databases, with Lambda powering our microservices.

We're going hard on AI adoption. Employees work daily with Confluence for documentation and collaboration, JIRA for project management, Workday for HR, and our MongoDB and PostgreSQL databases for business operations.

I joined Travelers in September 2023 as an Enterprise Architect. Before that, I was at AWS, which gave me solid grounding in their service ecosystem. I'd already done consulting work for Travelers and delivered some successful projects, so they knew what I could do. I'm one of three architects focused specifically on AI strategy, reporting to our VP of Architecture. The broader AI team has over 20 architects, and I've got about 9-10 engineers working on implementation.

### AI Application Scale and Data Access Challenges

By late 2024, our AI applications were making roughly 5-6 million inference requests per month. That's just the machine-to-machine calls. Add in the human side - over 1,000 employees using AI tools, averaging maybe 10 messages per day - and you're looking at another 300,000+ interactions monthly just from conversations.

Here's the problem we had. Those 20 AI applications? Each one built its own tools to access enterprise systems. We ended up with 500+ tools across all these apps, and more than 200 of them were doing some flavor of database access. Massive code duplication. But the bigger issue was what we couldn't do - our AI applications had no safe way to access Confluence, JIRA, or Workday. That limitation was holding back what we could actually accomplish with AI.

### Model Context Protocol as an Industry Opportunity

Then Anthropic dropped MCP in November 2024. It became the industry standard for AI-to-data integration almost overnight. The core idea is simple but powerful - give AI models a unified way to safely access any data source. Files, databases, APIs, enterprise apps, whatever you need.

The whole industry jumped on it. Every major AI platform, every enterprise software vendor started supporting MCP. For Travelers, the math was pretty straightforward. If we wanted to stay competitive in AI, we needed to get on this fast. Build enterprise-grade MCP infrastructure so our internal apps could tap into core systems using this standard approach.

We weren't under direct competitive pressure, but we could see where things were heading. Fintech vendors in insurance were already rolling out AI-powered solutions. Those use cases validated that AI works in our industry. Since the direction was proven, and we had the capability, it made sense to move quickly.

### The Immaturity of Early MCP

But here's the catch. When MCP launched in November 2024, it was designed for personal, local use. The enterprise capabilities we needed just weren't there yet.

**Remote deployment didn't exist.** Early MCP ran locally on developer machines. We needed remote servers for centralized management and high availability, but the community only had hacky workarounds. The typical approach? Wrap your MCP in an HTTP API, run a local proxy to forward requests. It worked, sort of, but the performance was terrible, security was questionable, and maintenance was a nightmare.

**No permission management.** The initial design assumed users would manually paste their auth tokens into config files. That's fine for personal projects, but completely unworkable in an enterprise. We needed real permission management. Employee A can read team A's Confluence but not team B's. Employee B can view JIRA tickets but not edit them. That kind of granular control didn't exist in early MCP.

**Rapid evolution.** Because everything was so new, the ecosystem moved incredibly fast. New implementations, new best practices, constant changes. Nothing felt settled. Remote deployment didn't mature until May 2025. OAuth and proper authorization didn't stabilize until April 2025. As an architect, I had to make decisions knowing that whatever we built might need rework in six months.

### My Unique Advantages

I'd been deep in the MCP space since day one, building open source projects that put me ahead of the curve.

I built a **Google MCP Server** right after the standard launched - Gmail and Google Drive integration, four months before Google's official version shipped.

My **Atlassian MCP Server** connected JIRA and Confluence six months before Atlassian released theirs.

And I authored an **SQL MCP Server** for querying databases across different engines.

These projects didn't make me internet famous or anything. I'm decent at writing code but not great at marketing myself. Still, inside Travelers, it mattered. I'd share updates in our work channels, post about progress on LinkedIn where colleagues and management would see it. When the company started talking seriously about MCP, everyone knew I was the person who'd been living and breathing this stuff. That's why they tapped me to lead the infrastructure work.

## Situation

### Urgency of the Enterprise AI Strategy

By late 2024, Travelers was at an inflection point with AI. We had 20 applications in production generating 5-6 million inference requests monthly. Add over 1,000 employees using TravAI - our internal ChatGPT-style system - for daily work, and AI had become deeply embedded in how the company operates.

But we'd hit a wall. Without MCP, our AI apps couldn't touch Confluence, JIRA, Workday, or our databases in any safe, standardized way. Those 20 applications each rolled their own integration tools - 500+ tools across everything, 200+ just for database access. Mountains of duplicated code and maintenance headaches. Worse, the approach didn't scale. Every new data source meant reinventing the wheel.

We needed a standard way for AI applications to securely access internal data. Not a rush to ship X number of apps in the next quarter, but infrastructure that could support our AI strategy for the next 3-5 years. This was strategic-level work, exploratory in nature, and meant to set us up for the long haul.

### MCP Standard Uncertainty and Technical Risk

When I started pushing MCP internally in November 2024, the industry hadn't picked a winner yet. Anthropic had MCP, OpenAI had Tool Use. Both competing to become the standard. I believed MCP was the better choice - more open design, better abstractions for enterprise permission models, stronger ecosystem compatibility. But leadership wasn't ready to bet on my judgment alone. They wanted to see market signals before committing resources.

That waiting game lasted until March 2025. By then, MCP had clearly won - mainstream platforms and vendors were all adopting it. That's when we got official project approval, team formation, budget. Though everyone was still splitting time with other work.

During those waiting months from November to February, I did three things that set us up for success later. First, I tracked every MCP-related open source project, evaluated technical directions and maturity, especially around remote deployment and authorization. Second, I kept building my open source MCP servers - Google, Atlassian, SQL. That hands-on work gave me deep knowledge of MCP's strengths and limitations. Third, I started conversations with our security teams early about enterprise permission requirements, making sure future designs would get their buy-in.

By February 2025, while the company was still waiting for market confirmation, I'd already written working MCP code in my personal projects. That head start made all the difference.

### Why I Led This Project

My early MCP work and open source contributions made me the obvious choice internally. When leadership started serious discussions about MCP in early 2025, they came to me. Other teams were working on MCP too, but for specific application needs. I was being asked to build company-level infrastructure - the foundation everyone else would use.

My advantage wasn't just coding ability. Three things set me apart. First, I'd correctly called MCP over OpenAI's Tool Use when the outcome wasn't obvious. Second, I understood where the open source ecosystem was heading - what would mature, what would fade. Third, I had years of Python framework experience and knew how to design abstraction layers that could adapt as standards evolved.

This combination made me the right person to build infrastructure in the middle of uncertainty.

## Task

### My Role and Responsibilities

I led this project as Enterprise Architect - overall architecture, technology choices, standards, core implementation. I had 9-10 engineers on my team, but during the first six months (POC phase), most of the heavy lifting fell to me, another architect, and a senior engineer. Everyone else was juggling multiple projects. I put roughly 80% of my time into MCP.

I worked closely with several groups. Our Enterprise Security and Cyber Security teams were full partners - any authentication or authorization design had to get their sign-off. The AI application teams were our end users, and their feedback shaped the framework. And I reported progress to , our VP of Architecture, who handled strategic direction and resources.

### Project Goals and Constraints

The goal was straightforward - build a standard way for AI applications to securely access enterprise MCP Servers. Not about shipping features fast, but about creating infrastructure that could evolve sustainably over 3-5 years.

Success hinged on one critical milestone: **get Atlassian MCP Server working in production with full OIDC and OAuth integration, then turn it into a reusable template**. Why Atlassian? Because it represented every hard problem we'd face - remote deployment, fine-grained authorization, Okta integration, OAuth with third-party services. Nail Atlassian, and everything else (databases, Google services) would be relatively easy.

We faced three major constraints. First, technical uncertainty - the MCP standard was evolving fast, so our architecture needed flexibility. Second, security requirements - as a regulated insurance company, every data access design needed security team approval, which added complexity. Third, limited resources - everyone was splitting time across projects, so we needed a "move fast" strategy that delivered value quickly while keeping the architecture adaptable.

### Two Core Deliverables

**Deliverable 1: MCP Server Development Framework**

A complete development and deployment standard with four pieces:

- **Python SDK** - Handles all the common stuff developers would otherwise rebuild: authentication, authorization, logging. Saves tons of duplicate work.
- **Template Repository** - Cookie Cutter templates to spin up new MCP Servers quickly
- **CI/CD Pipeline** - GitHub Actions workflows that handle testing through deployment automatically
- **Deployment Scripts** - Shell scripts and tooling for EKS deployment

This framework is mandatory for all remote MCP Servers. Local development can use whatever, but production has to follow the standard. That's how we maintain consistency across the enterprise.

**Deliverable 2: MCP Registry Platform**

An internal platform for discovering and managing MCP Servers, built with FastAPI, Python, and MongoDB. Core features:

- **Discovery** - Web UI and APIs for finding available MCP Servers, viewing docs and examples
- **Access Control** - Two-tier permissions: who can publish, who can install and use
- **Metadata Management** - Centralized version info, dependencies, configuration

The Registry focuses purely on metadata. It doesn't handle actual installation or deployment - just provides documentation and guides for manual setup.

### Project Timeline and Phase Strategy

The project ran from November 2024 to September 2025 - ten months total, still ongoing. With MCP evolving so rapidly, we couldn't do traditional waterfall. Instead, we used an incremental approach I call "move fast, stay flexible."

**First 6 Months (November 2024 - April 2025): Get It Working**

Goal: Build something that functions end-to-end, even if the implementation is rough. Use workarounds where needed. The milestone was getting Atlassian MCP Server running on EKS with basic OAuth working.

**Last 4 Months (May 2025 - September 2025): Make It Production-Ready**

Goal: Clean up the implementation, modularize everything, swap workarounds for proper solutions as community standards mature. Ship the framework template, launch the Registry, start onboarding other teams.

The philosophy: keep the external interface stable (the black box), but continuously improve what's under the hood (the gray box). That way we deliver value to stakeholders fast while tracking community evolution and avoiding technical debt pileup.

## Action

### Phase 1: Technical Direction Judgment and Project Launch (November 2024 - February 2025)

**Picking the Right Standard and Getting Buy-In**

The day Anthropic released MCP in November 2024, I started evangelizing it internally. But we had a problem - two competing standards, MCP and OpenAI's Tool Use, and nobody knew which would win. I believed MCP was the better bet. More open design, better abstractions for enterprise permissions, stronger ecosystem potential.

Leadership wasn't sold. They needed market proof, not my gut feeling. This standoff lasted until March 2025. By then, MCP's victory was clear - every major platform had adopted it. That's when we got official approval, team formation, budget allocation. Everyone still working part-time across multiple projects, but at least we had the green light.

During the November-to-February waiting period, I wasn't idle. I did three things that paid off huge later. First, I tracked every MCP project in the wild - evaluated their architectures, watched for patterns in how they handled remote deployment and auth. Second, I kept shipping my open source MCP servers. Google, Atlassian, SQL - each one taught me something about MCP's real strengths and weaknesses. Third, I got ahead of the security conversation. Started talking with our Enterprise Security and Cyber Security teams about what enterprise permissions would need to look like. By the time we got official approval, I'd already laid groundwork with the teams who could kill the project later.

By February 2025, I had working MCP code in my personal repos while the company was still deciding whether to commit. That head start was everything.

**Architecture Philosophy: Design for Change**

With MCP evolving so fast, I knew the standard approach - "design it perfectly upfront" - would fail. Instead, I built around a three-layer architecture that isolated change.

The **abstraction layer** defines core concepts - what a Tool is, what Authentication means, what Authorization looks like. This layer stays stable.

The **implementation layer** is where business logic lives - how a JIRA tool actually works, what Confluence integration does. This layer evolves slowly.

The **integration layer** connects to external systems - Atlassian's OAuth API, Okta SSO, the MCP protocol itself. This layer changes fast and often.

When MCP's authorization model evolved from manual tokens to OAuth, we rewrote the integration layer but left abstraction and implementation alone. When we needed to support Microsoft OAuth alongside Atlassian, same story - changes stayed in the integration layer.

This design came from years of maintaining Python frameworks. I've seen dependency APIs change and break everything downstream. The fix is always the same - clear interfaces, isolated external dependencies. When something changes, the blast radius stays small.

### Phase 2: Remote Deployment Evolution (November 2024 - May 2025)

**Early Workaround: HTTP Proxy Hack**

From November 2024 through April 2025, mature remote deployment for MCP simply didn't exist. The community hack was projects like mcp-remote - basically a fake local MCP Server that forwards everything to a remote HTTP API.

This approach sucked. Every MCP call went through the local proxy, adding latency. The proxy stored credentials locally, adding security risk. Each developer configured their own proxy, making troubleshooting a nightmare.

But we had no choice. I used this workaround to get our first demo running - Atlassian MCP Server deployed as an HTTP API, accessed through a proxy in the [http://trav.ai](http://trav.ai)  backend. Ugly, but it proved the concept worked.

The key move was keeping "how we call remote MCP" isolated in the integration layer. Upper-layer code didn't know or care whether we used HTTP proxy or native MCP protocol. That isolation made the later migration smooth.

**Migrating to Native Remote MCP**

In May 2025, everything shifted. Web frameworks started natively supporting MCP remote deployment. AWS Lambda added serverless MCP support. Amazon Bedrock rolled out Agent Core with fully-managed MCP runtime.

I evaluated our options fast. For Travelers, the real choice was EKS versus Agent Core.

**EKS** gave us complete control and fit our existing infrastructure. Security would love it. But we'd have to build logging, monitoring, memory management, everything. Heavy operational lift.

**Agent Core** was AWS fully-managed. Built-in logging, memory, monitoring - production-ready out of the box. High development efficiency. But it's multitenant, no physical isolation. Security might balk.

Personally, I leaned Agent Core. The maturity and feature completeness were miles ahead of EKS. But I could see the political reality - our company culture favors "physical isolation equals more secure," even when that's not technically true.

So I played it smart. **Build the complete POC on EKS, but keep the architecture Agent Core-compatible.** How? Deployment logic lives in shell scripts - switching targets means editing configs, not rewriting code. MCP Server code doesn't depend on runtime specifics - runs on either platform without changes. Framework documentation covers both deployment paths.

As of October 2025, we're production on EKS with Atlassian MCP Server. But we're also evaluating Agent Core migration. This dual-track approach means we're ready regardless of which way leadership eventually goes.

### Phase 3: Permission Management Architecture (November 2024 - May 2025)

**Early Days: Manual Tokens and Vault**

Before April 2025, MCP's authorization model was basically "users paste tokens into config files." That's a non-starter in enterprise environments - we can't have AI app developers manually managing Confluence, JIRA, and database credentials.

My temporary fix was HashiCorp Vault as central secrets storage. The flow worked like this. All authentication tokens live in Vault, each with a unique ID. MCP Server configs contain only Vault secret IDs, never actual credentials. At runtime, MCP Server pulls the real token from Vault based on ID. IAM policies control which MCP Servers can access which Vault secrets.

Not elegant - still requires preconfiguring tokens in Vault - but at least credentials were centralized. No hard-coded secrets in code or configs. And critically, this design had a path to OAuth. Vault secret IDs could point to OAuth tokens instead of static credentials. When OAuth matured, we'd update what those IDs pointed to without changing the architecture.

**OAuth and OIDC Integration: Real Enterprise Auth**

After April 2025, MCP's authorization spec matured ([https://modelcontextprotocol.io/specification/draft/basic/authorization](https://modelcontextprotocol.io/specification/draft/basic/authorization) ). The community added OAuth 2.0 and OIDC support. Major vendors - Atlassian, Microsoft, Google - launched MCP OAuth integration.

I immediately started integrating this into our architecture. The design uses **three permission tiers**.

**Tier 1: Enterprise Identity (Okta SSO)**

For human users accessing MCP through TravAI, we use Okta as Identity Provider. User logs into TravAI through Okta. Okta issues a JWT containing user identity and enterprise roles. When AI calls MCP Server, that JWT passes as context. MCP Server validates the JWT and confirms user identity.

**Tier 2: Third-Party OAuth (Atlassian/Microsoft/Google)**

For accessing external systems - JIRA, Confluence, SharePoint, Gmail - we use OAuth 2.0. MCP Server acts as OAuth client, registered with the third-party system. On first use, MCP Server guides the user through OAuth authorization. Third-party system issues access token. MCP Server stores it in Vault, tied to user identity. For subsequent calls, MCP Server retrieves that user's access token from Vault and calls the API.

**Tier 3: Fine-Grained Authorization (Resource-Level)**

For granular permissions - like read/write access to specific JIRA Boards - my strategy is controversial but pragmatic: **don't abstract, just pass through the service provider's native permissions**.

I could've built a unified permission model - some generic "Action + Resource" abstraction that maps JIRA, Confluence, and database permissions to one system. But I chose not to for three reasons.

First, **complexity explosion**. JIRA's permission model is insanely complex - project-level, board-level, issue-level permissions, role-based and user-based combinations. Confluence is completely different - space-level, page-level with inheritance. Building a unified abstraction would cost more than it's worth.

Second, **information loss**. Any unified abstraction loses expressive power from native models. JIRA's dynamic permissions like "Assignee can edit" don't map cleanly to generic models.

Third, **enterprise reality**. Our employees already know JIRA and Confluence permissions. IT already has mature permission management processes. Adding a new abstraction layer creates cognitive overhead without solving actual problems.

So instead, MCP Server directly uses the user's OAuth token when calling JIRA API. JIRA backend does authorization based on that user's permissions in JIRA's own system. If the user can't access a board, JIRA returns 403, MCP Server passes that through to the AI app. Permission complexity lives where it belongs - in the service provider, not our infrastructure.

**Machine Applications: Service Accounts and API Keys**

Besides human users, we have machine applications using MCP - scheduled jobs, background tasks, automated workflows. No interactive OAuth flow possible.

My solution: **use Service Accounts where supported, API Keys otherwise**. For systems like Google Workspace that support service accounts, we create dedicated accounts with minimal permissions. For systems that only support API Keys - some databases, for example - we store keys in Vault with IAM policy-controlled access.

The Framework provides clear documentation and code templates for both patterns so developers can implement whichever fits their use case.

**What Went Wrong: Overly Conservative Permissions**

Permission implementation had one major pain point - not technical, but operational. Security teams demanded very conservative default permissions. Result? Developers constantly hit "insufficient permissions" errors. Each one required approval processes to request more access. Development velocity tanked, and people started questioning whether MCP was actually making their lives easier.

Big lesson learned. In enterprise environments, architecture design isn't just about security - it's about **operability**. Overly conservative permissions, while theoretically more secure, actually push developers to find workarounds. They request excessive permissions "just in case" to avoid the approval loop. That paradoxically reduces security.

We're now working on role-based permission presets. Common roles like "AI Application Developer" automatically get a standard set of permissions for common MCP Servers (SQL, Atlassian). Reduces manual approval friction while maintaining security where it matters.

### Phase 4: MCP Server Development Framework (March 2025 - August 2025)

**Framework Core Components**

After the POC phase, I started hardening everything into a reusable Framework in March 2025. Four core components.

1. **Python SDK (travelers-mcp-sdk)**

This is the Framework's heart. It encapsulates everything developers would otherwise rebuild:

**Authentication Module** - Handles Okta SSO, OAuth 2.0 flows, Vault secrets. Developers just specify in config which auth method to use. SDK manages token acquisition, refresh, storage automatically.

**Authorization Module** - Provides unified authorization check interfaces. We don't abstract permission models, but the SDK has helper functions that make calling service provider authorization APIs easy.

**Logging and Auditing Module** - Automatically records all MCP tool calls. Who called it, what was called, parameters, results. Logs go to centralized infrastructure (AWS CloudWatch) for compliance.

**Error Handling Module** - Unified retry logic. OAuth token expired? Auto-refresh and retry. Service provider rate limiting? Auto-backoff and retry.

**MCP Protocol Layer** - Handles MCP protocol details. Request/response serialization, deserialization, validation. When MCP protocol updates, we update the SDK and all MCP Servers using it become compatible automatically.

Design philosophy: **developers focus on tool logic, not infrastructure**. They don't need to understand MCP protocol internals, OAuth flows, or auth complexity. Just implement the business logic.

2. **Template Repository (mcp-server-cookiecutter)**

Cookie Cutter-based project template providing scaffold code:

- Standard project structure (src/, tests/, docs/, deploy/)
- Preconfigured CI/CD pipeline (GitHub Actions)
- Example tool implementation (shows how to use SDK)
- Config file templates (auth, authorization, logging)
- README and development docs

Run cookiecutter command, answer a few questions (MCP Server name, which service provider, which auth methods), and the template generates a complete project framework.

3. **CI/CD Pipeline (GitHub Actions)**

Every MCP Server generated from the template includes a complete CI/CD pipeline:

- **Linting and Testing** - Auto-runs pytest, flake8, mypy for code quality
- **Security Scanning** - Uses bandit and safety for vulnerability and dependency checks
- **Build and Package** - Builds Docker image, pushes to enterprise container registry
- **Deployment** - Auto-deploys to dev/staging/production based on branch

The pipeline configuration is highly standardized. Developers don't need to understand CI/CD - just push code to GitHub and the pipeline handles everything from tests to production deployment.

4. **Deployment Scripts (Shell Script + Terraform)**

Scripts handle MCP Server deployment on EKS:

- Terraform defines infrastructure (EKS cluster, IAM roles, security groups)
- Helm charts deploy MCP Server to Kubernetes
- Auto-configures monitoring and alerting (CloudWatch metrics and alarms)
- Provides rollback mechanism (auto-rollback on deployment failure)

Scripts are designed for EKS/Agent Core compatibility. Switching deployment targets means changing one config parameter.

**Why Framework is Mandatory**

The Framework is required for all remote MCP Servers. Three reasons.

First, **security compliance**. Security teams require all data access systems to be audited. Mandating Framework use ensures every MCP Server implements unified auth, authorization, and logging that meets compliance.

Second, **maintainability**. If developers could choose any tech stack, protocol upgrades or policy changes would require modifying each MCP Server individually. With mandated Framework use, we just upgrade the SDK and all MCP Servers automatically get new features and bug fixes.

Third, **lowering barriers**. For developers unfamiliar with MCP, the Framework provides a clear starting point. They don't need to learn MCP protocol from scratch - just follow templates and SDK APIs to build production-ready servers quickly.

For local MCP (experiments and prototypes), we don't mandate anything. Developers can use whatever tech stack they want. Only requirement is adopting Framework when moving to production. Keeps innovation flexible.

**Framework Impact: 30x Faster Development**

Results were dramatic. Before Framework, developing and deploying an MCP Server took roughly one month:

- 1 week: understand MCP protocol and enterprise auth/authorization requirements
- 1 week: implement tool logic
- 1 week: configure CI/CD and deployment
- 1 week: troubleshooting and security approvals

With Framework, simple MCP Servers (like GetWeather) take one day:

- 1 hour: cookiecutter generates project framework
- 4 hours: implement tool logic
- 2 hours: configure auth and authorization
- 1 hour: testing and deployment

Complex MCP Servers (like Atlassian with dozens of tools) take 1-2 weeks. Still dramatically faster than the previous one-month cycle.

Developer feedback has been overwhelmingly positive. They don't worry about auth, logging, deployment - the "undifferentiated heavy lifting." They just focus on business logic. That was the whole point.

### Phase 5: MCP Registry Platform (May 2025 - September 2025)

**Registry Scope: Why Lightweight?**

When designing the MCP Registry, I faced a key question - what should it actually do?

Looking at the community, Anthropic has an official MCP Registry project (maintained by Astral), written in Golang. It's designed like PyPI - a public, open ecosystem where anyone publishes, anyone discovers and installs.

That design doesn't fit our enterprise needs. Three mismatches.

First, **closed versus open**. We're a closed enterprise. Don't need public discovery. Registry only serves Travelers internal employees and applications.

Second, **tech stack mismatch**. Official Registry uses Golang. We don't have Golang expertise. Adopting it would create maintenance and customization problems.

Third, **standards still evolving**. Official Registry hasn't become the de facto standard. Community is still moving fast. If we fully adopt now and standards shift, we might face massive refactoring.

Based on this, I decided: **build a lightweight in-house Registry, scope limited to metadata management**. No automated installation, no complex lifecycle management.

Specific scope:

- **Discovery** - Web UI and API for searching and browsing available MCP Servers
- **Metadata Management** - Store MCP Server names, versions, descriptions, authors, dependencies, config examples
- **Access Control** - Manage who can publish and who can use MCP
- **Documentation** - Each MCP Server includes Markdown installation guide for manual setup

Explicitly excluded:

- Automated Installation - Registry doesn't deploy MCP Servers, just provides docs
- Lifecycle Management - Doesn't manage version upgrades, deprecation, removal (CI/CD handles that)
- Monitoring and Analytics - Doesn't collect usage metrics (logging infrastructure handles that)

This lightweight approach has two benefits. First, **low development cost**. Registry is essentially a CRUD app managing metadata. With FastAPI + MongoDB, we shipped in a month. Second, **low evolution risk**. When community Registry standards mature, migration is cheap - just export MongoDB data, convert format, import to new system.

**Two-Tier Permission Model**

One of Registry's core values is access control. I designed an innovative two-tier model: Publish Blacklist + Installation Whitelist.

**Publish Uses Blacklist**

By default, any Travelers employee can publish MCP Servers to Registry. Only people explicitly on the blacklist can't publish. This encourages innovation and lowers contribution barriers.

Design philosophy: **trust by default, restrict when necessary**. Enterprise employees undergo background checks. We should assume good intent. Only when someone abuses the privilege (like publishing malicious code) do we blacklist them.

**Installation Uses Whitelist**

By default, newly published MCP Servers are "private" - only the author can use them. Only after approval and whitelisting can others install and use them. Whitelist approval includes code review, security scanning, functional testing.

Design philosophy: **quality gate before adoption**. We don't know each MCP Server's quality and security. Approval required before broader use ensures Registry contains only production-ready servers.

This two-tier model balances innovation (easy to publish) with quality (strict approval before use).

**Registry Technical Implementation**

Simple tech stack:

- **Backend** - FastAPI (Python), provides RESTful API
- **Database** - MongoDB, stores metadata (JSON format, flexible schema)
- **Frontend** - Simple web UI, mostly hand-written HTML + JavaScript (not complex React framework)
- **Authentication** - Okta SSO integration, users log in with enterprise accounts

Core data model:

```
MCPServer {
    id: ObjectId
    name: str  # unique MCP Server name
    version: str  # semantic version
    description: str  # brief description
    author: str  # author (Okta user ID)
    created_at: datetime
    updated_at: datetime
    
    # Metadata
    service_provider: str  # e.g. "Atlassian", "Google", "SQL"
    authentication_type: List[str]  # e.g. ["OAuth", "API Key"]
    tools: List[ToolMetadata]  # included tools
    
    # Documentation
    readme: str  # Markdown README
    installation_guide: str  # installation guide
    configuration_example: str  # config file example
    
    # Access Control
    publish_status: str  # "published" or "blacklisted"
    installation_status: str  # "private" or "whitelisted"
    whitelist_approver: str  # approver (if whitelisted)
    whitelist_approved_at: datetime
}

```

API endpoints:

- `POST /api/mcp-servers` - Publish new MCP Server (auth required)
- `GET /api/mcp-servers` - List available MCP Servers (filtered by permissions)
- `GET /api/mcp-servers/{id}` - Get specific MCP Server details
- `PUT /api/mcp-servers/{id}` - Update metadata (author only)
- `POST /api/mcp-servers/{id}/request-whitelist` - Request whitelist approval
- `POST /api/mcp-servers/{id}/approve-whitelist` - Approve whitelist (admins only)

Web UI provides:

- **Discovery Page** - Search and browse MCP Servers, categorized by service provider
- **Detail Page** - Display server details, README, installation guide
- **Publish Page** - Publish new server (upload JSON metadata)
- **Admin Page** - Manage blacklist and whitelist

**Registry Actual Usage**

As of October 2025, Registry is live with dozens of published MCP Servers:

- Atlassian MCP Server (JIRA + Confluence)
- Google MCP Server (Gmail + Google Drive)
- SQL MCP Server (PostgreSQL and MongoDB support)
- Other internal servers (HR, Finance systems)

Most are still "private" though. Only Atlassian MCP Server completed whitelist approval and is in official production use.

Slow rollout has two reasons. First, **quality evaluation**. Despite Framework use, different developers have different approaches to tool design and error handling. We're reviewing each individually to ensure production standards. Second, **AI application onboarding**. Migration of 20 existing AI apps just started. We don't want to open floodgates and overwhelm support. Strategy is pilot a few applications with Atlassian MCP Server, collect feedback, optimize Framework, then gradually expand.

Registry's value isn't current MCP Server count. It's **establishing standardized discovery and access control**. When we have 100+ MCP Servers in the future, Registry becomes indispensable infrastructure.

### Phase 6: Continuous Evolution and Community Sync (May 2025 - Present)

**Abstraction Layer Validation**

As the project progressed and MCP standard + community ecosystem underwent major changes, my early abstraction layer design proved its worth repeatedly.

**Case 1: Remote Deployment Evolution**

Migrating from HTTP proxy workaround to native remote deployment required only integration layer changes. Modified deployment scripts from deploying proxy to deploying native MCP Server. Updated SDK protocol layer to use MCP remote protocol instead of HTTP forwarding. Tool logic, auth, authorization stayed completely unchanged. Migration completed in one week without affecting any AI applications using MCP.

**Case 2: Logging Refactoring**

We initially used open source frameworks for recording MCP tool calls. Later realized these couldn't meet enterprise-specific needs - we needed to track "which AI application made the call" and "which business unit triggered it," information not defined in standard frameworks.

We built our own logging module. Thanks to abstraction layer, refactoring went smoothly. Defined LoggingInterface in abstraction layer with methods like log_tool_call() and log_error(). Removed open source framework dependency, implemented our own LoggingService. Upper-layer code calls abstraction layer logging interfaces, doesn't know about underlying implementation changes. Refactoring completed in two weeks, proving abstraction layer flexibility.

**Case 3: Adding OAuth Providers**

Initially only supported Atlassian OAuth. Later needed Microsoft (SharePoint, Teams) and Google (Gmail, Drive). Through abstraction layer, extension was simple. Added MicrosoftOAuthProvider and GoogleOAuthProvider classes in SDK implementing unified OAuthProviderInterface. Specify provider in config, SDK auto-handles OAuth flow. Developers don't need to understand each provider's OAuth details, just configure client ID and secret. Each new provider took 1-2 days, dramatically reducing cost of supporting new systems.

**Open Source Community Interaction**

While enterprise MCP infrastructure is homegrown, I've maintained active community engagement.

**Contributing Back**

Some general functionality I developed in enterprise (like OAuth integration helpers), after removing enterprise-specific logic, got contributed back to my open source MCP Server projects. I share enterprise MCP deployment experience and lessons learned on LinkedIn and GitHub, helping other enterprise architects avoid pitfalls.

**Learning from Community**

I closely track Anthropic's official MCP spec updates and evolution of mainstream frameworks (FastMCP, mcp-python). I participate in MCP-related GitHub discussions and issue triage, understanding community pain points and feature requests. When community has mature solutions, I evaluate whether to replace homegrown implementations.

This two-way interaction lets our enterprise infrastructure benefit from customization while continuously gaining from community innovation.

**EKS versus Agent Core: Still Deciding**

As of October 2025, final choice between EKS and Agent Core isn't locked in.

I've completed Agent Core technical validation. Proved Atlassian MCP Server runs on Agent Core without modifications, performance and reliability meet requirements. Prepared detailed comparative analysis and submitted to leadership and security teams.

My recommendation is migrate to Agent Core. Three reasons. First, **maturity** - Agent Core's logging, monitoring, memory management are production-ready while EKS requires building everything ourselves. Second, **operational cost** - Agent Core is fully-managed, reducing our operational burden. Third, **AWS ecosystem integration** - Agent Core integrates more tightly with Bedrock, Lambda, S3, making it more future-proof.

However, security team has concerns about multitenant architecture. They lean toward "physically isolated" EKS approach. Classic technical versus organizational culture conflict.

My strategy: **maintain compatibility with both, wait for culture evolution**. Short term, continue using EKS ensuring production stability. Long term, as AWS invests more in Agent Core (like offering dedicated tenant options) and more enterprises adopt it industry-wide, security concerns may ease. By then, our architecture is ready and migration becomes low-risk.

## Result

### Achievements to Date (As of October 2025)

**Successful MCP Server Deployment**

Project's most important milestone achieved: **Atlassian MCP Server (JIRA + Confluence) successfully deployed to production with OIDC and OAuth integration implemented and templatized**. This server contains dozens of tools covering JIRA issue management and Confluence document access - our first production-ready MCP Server.

From technical perspective, this validates our architecture. Remote deployment runs stably on EKS meeting performance and reliability requirements. OAuth integration with Okta SSO and Atlassian works properly with smooth user experience. Fine-grained permissions (like specific JIRA Board access) function correctly with no unauthorized access incidents. Logging and auditing meet compliance requirements with all tool calls recorded and traceable.

More importantly, Atlassian MCP Server's success provides replicable template for subsequent servers (Google, SQL, HR systems). Other developers can reference Atlassian implementation to quickly build new MCP Servers.

**Development Efficiency Gains**

Framework impact exceeded expectations. MCP Server development and deployment time compressed from one month to one day (simple servers) or 1-2 weeks (complex servers like Atlassian). **Development efficiency improved at least 10x.**

This improvement comes from three sources. First, **eliminating duplicate work**. Developers don't reimplement auth, authorization, logging, error handling - SDK provides all common components. Saves estimated 40-50% of development time. Second, **standardized CI/CD**. Template-generated projects include complete CI/CD pipelines. Developers don't configure GitHub Actions, Docker builds, Kubernetes deployment. Saves 20-30% of time. Third, **lowering learning curve**. New developers don't learn MCP protocol, OAuth flows, EKS deployment from scratch. Just follow Framework docs and examples to quickly onboard. Especially valuable for developers unfamiliar with these technologies.

**Code Reuse and Technical Debt Reduction**

Among 20 existing AI applications with 500+ tools, 200+ relate to database access - massive duplication. While MCP migration just started, we're seeing clear effects.

**SQL MCP Server** deployment will replace all independent database-related tools. Those 200+ duplicate database access tools get replaced by unified SQL MCP Server, significantly reducing codebase complexity. Initial estimates suggest eliminating roughly 10,000 lines of duplicated code.

**Atlassian MCP Server** replaces all independent JIRA and Confluence integrations. While fewer such tools exist in current apps (about 20-30), each implementation is complex (auth, pagination, error handling). Unifying to MCP dramatically reduces maintenance costs.

**Registry Infrastructure Value**

Though Registry currently only has dozens of published MCP Servers, it established standardized discovery and access control mechanisms.

**Discovery mechanism** lets developers easily find available MCP Servers, avoiding duplicate development from "not knowing tools already exist."

**Publish Blacklist + Installation Whitelist two-tier model** proved effective balance - encourages innovation (low publish barrier) while ensuring quality (approval required before use).

**Metadata Management** standardized MCP Server documentation format so different developers' servers all have clear installation guides and config examples, lowering usage barrier.

Registry's value shows in the future. When MCP Server count reaches 50+ or 100+, without Registry we'd have chaos - "can't find it, don't know how to use it, unclear who has permission." We built this infrastructure ahead of time, preparing for future scale.

### Permission Management Effects and Lessons

**Security Assurance**

To date, zero unauthorized access security incidents. OAuth integration with Okta and Atlassian works stably, fine-grained permissions function correctly. Security team acknowledged architecture design.

This achievement came from treating security as first-class concern during design. All credentials stored in Vault, no sensitive info in code or configs. OAuth tokens have clear expiration and refresh mechanisms reducing leak risk. All tool calls have logging and auditing for post-incident tracing.

**Operability Challenges**

We also hit the "overly conservative permission configuration" problem. Developers frequently encountered "insufficient permissions" errors, each requiring approval processes to re-request access, significantly reducing development efficiency.

This issue taught me deeply: **in enterprise environments, technical architecture must consider not just security but also operability**. Overly conservative permission policies, while theoretically more secure, actually cause developer frustration and workarounds (like requesting excessive permissions "just in case"), paradoxically reducing security.

We're pushing improvement - introduce role-based permission presets. For instance, "AI Application Developer" role automatically gets basic permissions for common MCP Servers (SQL, Atlassian), reducing manual application frequency.

This lesson was valuable for personal growth. As architect, can't just focus on technical correctness but also on technical usability and user experience. Best architecture is "pit of success" - making the right way the easiest way.

### Impact on AI Application Development

While 20 existing AI applications' migration just started, we're already seeing MCP change development patterns.

**From "each app integrates data itself" to "using unified MCP Servers"**. Developers don't need to understand JIRA API, Confluence API, SQL database details - just call MCP Server tools. Lowers development barrier, allowing more developers (including frontend devs unfamiliar with backend integration) to build AI apps.

**From "hard-coded business logic" to "AI autonomously choosing tools"**. With MCP, AI applications dynamically discover and use tools rather than hard-coding "if user asks X, call Y API." Makes AI apps more flexible and intelligent.

**From "fragmented permission management" to "unified enterprise IAM"**. All MCP Servers integrate Okta SSO and OAuth. Developers don't separately implement auth for each data source. Simplifies development while improving security and user experience.

TravAI (our internal ChatGPT-style system) started integrating Atlassian MCP Server. Users query JIRA issues and Confluence documents in natural language, AI auto-calls corresponding tools. Still early adoption, but user feedback is very positive - they no longer manually switch to JIRA or Confluence, can complete all work in TravAI.

### Personal Technical Impact and Career Value

**Enterprise Internal Recognition**

This project received Level 4 rating in my performance review (second highest in 5-level system), recognized as successful strategic POC. While I don't personally care much about enterprise internal visibility (my career value comes more from external market), this rating proves project value was recognized by leadership.

Inside the enterprise, when other teams discuss MCP or AI infrastructure, they proactively consult me. I've helped 3-4 other teams evaluate and adopt MCP, expanding my influence within the enterprise.

**Continued Open Source Contributions**

During enterprise MCP development, I accumulated extensive practical experience feeding back to my open source projects. My Atlassian MCP Server added enterprise authentication support, making it suitable not just for personal use but enterprise environments. MCP deployment experience I share on LinkedIn consistently gets 1,000+ views, helping architects at other enterprises. I participated in multiple MCP community discussions with some suggestions adopted by Anthropic officially.

While my open source influence isn't "influencer" level, I built solid reputation in the vertical MCP domain. When enterprises want to deploy MCP, they reference my practices and recommendations.

**Long-term Career Development Value**

This project has three aspects of long-term career value. First, **technical depth**. Deep understanding of MCP protocol, OAuth, enterprise IAM, Kubernetes deployment - all hot topics in current AI infrastructure with high market value. Second, **architecture capability**. My ability to build evolvable architecture amid uncertainty was validated. Not simply "implementing a feature" but "designing a system that continuously adapts to change" - core competitive advantage for senior architects. Third, **end-to-end ownership**. I led this strategic project end-to-end from technology selection, architecture design, code implementation to team collaboration and stakeholder management. This end-to-end experience is key differentiator when interviewing for senior positions (Principal Architect, Engineering Director).

This case study itself is important part of my professional brand. I can reference this project in interviews, conference talks, LinkedIn articles to demonstrate technical depth and business impact.

### Project Future and Sustainability

**Short Term (Next 3-6 Months)**

Complete migration of 20 existing AI applications, eliminate 200+ duplicate database tools. Deploy 5-10 new MCP Servers covering more enterprise systems (HR, Finance, Sales). Optimize permission management, introduce role-based permission presets, reduce application frequency. Complete Agent Core evaluation and potential migration.

**Medium Term (Next 6-12 Months)**

Establish MCP as a Service operational model with dedicated support team helping developers onboard. Introduce more monitoring and analytics understanding MCP usage patterns, optimizing performance. Explore MCP applications in non-AI scenarios like integration layer between microservices.

**Long Term (Next 1-3 Years)**

MCP becomes Travelers' standard data access layer with all AI apps and some traditional apps accessing data through MCP. Registry contains 100+ MCP Servers covering all enterprise core systems.

**About Project Sustainability**

Key question: if I leave Travelers, can this project continue evolving or become legacy code?

My answer is cautiously optimistic.

**Favorable factors**: Framework and Registry code quality high with complete documentation, new developers can quickly onboard. Abstraction layer design ensures architectural flexibility - even if standards change, large-scale refactoring isn't needed. Security team and leadership already recognize MCP's value with intention for continued investment.

**Risk factors**: I'm main architect and core contributor, many key decisions rely on my judgment. Team members don't understand MCP standards as deeply and may lack troubleshooting capability for complex problems. Enterprise environment changes (leadership changes, strategic shifts) may affect MCP investment.

My response strategy: **knowledge transfer and team development**. Writing detailed architecture documentation recording rationale and trade-offs for all key decisions. Developing other team engineers, having them participate in architecture and code reviews improving their capabilities. Hope that even if I leave, project continues evolving on solid foundation.

---
**Final Note**

This MCP Infrastructure project is one of my most fulfilling work experiences at Travelers. Not simply "implementing a function" but building sustainably evolving infrastructure against rapidly evolving technical standards and complex, changing enterprise requirements.

What I'm most proud of isn't lines of code written but architectural decisions proven correct in practice - particularly abstraction layer design, lightweight Registry, two-tier permission model. These decisions came from years of technical accumulation and industry trend judgment, plus deep understanding of enterprise environments.

Project still ongoing with many challenges and opportunities ahead. But at least for now, I can say - we're on the right path and built the right foundation.
  </markdown_content>
</document>
<document>
  <title>2023-02-27 -Build Lab - casualty insurance company</title>
  <markdown_content>
# 2023-02-27 -Build Lab - casualty insurance company

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

A Fortune 500 property and casualty insurance company was receiving over three million medical documents annually that required manual review and processing by agents. Each CMS1500 form (a standard insurance claims document) was processed through an on-premises Kofax system with poor accuracy rates, then manually reviewed and data-entered into application systems. This high-touch manual process created significant operational bottlenecks, slowed claims processing, degraded customer experience, and limited the company's ability to scale document processing for new form types. The company's Enterprise Data, Analytics, and AI/ML teams sought to build an Intelligent Document Processing (IDP) platform to automate document classification and data extraction while reducing manual effort.

### Task

I was assigned as the AWS Data Lab Architect for a follow-up Build Lab engagement focused on production readiness and deployment automation. The initial Build Lab had developed core application logic in a SageMaker Jupyter Notebook, but the solution needed production-grade infrastructure, CI/CD pipelines, and deployment automation to move to enterprise production environments. My responsibility was to architect and implement a fully automated Human-in-the-Loop (HIL) annotation job pipeline as the foundation for generating labeled training data, while building a comprehensive DevOps framework that could serve as a reusable template for the remaining four IDP subsystems. The engagement required navigating significant constraints including working from customer engineers' laptops (Cloud 9 was not approved), configuring access to the company's private Nexus repository instead of public PyPI, and building on MacOS while deploying to Linux Lambda environments.

### Action

**Architecture Design:** I designed a five-subsystem IDP platform where the HIL annotation pipeline would serve as both a functional component and a DevOps template. The architecture leveraged serverless AWS services including Amazon S3 for document storage, AWS Lambda for computational logic, Amazon SageMaker Ground Truth for workforce management, Amazon Augmented AI for human review workflows, Amazon DynamoDB for metadata tracking, and Amazon SNS for asynchronous system integration. I used AWS CloudFormation for infrastructure as code to define IAM roles and policies required by Lambda functions and human review workflows.

**DevOps Framework Implementation:** I built a production-ready DevOps system using Python shell scripts and Make commands to standardize the development workflow across config management, infrastructure deployment, dependency management, Lambda layer building, Lambda app deployment, and testing. I implemented centralized configuration management using AWS Parameter Store with a Python-based schema definition system that separated constant values from derived values and supported reading from local JSON files during development and Parameter Store during runtime. For deterministic dependency management, I used Poetry to resolve the full dependency tree with integrity checksums, storing results in poetry.lock and exporting to requirements files for the company's private Nexus repository. I created a semantic Git branching strategy (feature/, fix/, cf/, hil/, layer/, lambda/, int/, release/, cleanup/) that automatically triggered specific CI/CD workflows based on branch names, allowing conditional execution of deployment steps without requiring complex CI system configuration.

**Lambda Development and Deployment:** I structured Lambda function code using a "low_level_api" pattern that separated business logic from handler code, enabling comprehensive unit testing without deployment. I implemented AWS Chalice as the deployment framework to convert Python functions into Lambda functions with simple decorator syntax, automatically managing event source mappings for S3 triggers. I built automation scripts for publishing versioned Lambda layers that compared hash values against historical versions to avoid unnecessary rebuilds, and for deploying Lambda applications with complete traceability through versioned build artifacts stored in S3 following a structured folder convention.

**HIL Pipeline Implementation:** I developed a conditional HIL trigger system with configurable sampling rates, created custom task UI templates using the Liquid template language with local HTML preview capabilities for rapid iteration, and built HIL output post-processing logic with validators ensuring all human reviewers provided consistent decisions before finalizing labels. I designed the solution to store document type classifications in both S3 and DynamoDB for flexible downstream consumption.

**Knowledge Transfer and Template Creation:** Throughout the four-day Build Lab, I conducted daily hands-on sessions teaching customer engineers to use the automation scripts and follow the software development lifecycle. I used the cookiecutter tool to convert the concrete implementation into a reusable template project that could generate new HIL pipeline codebases by answering a few configuration questions, dramatically reducing the setup effort for the remaining subsystems.

### Results

I successfully delivered a fully functional HIL annotation job pipeline deployed to production that automated the generation of labeled training data for document classification models. The customer received a production-ready codebase including config management, infrastructure as code, multi-environment deployment automation, unit testing frameworks, integration testing patterns, versioned build artifact management, and rollback capabilities. The DevOps framework I developed can be reused across the remaining four IDP subsystems with 95% of the code unchanged, and the semantic versioning system with immutable deployments through Git tags provides enterprise-grade release management and disaster recovery capabilities. The customer's engineering team gained hands-on experience with AWS serverless development best practices and can now independently deploy new HIL pipelines for additional business use cases.
  </markdown_content>
</document>
<document>
  <title>2022-05 to 2022-12 Third-Party Data Entity Resolution</title>
  <markdown_content>
# 2022-05 to 2022-12 Third-Party Data Entity Resolution



## Overview

Streaming platforms and venues were sending us royalty data, but the artist names were a disaster—totally inconsistent formatting. We had no way to match them against our verified rights-holder database. Result? Millions in royalties just sitting there.

I tackled this as a solo Data Engineer without any Data Science backup. Built an entity matching system with AWS Glue ML FindMatch that hit 90% accuracy across 500K tracks. Got $5M in annual royalties flowing to 100K+ musicians. First ML project SoundExchange ever shipped.

## Background

SoundExchange distributes music royalties. Spotify plays a song, we collect the data and pay the artist. Simple concept, massive scale. We're pulling usage data from streaming services, radio, live venues—you name it—across the US, Canada, Europe, and Asia.

I picked up this contract in May 2022. Standard data engineering stuff: ETL pipelines, data lakes, analytics. The company was all-in on AWS, which worked out great since I'd just left AWS as a senior data architect. When they needed someone to prototype new tech, I was the obvious choice.

## Personal Note

First project at the company. Pretty unusual to hand a contractor an exploratory POC right out of the gate, but my AWS background opened doors. The engineering team skewed young, business side more traditional. Good vibe overall.

Going solo without Data Science support was nerve-wracking but exciting. I needed to prove that solid engineering could punch above its weight—that you could tackle hard ML problems with cloud services and good process instead of building everything from scratch. The business team became my secret weapon. Their domain expertise was everything for training data quality.

Biggest takeaway: sometimes engineering matters more than fancy algorithms. Managed services plus rigorous methodology can ship real value fast.

## Situation

We maintain the "Golden Record"—our canonical database of verified rights holders. Proper artist names, emails, addresses, trademark registrations. Everything needed to cut checks. It's our source of truth.

Every day we're ingesting music usage data from third parties. The data includes full metadata—track title, duration, artist info—but the artist fields are garbage. No way to directly match against our Golden Record.

### The Core Problem

Third-party artist data was chaos:

- Same musician using different names across platforms
- Dozens of fields (name, alias, trademark, address) with inconsistent or missing data
- Spelling variations, formatting differences, duplicates everywhere
- Zero standardized identifiers

We had 500K tracks with actual artist names (nothing anonymous), but couldn't figure out which messy third-party name matched which clean Golden Record identity. Can't match? Can't pay.

### Business Impact

- **Money stuck**: $5-6M in annual royalties going nowhere
- **People affected**: 100K+ musicians not seeing their money
- **Trust issues**: Hard to be the "authoritative" royalty distributor when you can't actually distribute royalties

### Technical Challenge

Textbook fuzzy matching problem. Need to spot the same entity across wildly different representations in high-dimensional space. Rules don't scale to all the variations. Custom ML would need a dedicated Data Science team and months of work.

## Task

### Project Goals

Build and validate an entity matching system to unblock payments. What success looked like:

- **Coverage**: Match 80%+ of the $5M backlog
- **Accuracy**: Keep false positives under 10%—musicians weren't getting paid anyway, so some error was tolerable
- **Deliverable**: POC to prove feasibility, not production system

### My Role

I owned the full technical implementation:

- Evaluate solutions
- Design training data strategy
- Coordinate business team for labeling
- Train and validate models
- Deliver findings and recommendations

Worked with a PM for coordination and business experts for labeling. All engineering was me.

### Constraints

- **Team**: One Data Engineer (me), no Data Scientists
- **Time**: May-December 2022, ~7-8 months
- **Knowledge**: I couldn't judge artist matches—needed business team expertise

## Action

### Solution Evaluation (Month 1)

Looked at three options:

1. **ElasticSearch with Rules**

Fuzzy queries, n-grams, the usual text matching tricks. Spent a month testing. Didn't work:

- Rule complexity exploded—every field combo needed custom config
- Edge case handling was flaky, unpredictable results
- Zero ability to improve over time

2. **Build Custom ML**

Theoretically possible. Practically a non-starter:

- No Data Scientists available
- Timeline didn't work
- Too expensive for exploratory work

3. **AWS Glue ML FindMatch**

AWS had just launched this in early 2022 for entity matching. Went with it because:

- I'd used it at AWS and knew what it could do
- Managed service—no ML infrastructure to build
- Handles multi-field matching out of the box
- Pay-per-use made sense for a POC

Kickoff meeting: ruled out options 1 and 2, committed to FindMatch.

### Training Data (3 Weeks)

FindMatch needs supervised training data. This was make-or-break. The problems:

- Can't manually label 500K records
- Need coverage across all field combinations
- I don't know enough about the music industry to judge matches

#### Sampling Strategy

Wrote Python scripts for smart sampling:

- Made sure every field (name, alias, trademark, address, etc.) appeared in both filled and empty states
- 10% sample rate per dimension kept feature distributions realistic
- Used Polars for multi-dimensional sampling to cover all the field combinations

Generated 5-10K candidate pairs depending on what the sampling caught.

#### Labeling Workflow

Built a system since I couldn't do the labeling myself:

- Python scripts generated Excel templates: record A left side, record B right side, empty column in the middle
- Binary decision: match or no match
- Got 3 business experts to help part-time
- ~1K labels per person per day, finished 10K+ records in 3 weeks

#### Quality Control

FindMatch needs 3-5 examples per entity minimum or it won't train. My sampling naturally hit this:

- Field combination coverage gave plenty of positive and negative examples
- 10K+ labels crushed the minimum requirements
- Cross-checked between labelers for consistency

### Model Training (3-4 Months)

Standard ML iteration loop:

#### Data Split

- 70/30 train/val split
- 10-fold cross-validation to catch overfitting
- Logged every experiment's config and metrics

#### Tuning

Ran FindMatch jobs in AWS Glue, tweaked:

- Field weights (is trademark ID more important than address?)
- Match thresholds (precision vs recall tradeoff)
- Preprocessing (case normalization, special character handling)

Each run: check predictions against labels, calculate accuracy/recall/F1, study the confusion matrix, adjust, rerun. Kept going until metrics plateaued.

#### Performance

- Trained model processed 500K records in ~1 hour
- AWS Glue charges by compute time, costs were reasonable (can't share specifics)

### Validation (Final Month)

Hit 90% accuracy on validation set, beat the 80% target. Delivered complete POC report:

- Feasibility confirmation
- Cost analysis
- Production deployment roadmap
- Risk assessment and improvements

POC ended there. Another team handled production deployment.

## Result

### Technical Wins

- **Accuracy**: 90% matching, crushed the 80% goal
- **Scale**: Mapped 500K tracks to rights holders
- **Speed**: 1-hour processing vs 40K+ hours manual (5 min/record)—40,000x faster
- **Innovation**: Solved hard ML problem with engineering and cloud services, no Data Science team needed

### Business Impact

- **Payments unlocked**: $5-6M annual royalties ready to distribute
- **Artists paid**: 100K+ musicians can finally get their money
- **Risk managed**: <10% false positive rate, within tolerance
- **Cost savings**: Managed services beat building a team—total spend was just my 7-month salary

### Company Impact

- **First ML project**: SoundExchange's first AWS ML deployment, opened doors for future AI work
- **Methodology proven**: Showed you can validate complex tech fast with limited resources and cloud services
- **Data asset**: 10K+ labeled records became reusable training data for future improvements

### Personal Improvement

Did this solo in 7-8 months with no Data Science support. Shipped it by knowing AWS and keeping process tight. Showed:

- Fast learning on new tech (FindMatch was brand new in 2022)
- Cross-functional collaboration (got business team engaged in labeling)
- Creative problem-solving (sampling strategy, automation tooling)
- Execution under constraints

By December 2022, I'd proven the concept to leadership, saved millions, and helped tens of thousands of musicians get paid.
  </markdown_content>
</document>
<document>
  <title>2022-01 to 2022-09 - Mutual Fund Price ETL Pipeline</title>
  <markdown_content>
# 2022-01 to 2022-09 - Mutual Fund Price ETL Pipeline

> As a Data Architect / Engineer Consultant at Accenture Federal Service, I led the design and implementation of critical data infrastructure for the Federal Retirement Thrift Investment Board's pension asset management system, which serves all federal government employees. My work spanned the entire data engineering lifecycle, encompassing **data architecture**, **ETL pipeline development**, **workflow orchestration**, **data analytics**, and **PySpark job optimization**.

## Overview

Built a serverless ETL pipeline for the Federal Retirement Thrift Investment Board to automate their daily mutual fund pricing data. The vendor was sending us files in some weird proprietary format - no standard delimiters, totally unparseable with existing tools. I wrote a custom parser from scratch and set up an event-driven architecture that processes everything automatically. We brought over 10 years of historical data and turned it all into clean dashboard tables. After launch in May 2022 and ironing out some kinks through September, it's been running flawlessly ever since.

## Background

The Federal Retirement Thrift Investment Board (FRTIB) manages retirement investments for federal employees. Their system offers several mutual funds that employees can mix and match in their portfolios. When employees are deciding how to allocate their money, they need to look at historical fund prices.

Every day, a third-party vendor drops off updated fund prices in TXT files using some industry-specific format - honestly a nightmare to parse since there are no standard delimiters. Before this project, employees had to manually download these raw files to check the data. Not exactly user-friendly - most people couldn't make sense of the format. The business wanted an automated pipeline and a proper dashboard to fix this mess.

## Situation

Here's what we were up against:

- **Proprietary data format**: The vendor's TXT files used some industry-specific format with no standard delimiters - couldn't find any existing parser that would work
- **Deduplication headaches**: Daily files sometimes contained historical records mixed in, so we needed rock-solid append-only behavior without accidentally creating duplicates
- **Historical backfill**: We had to migrate 10 years of historical data before launch
- **Data reshaping**: Raw data came in flat, but the dashboard needed pivot tables
- **Terrible UX**: Users were stuck manually downloading raw files that were basically impossible to read

## Task

Build a complete ETL pipeline and visualization layer that would:

- Parse the vendor's proprietary file format
- Handle daily data ingestion with proper deduplication
- Reshape flat data into pivot tables for the frontend
- Guarantee data consistency (append-only, zero duplicates)
- Actually make it easy for users to check fund prices

The big challenges were wrangling that messy data format, backfilling 10 years of history, and making sure the whole thing would be stable enough to run itself without constant babysitting.

## Action

### Custom Parser Development

Since the vendor format had no delimiters, I built a parser from scratch in Python. Used pattern matching with validators to reliably pull out each fund's ID and price data. Spent a lot of time making sure it could handle all the weird edge cases. This parser ended up being the linchpin of the whole system.

### Event-Driven Data Ingestion

Wired up EventBridge to monitor an S3 bucket - whenever the vendor file drops, it kicks off a Lambda function automatically. Lambda grabs the file, runs it through the custom parser, and starts the ingestion process. Pretty straightforward event-driven setup that just works without any manual intervention.

### Partition-Based Deduplication

For deduplication, I went with an S3 partitioning strategy. Data gets organized into date-based folders (date=YYYYMMDD format), and each folder can hold multiple TXT files.

When new data comes in, the Lambda checks the timestamps and figures out which partition it belongs to. Then it runs deduplication checks within that partition to avoid writing the same records twice. This approach leverages S3's filesystem structure, which keeps the logic clean and performant.

### Glue Catalog and Data Modeling

AWS Glue Catalog automatically catalogs the partitioned S3 data. The raw data is denormalized - each row is just fund ID, date, and price. Glue Catalog handles all the metadata management and schema definitions.

### Pivot Transformation with Athena

A second Lambda runs Athena SQL queries to pivot the flat tables. It reads the partitioned data from S3, does the aggregation and pivoting, then writes the results to a separate S3 bucket for the frontend.

The frontend team just pulls those pivoted files and displays them straight up on the dashboard - no complex transformation logic on their end.

### Step Function Orchestration

Tied everything together with AWS Step Functions:

1. **First step**: Daily Lambda reads vendor files, parses them, writes to partitioned S3
2. **Second step**: Four hours later (usually around midnight), another Lambda calls Athena to pivot the data and generate dashboard files

That four-hour buffer makes sure all the data is fully written and available. Users see fresh data on the dashboard every morning.

### Historical Data Migration

Before going live, we needed to backfill 10 years of history. Did a one-time manual load - partitioned everything by date and dropped it directly into the S3 bucket. Kept it simple rather than building elaborate migration tooling.

### Iterative Refinement

Launched in May 2022, then spent May through September fixing a few things: decimal formatting (started forcing four decimal places with zero-padding), timezone handling (standardized everything to Central Time), and some schedule tweaks. By September, everything was dialed in perfectly.

## Result

Launched in May 2022, spent four months polishing it, and it's been rock solid since September - zero incidents. Nailed the proprietary format parsing, used partitioning to guarantee data consistency, and got daily automation working smoothly.

Completely transformed the user experience: employees went from manually downloading cryptic files they couldn't understand to viewing clean historical fund data on a proper dashboard. The custom parser and partition-based deduplication were the key technical wins that made the whole thing reliable and easy to maintain.

## Personal Note

Started this project in January 2022, launched in May. I owned it end-to-end as a Data Engineer at Accenture - designed the architecture, built everything, deployed it, and kept it running (DevOps handled prod deployments).

The toughest part was definitely parsing that bizarre TXT format - literally no standard delimiters anywhere, and I couldn't find any existing parser in the industry. Built one from scratch in Python using pattern matching and validators to handle all the weird corner cases. That parser became the secret sauce - everyone else on the team was stuck on that format.

Between May and September, we fixed a few minor things: decimal formatting (initially wasn't padding zeros, switched to always showing four decimal places), timezone conversion (made sure everything used Central Time), and schedule adjustments. After September, totally stable - no bugs at all.

Not a huge project scale-wise (daily data was only a few hundred KB to a few MB), but the technical challenge was all about that gnarly data format and nailing the append-only semantics.
  </markdown_content>
</document>
<document>
  <title>2023-01-23 - Build Lab - healthcare technology company</title>
  <markdown_content>
# 2023-01-23 - Build Lab - healthcare technology company

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

A healthcare technology company specializing in AI-powered radiology workflow automation sought to improve its machine learning infrastructure for predicting follow-up requirements in radiology reports. The company had developed an initial binary classification model that predicted whether the remaining 80% of radiology reports (without explicit doctor-noted follow-ups) required follow-up actions. While their manual quality control team achieved 99% accuracy reviewing predictions, this process was not scalable. The company needed to establish a standardized ML workflow to accelerate model iteration and accuracy improvements while maintaining their quality standards. Additionally, the customer's engineering team was working in a constrained sandbox AWS environment with limited experience in AWS machine learning services, requiring hands-on knowledge transfer alongside technical implementation.

## Task

I was assigned as the Data Lab Architect to design and build a production-ready machine learning consumption layer and establish an ML pipeline framework during this four-day Build Lab engagement. My responsibilities included architecting a real-time ML inference system triggered by S3 events that would store predictions in both Amazon DynamoDB (for mobile app access) and S3 Data Lake (for analytics), creating a proof-of-concept Amazon SageMaker Pipeline to automate training, evaluation, and model registration, and teaching the customer's engineering team to replicate these solutions in their production AWS environment using their actual data. I also needed to deliver working code while simultaneously conducting knowledge transfer sessions, navigating sandbox account limitations, and addressing several stretch goals including data preparation pipelines, deployment strategies, and human-in-the-loop workflows.

## Action

I designed and implemented a five-use-case ML infrastructure spanning both consumption and training workflows.

**ML Consumption Architecture:** I built an event-driven inference system using AWS Lambda as an ML proxy layer that decoupled upstream/downstream dependencies, integrated with Amazon Comprehend's real-time endpoint for binary classification predictions, and wrote results to both DynamoDB (with PynamoDB ORM for simplified CRUD operations) and a partitioned S3 Data Lake with AWS Glue catalog and Amazon Athena for analytics queries.

**ML Training Pipeline:** I developed custom Python abstraction modules that simplified Amazon Comprehend's boto3 API for training and testing multi-class classifiers, handling data formatting (8,000 training documents split into 1,000-document CSV batches, 2,000 testing documents as individual files for file-to-label mapping), and automated model evaluation workflows. I then architected a proof-of-concept SageMaker Pipeline by refactoring experimental Jupyter notebook code into modular Python functions covering pipeline parameters, training jobs, model evaluation, and conditional model registration with "PendingApproval" gates for quality control.

**Data Orchestration:** I created an AWS Step Functions workflow demonstrating parallel data loading from multiple sources, merge operations, and integration with SageMaker Pipeline using a "Job Poller" pattern for asynchronous execution monitoring.

**Novel Contributions:** I invented reusable Python modules that abstracted Comprehend's training and testing APIs, making them adaptable for various data sources and scenarios. I also identified an unanticipated use case during the lab—replacing their homemade quality control system with Amazon Augmented AI (A2I)—and rapidly built a complete human-in-the-loop workflow using Amazon SageMaker GroundTruth private workforce, Lambda-triggered human loop tasks, and DynamoDB for task state management.

**Knowledge Transfer:** Throughout the engagement, I conducted pre-lab training on AWS Lambda best practices (development, testing, deployment, and load testing strategies), taught the team to work with SageMaker Studio, demonstrated blue/green deployment strategies for model versioning and rollback capabilities, and created architecture diagrams with the team for their executive presentations. I coordinated post-lab SME sessions for topics requiring deeper expertise (SageMaker VPC-only mode setup, Comprehend cost optimization).

## Results

I successfully delivered all defined objectives plus all stretch goals within the four-day timeframe, including a working ML consumption layer, automated SageMaker pipeline, data preparation orchestration, deployment strategy documentation, and an unexpected A2I human-review workflow. The customer achieved a working proof-of-concept that demonstrated potential to reduce manual review workload by 80% while maintaining their 99% accuracy standard. I transferred comprehensive knowledge to the customer's engineering team through hands-on collaboration, enabling them to independently rebuild the solution in their production environment within 2-4 weeks post-lab.
  </markdown_content>
</document>
<document>
  <title>2023-03-21 - Design Lab - beverage alcohol distribution company</title>
  <markdown_content>
# 2023-03-21 - Design Lab - beverage alcohol distribution company

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

A leading national beverage alcohol distribution company, representing one of America's largest private enterprises with operations across 47 U.S. markets, faced a critical infrastructure challenge in their machine learning operations. The company had established Amazon Redshift as their centralized data platform and was operating four production ML models on Google Cloud Platform virtual machines with six additional models in staging. They used a single AWS account for all data storage and applications, creating scalability and governance limitations. The company needed to migrate from GCP JupyterLabs to AWS SageMaker while establishing a robust multi-account ML platform architecture that would support continuous development and deployment of machine learning models. The company partnered with Quantiphi, a consulting firm, to assist with infrastructure setup and AWS solution implementation.

## Task

I was assigned as the Data Lab Architect to design a comprehensive multi-account ML Ops platform architecture for this two-day Design Lab engagement. My responsibility was to create a detailed architectural blueprint that would enable the company to migrate their existing ML models from GCP to AWS while establishing enterprise-grade governance, security, and scalability. The design needed to account for the company's existing single-account AWS infrastructure and provide a practical implementation roadmap. Additionally, I needed to address a stretch goal of defining the ML development and deployment software development life cycle (SDLC) from both data scientist and ML engineer perspectives, ensuring the proposed architecture would support efficient workflows for their teams.

The challenge required balancing technical complexity with practical implementation—the architecture needed to be comprehensive enough to serve as a long-term foundation while being decomposed into manageable, prioritized components that the company and Quantiphi could build incrementally.

## Action

I designed a comprehensive enterprise ML Ops platform architecture using AWS best practices for multi-account setup, which I systematically broke down into component designs on day one before integrating them into the final architecture on day two.

**AWS Organization Structure:** I recommended a hybrid function-oriented and workload-oriented organizational structure with dedicated Infrastructure and Networking organizational units, plus separate Dev, Staging, and Prod OUs containing ML-specific accounts (ml-dev, ml-staging, ml-prod). I established the Infrastructure account as the organization's delegated administrator for deploying infrastructure-as-code via AWS CloudFormation StackSets.

**Cross-Account Architecture:** I designed cross-account IAM permission patterns using AssumeRole for the customer's preferred orchestration tool (Amazon MWAA) to deploy resources across accounts, and architected cross-account data sharing strategies leveraging Amazon S3 bucket policies, AWS Glue Catalog without Lake Formation, and Amazon Redshift Data Sharing to enable secure data access across the ml-dev, ml-staging, and ml-prod environments.

**ML Infrastructure Design:** I evaluated and recommended all four Amazon SageMaker inference options (Serverless, Real-time, Async, and Batch Transform) with specific use-case guidance, designed an API consumption layer using AWS API Gateway and AWS Lambda as an ML proxy to enable integration with their existing Apigee infrastructure, and incorporated SageMaker's built-in blue/green deployment capabilities with traffic shifting modes (all-at-once, canary, linear) and auto-rollback monitoring.

**Development Workflow:** I architected the complete ml-dev account setup including Amazon SageMaker Domain and Studio for data scientist development, SageMaker Pipelines for automated training/evaluation/registry, SageMaker Model Registry for versioned model metadata, and AWS CodeCommit/CodeBuild/CodePipeline for CI/CD automation.

**DevOps Standards:** I established semantic branching strategies mapping git branches to environments and CI actions (Feature→dev→unit tests, Test→staging→integration tests, Release→prod→deployment), recommended unique container image tagging using SageMaker model registry version numbers, and defined comprehensive AWS resource tagging strategies across technical, automation, and business dimensions.

**Network Architecture:** I designed a single-region, multi-account network configuration using a centralized AWS Transit Gateway in a dedicated Networking account to connect all VPCs and on-premises networks, with VPC peering as a fallback for existing infrastructure incompatibility.

**Knowledge Transfer:** Throughout the engagement, I documented all architectural decisions with detailed rationale, AWS service references, and implementation guidance, and collaborated with the customer's management team and Quantiphi's technical team to ensure understanding of each component and its dependencies. I concluded by creating a prioritized, dependency-ordered implementation roadmap with 22 specific action items grouped into five major phases.

## Results

I successfully delivered a production-ready multi-account ML Ops platform architecture that addressed all the customer's requirements for migrating from GCP to AWS SageMaker. The design enabled the company to scale from a single-account setup to an enterprise-grade, multi-account architecture supporting continuous ML model development and deployment across dev, staging, and production environments. I provided the customer and Quantiphi with a clear, prioritized implementation plan that accounted for their existing infrastructure and enabled incremental deployment, positioning them to begin building the platform immediately following the lab with confidence in the architectural foundation.
  </markdown_content>
</document>
<document>
  <title>2022-12-12 - Build Lab - digital marketing and automation platform company</title>
  <markdown_content>
# 2022-12-12 - Build Lab - digital marketing and automation platform company

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

I was assigned to a leading digital marketing and marketing automation platform provider serving small businesses and nonprofits. The company had successfully implemented data pipelines in production but faced challenges scaling their machine learning operations. Their ML pipeline was in early stages, and they needed to onboard a growing number of ML use cases while building different models for multiple customer accounts. With constantly changing datasets requiring weekly model updates, their current ML development workflow couldn't support rapid production deployment across their expanding portfolio of ML use cases, including customer churn prediction.

## Task

My responsibility was to design and build a production-ready ML pipeline that would automate model training, evaluation, and registration on a weekly schedule without manual intervention. Working on-site with the customer's team of four engineers (including the Director of Data ML, a Principal Data Scientist, a Senior Software Engineer, and a Data Scientist), I needed to deliver a working proof-of-concept using their production data while simultaneously teaching the team to replicate and maintain the solution. The deliverables included a functional Amazon SageMaker pipeline for customer churn prediction using XGBoost, plus stretch goals for hybrid orchestration workflows and a scalable MLOps codebase structure.

## Action

I designed and implemented an end-to-end ML automation solution using Amazon SageMaker Pipelines as the core orchestration engine.

**Pipeline Architecture:** I built a five-step SageMaker pipeline that automated data retrieval from Amazon S3, executed hyperparameter tuning jobs to optimize model performance, ran evaluation jobs to calculate accuracy metrics, and conditionally registered models in the SageMaker Model Registry only when accuracy thresholds were met. To resolve a critical technical challenge where the pipeline couldn't retrieve the best model from hyperparameter tuning, I implemented Pipeline Session instead of SageMaker Session to properly store context information.

**Code Refactoring and Knowledge Transfer:** I refactored the initial Jupyter Notebook experimentation code into modularized Python functions, splitting the pipeline definition into discrete components (parameters, training step, evaluation step, model registration, and assembly logic) to enhance maintainability and team understanding.

**Hybrid Orchestration:** To address the customer's requirement for integrating SageMaker pipelines with their existing AWS Step Functions workflows, I designed a hybrid orchestration architecture using the "Job Poller" pattern. This State Machine implementation periodically checked SageMaker pipeline execution status, failed gracefully when pipelines failed, and triggered AWS Lambda functions for model deployment upon success.

**Production-Ready MLOps Framework:** I established a scalable Git repository structure with a modularized Python library (`ctct_rni`) that organized ML models in separate subdirectories, each inheriting common logic from an abstract Pipeline base class. Using AWS Cloud9 as a collaborative development environment, I created deployment shell scripts that could simultaneously deploy multiple SageMaker pipelines and Step Functions workflows for CI/CD integration.

**Model Validation and Testing:** I trained the initial customer churn model using three months of production data (5 million records), achieving 90% accuracy, 89% precision, and 90% recall rates. To demonstrate automated retraining capabilities, I executed the pipeline twice with different time windows (two months vs. three months of data) without code modifications, validating the weekly automated retraining requirement.

## Results

I successfully delivered a production-ready ML solution that exceeded all objectives and stretch goals. The customer implemented the solution as their internal ML project standard and immediately moved it to production, using it to accelerate deployment of multiple ML use cases across different customer accounts with weekly automated model retraining.
  </markdown_content>
</document>
<document>
  <title>2022-01-24 to 2023-01-27 at Accenture Federal Service</title>
  <markdown_content>
# 2022-01-24 to 2023-01-27 at Accenture Federal Service

Key technical areas included:

- **Cross-Cloud Data Integration**: Designed solutions to bridge isolated cloud partitions (AWS GovCloud and Commercial Cloud), enabling seamless data flow across security boundaries
- **Data Pipeline Architecture**: Built end-to-end serverless data pipelines using AWS services (Lambda, Step Functions, Glue, DynamoDB, S3, Redshift) for data ingestion, transformation, and storage
- **Custom Parser Development**: Created robust parsers for non-standard industry data formats, enabling automated processing of third-party data sources
- **Log Auditing Infrastructure**: Developed high-performance crawlers to collect and process millions of user activity logs for compliance auditing and fraud detection
- **PySpark ETL Optimization**: Refactored complex SQL-based transformations into modular, maintainable PySpark jobs, dramatically improving code quality and iteration speed
- **Data Orchestration**: Implemented event-driven architectures and state management systems to ensure data consistency and reliability across distributed systems

Throughout this engagement, I owned projects from architecture design through production deployment, consistently delivering cost-effective solutions that replaced expensive third-party tools while maintaining enterprise-grade reliability and performance.
  </markdown_content>
</document>
<document>
  <title>2024-08 to 2024-12 - AI Transformation Consulting Framework</title>
  <markdown_content>
# 2024-08 to 2024-12 - AI Transformation Consulting Framework



## Overview

As an Enterprise Architect at Arch Systems, I built the AI Catalyst framework—the industry's first systematic approach to AI transformation consulting. The framework breaks down enterprise AI adoption into 7 pathways that take organizations from ground zero to full AI maturity. The impact was immediate: our sales cycle collapsed from 2-3 months to 3-4 weeks, we cut discovery meetings from 5-6 down to 3, and in early 2025 we closed a multi-million dollar, 5-year contract with the Administration for Children and Families (ACF).

## Background

Arch Systems is a women-owned IT consulting firm specializing in federal government contracts. By Q3 2024, generative AI was exploding, and federal agencies along with major enterprises were scrambling for AI transformation help. The catch? Nobody—not even McKinsey—had figured out a standard playbook for AI transformation.

The gap made sense when you looked at it. AI moves so fast that traditional consulting firms can't build methodologies quickly enough. Most business consultants have never built AI systems themselves, so they struggle to connect business needs with what's technically possible. And AI transformation isn't just a tech problem—it's security, compliance, organizational change, the whole nine yards. You need a real framework to navigate all that.

Arch Systems knew cloud modernization inside and out, but AI transformation was new territory. Our business development team could close deals, but they needed architectural backing for the technical conversations. That's where I came in—to build something that worked for both the tech folks and the business side.

## Situation

In August 2024, Arch Systems had a problem. AI consulting demand was through the roof, but we kept losing deals at the finish line.

### Client Needs Were Vague and Contradictory

Every prospect walked in talking about "transforming with AI," but nobody could tell us what they actually wanted to build. Worse, they'd agree that AI transformation takes time at the strategic level, then turn around and demand immediate ROI when we got to project planning. These contradictions killed deals left and right.

### Traditional Consulting Processes Were Inefficient and Misaligned

We were running the old IT consulting playbook—send out massive questionnaires covering everything from server specs to org charts. Most of it had nothing to do with AI. Clients got exhausted, and we couldn't zero in on what actually mattered.

A typical engagement looked like this: 5-6 weeks of weekly meetings covering infrastructure, data, team skills, budgets. We'd talk about everything except the actual project. By the end, we had no clear scope, no timeline, and no way to show value. Prospects ghosted us out of pure exhaustion.

### Lack of Competitive Differentiation

When prospects shopped around, we had no clear differentiator. Everyone showed the same slide decks about technical capabilities and past projects. We needed something that screamed "this is why you pick us."

### Technical Solutions Were Hard to Visualize

Our proposals dove straight into architecture diagrams and implementation details. That's great if you're talking to engineers, but useless when you're presenting to a federal program manager or a VP of operations. They couldn't picture what we'd actually deliver or how it would change their day-to-day work.

Here's what it came down to: AI transformation isn't one problem—it's infrastructure, data governance, team readiness, compliance, all tangled together. The industry didn't have a framework that could both guide the technical work and help sell the project.

## Task

My job as Enterprise Architect was to fix this. I needed to build a framework that worked for both technical teams and business stakeholders.

### Core Objectives

Build an AI transformation methodology that would let Arch Systems:

- Get clients from "we want AI" to signed contract in 3 meetings
- Cut sales cycles from 2-3 months to 3 weeks
- Win more deals by showing clear value and concrete deliverables
- Create something competitors couldn't easily copy

### Design Constraints

**Time**: I had until end of Q4 2024—about 3-4 months—to design and validate everything before the 2025 sales push.

**Resources**: This was my side project on top of my day job at Travelers. No dedicated team, just me iterating with the business development managers.

**Compatibility**: The framework had to plug into our existing federal contracting workflow—RFP responses, technical evaluations, contract negotiations, all of it.

**Security**: We work with federal agencies, so FedRAMP and FISMA compliance wasn't optional. Security had to be baked in, not bolted on.

### Key Challenges

**Technical depth vs. business clarity**: The framework had to work for both the engineers implementing it and the executives approving it. That's a tough balance.

**Standing out**: We needed something unique that prospects would actually believe in—not just another cloud migration framework with AI slapped on top.

**Flexibility**: Companies range from "we still use fax machines" to "we're already running GPT-4." The framework couldn't be a rigid checklist—it had to let clients start wherever they were.

### My Responsibilities

I owned the whole thing—concept, methodology, technical content, supporting materials (website, slide decks, demos). My partner handled business development and client relationships. He'd use the framework in sales calls and feed me real-world feedback.

This meant I lived in both worlds. I'd dig through old meeting recordings to see where we lost deals, design materials for different audiences, build demos that would resonate with prospects, then iterate based on what actually worked in the field. Way beyond the typical architect job description.

## Action

### Methodology Design: From Cloud Migration 6R to AI Transformation 7P

Building a methodology from scratch is intimidating. I needed a proven model to start from. AWS's Cloud Migration 6R framework (Rehost, Replatform, Repurchase, Refactor, Retire, Retain) caught my attention. The genius of 6R is that you don't migrate everything at once—you pick the right strategy for each application. That composability was exactly what AI transformation needed.

But I couldn't just copy-paste it. Cloud migration is about moving existing systems. AI transformation is about both modernizing what you have and building new capabilities from scratch. So my pathways needed to be more than just strategy options—they needed to represent stages of capability building.

I looked at Arch Systems' past projects and my own enterprise AI work to map out what companies actually struggle with. A few patterns emerged.

Data readiness was the big one. Is your documentation digitized? Can AI systems even access it? This led to Document Restructure and AI Knowledge Base Infrastructure pathways.

Infrastructure maturity came next. Do you have somewhere to run AI applications? This became AI Client & Server Foundation and Corporate Security Integration pathways.

Then process automation. What repetitive work could AI handle? That's the SOP-to-AI Workflow Transformation pathway.

Employee skills were another gap. Can your people actually work with AI tools? Enter AI Workplace Mastery.

Finally, autonomous systems. Are you ready for AI to make decisions on its own? That's the most advanced pathway—Enterprise AI Agent Development.

My first draft started with AI Agents because that's the sexy part. But talking through real client scenarios with our business manager, I realized most companies aren't anywhere close to ready for that. If your documents are still Word files scattered across SharePoint, if your security team hasn't approved AI touching company data, talking about autonomous agents is pointless.

That insight forced a redesign. I restructured everything as a progression from foundation to advanced, where each pathway solves a specific readiness problem.

### Framework Structure Design: The Four-Dimensional What-Why-Who-How Model

Once I had 7 pathways, I needed a way to present them that would click for different audiences. I studied successful frameworks like Gartner's Magic Quadrant and Forrester Wave. The best ones answer four questions:

**What**: What exactly are we building? What do you get at the end?

**Why**: What problem does this solve? What's the business value?

**Who**: Which companies should do this first? What do they need to have in place?

**How**: What does implementation actually look like? Timeline, milestones, the works.

I built this structure into every pathway. The language had to work at different levels—technical terms in the What section for engineers, pure business language in Why for executives, specific criteria in Who to help clients self-identify, and clear roadmaps in How so nobody's surprised by what they're signing up for.

### Iterative Optimization: From Technical Document to Business Tool

Version one was way too architect-y. Full of jargon and architecture diagrams. Our business manager took it to a client meeting and reported back: "They said it looks professional, but they have no idea what value we're delivering."

**First iteration: Adding business value narrative**

I rewrote every Why section. Stopped talking about what the technology does and started explaining what problems it solves and what money it saves. Document Restructure used to say "convert documents to AI-friendly formats." Boring. I changed it to "Transform your enterprise documentation into AI-ready formats while maintaining human readability. The fastest path to enabling AI with your organization's knowledge." Now clients got it—this isn't busywork, it's the fast track to AI adoption.

**Second iteration: Introducing cases and visualization**

Words weren't enough. Clients needed to see what "done" looks like. I built demo artifacts for each pathway. AI Workplace Mastery got a simple chatbot demo. SOP-to-AI Workflow got an interactive workflow showcase on GitHub Pages. AI Knowledge Base got a knowledge graph visualization.

These demos didn't need to actually work—they just needed to look real. I went lightweight: Streamlit for quick UIs, GitHub Pages for hosting, Lucide icons for consistent visuals. Everything stayed high-level on purpose. That protected our technical IP and kept conversations from spiraling into implementation details too early.

**Third iteration: Standardizing material packages**

As the framework solidified, I realized we needed a kit for different sales scenarios:

- **Website pages**: One page per pathway for prospects to research on their own (navy blue header strips, light gray cards for What sections, green checkmarks for Why bullets, light blue cards for Who sections)
- **Master PPT**: Overview of all 7 pathways and how they connect, for initial consultations
- **Pathway-specific PPTs**: Deep dives with architecture, roadmaps, and case studies
- **Demo gallery**: All demos in one place for easy screen sharing

**Fourth iteration: Process integration**

The final iteration focused on truly integrating the framework into Arch Systems' business processes. Working with the business manager, we mapped out every step from lead generation to contract signing, defining which materials to use at each stage:

- **Opportunity identification stage**: Use 7P overview to attract interest
- **Initial consultation stage**: Guide clients to self-assess through questioning, matching to 1-2 most suitable Pathways
- **Proposal drafting stage**: Use Pathway-specific content to build technical solutions and timelines
- **Technical evaluation stage**: Prepare demos and architecture diagrams, keeping them high-level but convincing enough
- **Contract negotiation stage**: Use roadmaps and milestones to clearly define deliverables

During this integration, I heavily referenced Arch Systems' past meeting records. I'd find opportunities that ultimately fell through and analyze where things went wrong. For example, one prospect said after the fourth meeting "This all sounds great, but we still don't know where to start"—exposing our lack of tools to help clients "decide." So I added simple "Pathway Selector" logic: if the client's documents aren't digitized yet → Document Restructure; if they have data but AI can't access it → Knowledge Base Infrastructure; if they want to improve employee AI skills → Workplace Mastery, and so on.

### Technical Implementation and Tool Development

Though this was a "consulting framework," my work as an architect went beyond writing documents—it included developing a series of tools and technical assets.

**Confluence knowledge base**: All Pathway content lives in Confluence, using structured XML format for easy version control and team collaboration, with scripts auto-generating website content. I designed markdown conventions ensuring content works for both human reading and programmatic processing.

**Demo application development**: To help clients quickly understand what AI applications ultimately look like, I developed two types of demos:

- **Human-facing AI clients**: Built web interfaces with React + Tailwind CSS, showing how AI assistants interact with users
- **Machine-facing AI clients**: Created API examples with Python + FastAPI, showing how AI automates task execution

These demos were deliberately "almost real but not fully functional"—they look like production systems but are actually carefully designed prototypes. This approach protected technical details while giving clients concrete expectations.

**Architecture diagram standardization**: I created a set of standardized architecture diagram templates where all Pathway technical architectures follow unified visual style and abstraction levels. These diagrams are intentionally high-level, showing only major components and data flows without exposing specific technology choices and implementation details.

**Master PPT template**: Designed PPT templates matching Arch Systems brand guidelines, using company theme colors (#596FE1 blue, #31FE9F green), integrating Lucide icon library vector graphics, ensuring visual consistency across all materials.

### Field Validation and Continuous Improvement

In December 2024, with the framework mostly complete, we started using it in actual client communications. The first few cases provided valuable feedback.

A typical scenario was the initial meeting with the Administration for Children and Families (ACF). Previously, we'd spend tons of time introducing Arch Systems' history, team capabilities, past projects, then ask clients "what do you want." Now we switched to:

1. Brief Arch Systems introduction (5 minutes)
2. Show 7 Pathways overview (10 minutes)
3. Ask questions guiding client self-identification (15 minutes): "What are your current AI application scenarios? What obstacles are you hitting?"
4. Deep dive into 1-2 most relevant Pathways (20 minutes)
5. Show demos and cases (10 minutes)

This structured communication made meetings far more efficient. ACF's feedback: "This is the clearest AI transformation methodology we've seen." They particularly valued Corporate Security Integration as a Pathway, because federal agencies have extremely high security compliance requirements, and most AI vendors treat security as an "afterthought" while we made it an independent, required transformation stage.

Ultimately, ACF chose two Pathways: AI Knowledge Base Infrastructure and Enterprise AI Agent Development, and asked us to use their preferred CrewAI framework (an open-source multi-agent orchestration tool). The contract is for 5 years involving about 8-9 headcount. Though I didn't participate in specific financial negotiations, based on typical federal contract pricing (each architect/engineer's hourly rate at $150-200, full-time annual cost around $200-300K), plus company profit margin, this is a multi-million dollar scale contract.

Here's the key: from our first contact with ACF to their decision to sign took only 3 meetings over about 3 weeks. Compare that to the previous pattern of 5-6 weeks and 5-6 meetings with no results.

## Result

### Significant Business Conversion Efficiency Gains

The most direct outcome of the AI Catalyst framework showed up in client conversion efficiency. After we started systematically applying the framework in January 2025, Arch Systems' AI consulting business showed clear improvement:

**Decision cycle shortened by 40%**: Average cycle from first contact to contract signing dropped from 5-6 weeks to 3 weeks, with meetings reduced from 5-6 to 3. The fundamental reason for this efficiency gain: the framework provided a "common language" and "decision map." Clients no longer needed to repeatedly weigh numerous vague options—they could quickly identify their current stage and the path they should take.

**Improved proposal quality and success rate**: Though I left Arch Systems and can't track long-term data, in the last few opportunities I participated in, conversion from technical proposal to contract award clearly improved. The key change: our proposals were no longer "comprehensive" technical capability showcases but precisely matched clients' chosen 1-2 Pathways, providing clear deliverables, timelines and success metrics.

### Successfully Delivered ACF Project Contract

In early 2025, Arch Systems landed a 5-year AI modernization contract with the Administration for Children and Families (ACF). ACF is a major agency under the Department of Health and Human Services (HHS), responsible for federal child welfare and family support programs.

The project covers two core Pathways:

- **AI Knowledge Base Infrastructure**: Building a RAG (Retrieval-Augmented Generation) knowledge retrieval system, integrating ACF's policy documents, case data, and operational guidelines scattered across departments
- **Enterprise AI Agent Development**: Developing multi-agent systems using the CrewAI framework to automate specific business processes and support intelligent decision-making

The contract supports about 8-9 full-time engineers and architects. Based on typical federal contract pricing models (considering each senior engineer/architect's loaded cost around $200-300K, plus company profit margin), total contract value should be in the $2.5-3.5M/year range.

More importantly, here's why we won: ACF's program manager explicitly mentioned in the final review that the AI Catalyst framework—particularly making Corporate Security Integration a standalone Pathway—demonstrated Arch Systems' deep understanding of federal compliance requirements. Compared to competitors who simply mentioned "we'll comply with FedRAMP and FISMA" in the last section of proposals, our framework treated security compliance as an intrinsic part of transformation from the start. This differentiated positioning became the deciding factor.

### Established Methodology Moat and Market Differentiation

The AI Catalyst framework's most profound impact is establishing a competitive advantage that's hard to replicate for Arch Systems. In the federal government contract market, differences in technical capability are actually small—most qualified contractors can hire competent engineers and architects. The real moat is:

**Repeatable delivery process**: The 7P framework isn't just a sales tool—it's a project implementation methodology. Each Pathway has clear stage divisions, deliverable definitions, and success criteria, letting Arch Systems deliver projects more predictably and reduce execution risk.

**Intellectual property assets**: Though the methodology itself can be described and learned, the demos, architecture templates, implementation docs, and training materials developed around each Pathway form a complete IP portfolio. Even if competitors understand the 7P concept, they need significant time and resources to rebuild these supporting assets.

**Client education and market positioning**: By openly sharing framework content (website, blog, technical talks), Arch Systems gradually built thought leadership in AI transformation consulting. When prospects search for "enterprise AI transformation methodology," they're starting to see Arch Systems as an expert in this field, not just one of many technology contractors.

### Technical Debt and Future Evolution Directions

As creator of any methodology, I also clearly recognize the framework's limitations and areas needing continuous improvement:

**Dependencies between Pathways not fully clarified**: Though I considered "foundation to advanced" progressive relationships during design, in actual application some enterprises may need to launch multiple Pathways simultaneously or combine them non-linearly. We need to develop a "Pathway Dependency Matrix" and "combination recommendation tool."

**Success metrics insufficiently standardized**: Each Pathway defines "what success looks like," but these definitions remain fairly qualitative. We should develop a quantitative maturity assessment model letting clients measure baseline before projects start and improvement after completion.

**Insufficient industry specialization**: The current 7P framework is a general design, but different industries (healthcare, finance, manufacturing, government) have very different AI transformation pain points and priorities. We could develop "industry-specific implementation guides" for each major industry.

**Integration with mainstream cloud platforms**: The framework is currently cloud-agnostic, but actual projects often require integration with specific AWS, Azure, or Google Cloud services. We could develop "platform-specific reference architectures."

### Personal Growth and Capability Expansion

This project's greatest value for me personally was dramatically expanding my understanding of the "Enterprise Architect" role. Traditionally, architect work focuses on technical design—drawing architecture diagrams, choosing tech stacks, defining API contracts. But this project made me realize that architects who truly have impact need capabilities at three levels:

**Technical depth**: Understanding all levels of the AI tech stack, from embedding models and vector databases to agent frameworks and LLM orchestration—this is the foundation.

**Business insight**: Understanding enterprise business processes, organizational structures, decision mechanisms, and translating technical capabilities into business value propositions—this is the leap.

**Methodology innovation**: Creating a systematic thinking framework and practice guide from scratch, making this framework a strategic organizational asset—this is the peak.

Through this project, I evolved from an "execution architect" (execute on given requirements) to a "strategic architect" (shape the requirements and strategy). I learned how to research markets, analyze competition, design differentiated positioning, how to transform technical insights into business assets, how to empower entire organizations through frameworks and tools.

Developing these capabilities has far more long-term value than simply completing a technical project. They let me—in any future scenario, whether joining a new company, starting my own business, or taking on higher-level technical leadership—quickly identify systemic problems, create structured solutions, and effectively influence stakeholders.

Though this project doesn't appear on my formal resume, it's one of the most fulfilling pieces of work in my career, because I didn't just solve a business problem—I created a knowledge asset that continues generating value.

## Reference

- [https://www.easyscalecloud.com/services/ai-catalyst-for-enterprise](https://www.easyscalecloud.com/services/ai-catalyst-for-enterprise)
- [https://easyscalecloud.atlassian.net/wiki/spaces/EP/pages/54689794/Page+-+AI+Catalyst+for+Enterprise+Service+Introduction](https://easyscalecloud.atlassian.net/wiki/spaces/EP/pages/54689794/Page+-+AI+Catalyst+for+Enterprise+Service+Introduction)
  </markdown_content>
</document>
<document>
  <title>2022-03-07 - Build Lab - A Fortune 500 global provider of mission-critical products and services to the biopharma</title>
  <markdown_content>
# 2022-03-07 - Build Lab - A Fortune 500 global provider of mission-critical products and services to the biopharma

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

A Fortune 500 global life sciences and biotechnology company specializing in mission-critical products for biopharma, healthcare, and advanced technologies had acquired a B2B medical supply e-commerce platform ([http://VWR.com](http://VWR.com) ) generating $6.4B in revenue, but the platform lacked product recommendation capabilities. The company maintained 7 million products in their centralized Snowflake data warehouse, organized into "Match Groups" by a third-party ML contractor whose contract was ending. They needed to build a rule-based recommendation engine to suggest alternative products that could either save customers money or increase profit margins, along with a human review workflow to validate recommendations before deployment. The challenge included undefined business rules at project start, unpredictable data formats, and the company's engineering team having no prior experience with AWS Glue, Lambda, or Augmented AI.

## Task

I was assigned as the lead AWS Data Lab Architect to design and build a proof-of-concept recommendation system during a four-day on-site Build Lab engagement. My responsibility was to implement a "VWR" business rule recommendation engine that would process 3,000 products across 100 Match Groups, create an automated human review workflow using AWS Augmented AI with conditional probabilistic triggering, benchmark the solution's scalability to 7 million products, and transfer complete knowledge to the customer's data engineers and data scientists. I needed to architect a flexible solution that could accommodate evolving business logic while working within the constraints of their existing Snowflake infrastructure and teaching the team AWS services in real-time as we built the solution together.

## Action

I designed an event-driven serverless architecture using AWS Glue for ETL partitioning, AWS Lambda for business logic execution, and AWS Augmented AI for human review workflows.

**Architecture Design:** When business rules remained undefined at lab start and I recognized they were too complex for PySpark, I pivoted the design to use AWS Lambda instead of AWS Glue for recommendation computation, since each Match Group contained fewer than 1,000 products that could fit in single-instance memory. I built an AWS Glue job that partitioned source data by Match Group and stored results as individual JSON files in Amazon S3, with each file triggering a Lambda function that applied pandas-based business rules to calculate recommendation results.

**Human Review Workflow:** I invented a custom conditional triggering mechanism for the human-in-loop workflow by designing a Lambda function that analyzed recommendation results and used probabilistic logic (random integer generation with threshold comparison) to trigger AWS Augmented AI tasks, since A2I lacked native conditional trigger support for custom workflows. I created a private workforce in Amazon SageMaker GroundTruth and developed a custom task UI template, then built a local rendering tool to accelerate UI development and bypass the traditional deploy-test-preview cycle. I implemented a second Lambda function for post-processing HIL outputs that automatically flagged recommendations as "disabled" based on reviewer decisions (deny/TBD) and sent notifications to developers.

**Knowledge Transfer & Development Environment:** I established AWS Cloud9 as the collaborative IDE for real-time pair programming, created a Glue Dev Endpoint with Jupyter Lab for interactive ETL development, and configured AWS CodeCommit for version control. I conducted focused training sessions on AWS Lambda best practices using the AWS Chalice framework and delivered live demonstrations of AWS Augmented AI capabilities to business stakeholders.

**Data Engineering Solutions:** I resolved critical data encoding issues by identifying that Microsoft Excel exported TSV files in UTF-16 encoding while Python defaulted to UTF-8, then implemented explicit encoding specifications. I orchestrated all pipeline components using AWS EventBridge to create a fully automated workflow from data ingestion through recommendation calculation, HIL triggering, and output post-processing.

**Performance Optimization:** I conducted scalability benchmarking using 7 million oversampled records with 10 G1.X Glue workers and 500 concurrent Lambda functions, achieving end-to-end processing in 13 minutes (5.5 minutes ETL + 7 minutes recommendation computation) to validate production readiness.

## Results

I successfully delivered a production-ready, event-driven recommendation engine and human review workflow that exceeded all project objectives in four days. The solution processed recommendation results for all products and enabled the customer to update recommendations every 15 minutes instead of their initially planned quarterly updates—demonstrating 2,880x improvement in refresh capability. The customer's engineering team gained hands-on expertise with AWS Glue, Lambda, Augmented AI, and event-driven architectures, with all code stored in AWS CodeCommit for internal reuse across other projects.
  </markdown_content>
</document>
<document>
  <title>2025-01 to 2025-09 Modernized Student Search Service (MSSS) - OpenSearch Migration Project</title>
  <markdown_content>
# 2025-01 to 2025-09 Modernized Student Search Service (MSSS) - OpenSearch Migration Project



## Overview

From January through October 2025, I served as Data/AI Architect at College Board, leading a complete backend migration of their core MSSS (Modernized Student Search Service) platform from DataStax to Amazon OpenSearch Service. We're talking about a mission-critical system here—1,900 university clients, 5 million student records, and roughly a million search queries every single day.

The results? Monthly operating costs dropped from $10K to $2.4K. Critical operations that used to take 1-2 minutes now complete in 1-10 seconds. Data freshness went from monthly to daily updates. And we completely eliminated a 3-person DevOps team. After going live in production, the system achieved zero failures from my code or architecture.

Then came the October stress test. A critical production incident brought down the entire system. The full 15-person team worked all day without finding the problem. I joined the call, diagnosed the root cause in 10 minutes, and had everything back online within the hour.

## Background

### About College Board

College Board has been around since 1900. If you've taken the SAT or an AP exam, you know them. They serve over 7 million students annually through standardized testing and their BigFuture platform, working with 6,000+ member institutions across the US higher education ecosystem.

But there's more to the story than testing. College Board runs critical enrollment management platforms that help universities identify, recruit, and connect with prospective students—which is where MSSS comes in.

### MSSS Platform: Mission-Critical Revenue Engine

MSSS (Modernized Student Search Service) is College Board's flagship student recruitment platform—a sophisticated backend search system that lets over 1,900 universities identify and contact target students from a pool of 5+ million high schoolers. Universities filter students across 300+ dimensions: academic profiles, geography, intended majors, you name it. Then they send targeted recruitment messages to build their prospect pipelines.

This is an extremely profitable core business for College Board, handling roughly 1 million queries per day. Here's the key thing to understand: these aren't your typical web searches. We're talking large bulk enterprise queries with massive result sets, and every single query directly converts to a customer order. The traffic volume might not sound huge, but the business value per query is through the roof. The system needs to handle these massive queries at millisecond-level response times while maintaining strict privacy controls and data security.

### Why I Was Brought In

In January 2025, BunnymanTech Data/AI Consulting signed a contract with College Board, and I was deployed as Data/AI Architect to two core platforms: MSSS and the Higher Education Reporting Portal. I reported directly to the product VP, working with a team of 30+ engineers total, with 15-16 people directly involved in the MSSS migration.

They brought me in for one main reason: **OpenSearch expertise is incredibly scarce.** College Board was migrating MSSS from DataStax to Amazon OpenSearch Service, but nobody in the organization had architect-level OpenSearch experience. When AWS first launched Amazon OpenSearch Service, there were only five subject matter experts across all of Amazon—I was one of them. During my time at AWS Data Lab, I led OpenSearch implementations for Xe Currency Exchange's entity matching system and Cox Automotive's entity resolution platform. College Board needed someone who could lead migration strategy, optimize large-scale query performance, ensure data integrity, and establish production best practices. That's where I came in.

## Personal Note

### Political Dynamics & Relationship Building

The biggest challenge on this project wasn't technical—it was navigating the politics as a contractor in an environment with extreme knowledge asymmetry. Across the entire company, I was the only expert in both OpenSearch and AI. Everyone else? Not even close. I needed to simultaneously play architect (system design), hands-on engineer (writing code), and mentor (guiding the team).

My strategy was simple: build long-term partnership through visible excellence, not dependency. This is the opposite of what many contractors do—they deliberately skip good automation and clear documentation to create dependency and extend their contracts. I went the other direction. Highest quality code (8,000+ lines with 70% documentation). Complete automation. Detailed knowledge transfer. I wanted stakeholders and engineers to clearly see my value.

My philosophy: when clients see maximum value, they'll keep giving you new projects. You don't need to create problems to stay employed.

This worked incredibly well. Technical leadership (Principal Architect, VP) recognized my work. Business stakeholders appreciated my approach. Engineers respected my style. And when that October production incident hit—where I diagnosed in 10 minutes what took the team a full day—it cemented my expert status.

### AI-First Engineering as Differentiator

While MSSS itself wasn't an AI project, I used it to demonstrate what "AI-first engineering" actually means. Most people's AI integration stops at chatbots. Mine was wired into everything:

- JIRA (project progress tracking)
- Confluence (specific folders of project docs)
- Code repositories (dynamic query access)
- AWS deployment status, logs, debugging tools

But I didn't just use these tools myself—I actively promoted them and shifted the team's mindset. Every line of code we write, every JIRA ticket, every document: the primary goal isn't human consumption, it's making it consumable by AI. This places higher demands on us as thinkers, and it's where my value as an AI-first practitioner really shines.

My open-source contributions in AI are leading-edge. My adoption speed for new AI tech often outpaces even large companies like Google. This isn't following—it's leading.

## Situation

### Legacy System Pain Points

College Board had been planning the MSSS backend migration for about a year. The existing system ran on DataStax (the commercial version of Apache Cassandra) as the search backend, and it was struggling:

**Expensive licensing.** DataStax monthly fees exceeded $5,000, pushing annual licensing costs over $60,000—a serious burden for a nonprofit.

**Dated user experience.** Query response times and UI interactions felt old. The system particularly struggled with large batch data updates, frustrating university admissions officers.

**Performance bottlenecks.** The critical bulk update operation (update by query) took 1-2 minutes. Users would click the update button, then wait 1-2 minutes before they could refresh and see new data. This was a high-frequency, revenue-impacting core function—universities constantly need to make incremental updates to search student data. This is what MSSS is all about.

**Heavy operational burden.** DataStax ran on self-managed EC2 instances, requiring a 3-person DevOps team. One engineer was completely dedicated to cluster management, patching, scaling, and other operational tasks. Massive waste of talent.

**Cloud-native strategy mismatch.** College Board had been all-in on AWS for years. They wanted to replace this expensive third-party solution with an AWS native service, simplifying their tech stack and improving operations.

### Inadequate Pre-Migration Planning

Before I joined, College Board had done some homework. They'd brought in AWS consulting companies' ProServe teams to talk about OpenSearch. Internal employees did initial research. The qualitative conclusion: "OpenSearch is the way to go."

But when I took over in January and dug deeper, I found critical gaps:

**No quantitative validation.** Everything was qualitative feature comparison—zero performance benchmarking, zero capacity planning. The team assumed "OpenSearch can handle this" but never actually proved it.

**No hands-on PoC.** Nobody had launched an OpenSearch cluster, let alone implemented the most complex, critical business logic in a test environment. The hardest part—bulk updates of 1 million records (update by query)—had zero feasibility validation.

**Capacity planning void.** Basic questions had no answers: How many nodes? What instance types? How much storage? Estimated monthly cost?

**Architecture capability gap.** The team had architects who could do qualitative analysis, but they lacked strong hands-on developers. Nobody really understood how to build OpenSearch applications, especially production-grade implementations with edge cases and performance optimization.

### Recognition of Critical Blocker

I have a particular style of uncovering problems—asking the right questions to surface hidden issues. Through this approach, I quickly identified the core risk: this team wasn't ready to execute the migration. If they pushed ahead with the original plan, worst case scenario was discovering halfway through that OpenSearch couldn't support core business requirements. That would mean massive wasted time and resources.

After deep discussions with the team, everyone came to the same realization: we needed a comprehensive PoC to validate technical feasibility before any full-scale implementation. Getting everyone aligned on this became the project's critical turning point.

## Task

### Primary Objective: Validate & Execute Full Migration

My core task was to plan and execute the entire MSSS backend migration from DataStax to OpenSearch, but based on situation analysis, I redefined the project phases:

**Phase 1 (Months 1-3): Comprehensive PoC & Architecture Validation**

- Validate whether OpenSearch can support all core business logic, especially the most complex bulk update scenarios
- Determine optimal architecture choice (Managed Cluster vs Serverless)
- Complete capacity planning and cost estimation
- Performance benchmarking with real production data

**Phase 2 (Months 3-6): Full Implementation & QA Deployment**

- Refactor PoC code into production-grade codebase
- Implement complete ETL pipeline, config management, testing framework
- Deploy to Dev and QA environments using AWS CDK for Infrastructure as Code
- Build operational debugging tools and automation

**Phase 3 (Months 6-9): QA, Improvement & Production Preparation**

- Comprehensive quality assurance and performance tuning
- Handle edge cases and production readiness checklist
- Knowledge transfer and team enablement
- Production deployment planning

**Phase 4 (Months 9+): Production Deployment & Support**

- Blue-green production cutover
- Post-deployment monitoring and incident response
- Handoff to internal team

### Multi-Dimensional Role

On this project, I wasn't just wearing one hat—I was wearing three:

**Architect**: Made all critical technical decisions—OpenSearch Managed Cluster vs Serverless, ingestion pipeline design, blue-green deployment strategy, transformation architecture.

**Hands-on Engineer**: Wrote the core code myself, especially transformation logic, ETL orchestration, and operational tools. The final 8,000+ line codebase (70% documentation) had quality that far exceeded typical contractor work.

**Project Manager**: Managed a 15-16 person team (3 direct reports), handled task breakdown, prioritization, milestone tracking, and cross-team coordination. I used my self-developed AI-powered project management system that reads JIRA tickets, Confluence docs, and code repositories to make informed decisions across the entire project. This AI assistant let me excel at all three demanding roles simultaneously.

### Success Criteria

Project success criteria were clearly defined:

**Functional completeness**: OpenSearch must support 100% of existing DataStax functionality, especially complex query filters (300+ searchable fields) and bulk update operations.

**Performance improvement**: Critical business operations (especially bulk updates) latency must be significantly better than DataStax, with perceptible user experience improvements.

**Cost optimization**: Monthly operating costs (infrastructure + licensing) must be lower than DataStax total cost of ownership.

**Zero-downtime migration**: Production cutover must be seamless without impacting business continuity for 1,900 university clients.

**Maintainability**: System must be easy for internal team to take over, eliminating dependency on dedicated DevOps engineers.

**Data freshness**: Increase from monthly updates to daily updates, improving data timeliness.

### Key Constraints

**Time constraints**: Complete the full cycle from PoC to production deployment within 9 months - an aggressive timeline for such a critical system.

**Knowledge gap**: Team had virtually zero OpenSearch experience, requiring extensive knowledge transfer and mentoring.

**Business continuity**: MSSS is a revenue-critical system where any production issue directly impacts customer orders and revenue, with extremely low risk tolerance.

**Handoff requirements**: As a contractor, I had to ensure the internal team could fully own this system without creating long-term dependency.

## Action

### Phase 1: Strategic PoC & Architecture Validation (Months 1-3)

#### Critical Decision: Managed Cluster vs Serverless

The first big decision: OpenSearch Managed Cluster or OpenSearch Serverless? The team's initial thinking was "obviously Serverless"—it's managed, scalable, hands-off. But as an OpenSearch expert, I immediately spotted two critical problems:

**Update by query not supported.** MSSS has a core requirement for bulk updates (think SQL: SELECT then UPDATE). In OpenSearch, that's "update by query." OpenSearch Serverless didn't support this feature at all. If the team had started migrating before discovering this, we'd have wasted massive time and resources.

**Pagination performance issues.** Serverless pagination struggles with large result sets, and MSSS enterprise searches routinely return tens of thousands of results.

Based on these two fundamental constraints, I went with OpenSearch Managed Cluster instead of Serverless. This seemingly simple decision prevented catastrophic failure down the road—a clear example of why early expert judgment matters.

#### Capacity Planning: From Months to One Day

Traditional capacity planning takes about a month: deploy test cluster, run various workloads, gradually tune configuration. But from my time at AWS as an OpenSearch SME, I'd built a private sizing calculator that could automatically determine cluster configuration based on data scale and traffic patterns.

I analyzed MSSS's numbers—5+ million student records, 300+ fields, 1 million daily bulk searches—and completed precise capacity estimation in **one day**:

- Instance type: r6g.xlarge (memory-optimized for search workloads)
- Node count: 3 data nodes + 3 dedicated master nodes
- Storage: 500GB per node with GP3 provisioned IOPS
- Monthly cost: ~$2,400

More importantly, my analysis showed OpenSearch infrastructure costs significantly lower than DataStax's EC2 fleet, and DataStax still carried $5,000+/month license fees plus a 3-person DevOps team. Strong business case for migration.

#### Transformation Pattern Abstraction: From 1,800 Lines to 395 Lines

MSSS data lives in Parquet files on S3, with **300+ searchable fields** needing transformation before ingestion into OpenSearch. Naive approach? Write one transformation function per field, averaging 5-6 lines each. That's 1,800+ lines of repetitive transformation logic—bloated codebase, nightmare to maintain, impossible to debug.

I designed something better—a "Transformation Strategy abstraction layer":

**Pattern recognition**: After analyzing all 300+ fields, I found only **15-16 transformation patterns**—things like "string lowercase," "date format conversion," "nested object flattening," "array de-duplication."

**Strategy abstraction**: Implement one reusable transformation operator (function) for each pattern.

**Declarative configuration**: Maintain a fact table (key-value mapping) where the key is the field name and value is the corresponding transformation pattern name.

**Dynamic dispatch**: The program dynamically looks up the transformation operator based on configuration and executes it.

This design reduced transformation logic for 300 fields down to **roughly 395 lines of code** (80+ lines defining patterns, 300 lines declarative configuration, 15 lines dispatch logic). Code maintainability improved by orders of magnitude:

- Pinpointing problems became trivial
- Adding new patterns is straightforward
- Code is structured and organized, not thousands of lines thrown together

This demonstrates strong Python design skills and abstraction thinking—and it's the foundation for the system's maintainability going forward.

#### Blue-Green Deployment via Index Alias

Traditional blue-green deployment in systems like DataStax means maintaining two complete clusters, then routing traffic between them—infrastructure utilization tanks, costs double.

OpenSearch has a cleaner solution: **index aliases**. You create two indices in the same cluster, and the alias is just a logical pointer. Blue-green deployment becomes elegant:

1. Ingest new data into new index (like `msss-v2`)
2. Run all testing and validation on new index
3. Once validated, update the alias pointer from `msss-v1` to `msss-v2`
4. Traffic instantly routes to new index—zero-downtime cutover

I open-sourced a Python library specifically for index lifecycle management and alias switching. This complex blue-green operation became a few lines of automated code. That's my engineering philosophy: build reusable solutions.

#### OpenSearch Ingestion Pipeline Design

OpenSearch Ingestion Pipeline is a serverless ingestion service AWS launched in late 2024—very new technology. The team didn't understand OpenSearch itself, let alone this pipeline, so research fell to me.

While OpenSearch Ingestion Pipeline supports on-the-fly data transformation, I made a key architectural decision: **fully prepare data before ingestion, not on-the-fly transformation.** Here's why:

- On-the-fly transformation is hard to debug—can't quickly locate issues
- Less stable than pre-transformation pipeline
- Insufficient control and observability
- Best practice: prepare data properly, then ingest as-is

This decision simplified the entire ingestion architecture. Each stage had clear, separated responsibilities, dramatically improving system debuggability and reliability.

#### Benchmarking Critical Business Logic

I evaluated all backend operations triggered by user actions, sorted by complexity, and picked **three most complex scenarios** for PoC validation. The most critical: **bulk update of 1 million records (update by query)**.

Here's the challenge: OpenSearch isn't a transactional system—it's eventually consistent. You can't update 1 million records instantly. I used an async query approach:

1. Submit update by query request
2. OpenSearch processes asynchronously
3. Poll progress through task API
4. Verify data consistency after completion

Through comprehensive benchmarking, I proved OpenSearch bulk updates are **5-6x faster than DataStax**:

- DataStax: 1-2 minutes (users wait 1-2 minutes before refreshing to see new data)
- OpenSearch: 1-10 seconds (most cases under 10 seconds, small batches just 1 second)

This performance improvement is massive for user experience. When users click the update button:

- Before: wait 1-2 minutes, then refresh to see new data
- After: refresh within 10 seconds to see new data, sometimes just 1 second

This is a **high-frequency revenue-critical operation**—universities constantly make incremental updates to search student data. That's MSSS's core value. My optimization here delivered extremely high business value.

#### ETL Pipeline: Divide-and-Conquer at Scale

The ETL pipeline from S3 Parquet files to OpenSearch index needs to process massive data—300+ files, each hundreds of MB. Traditional approaches like AWS Glue? Long processing times, high costs.

I reused the **Divide-and-Conquer open-source framework** I'd built for a Thrivent project in 2024:

- Split 300+ files into small batches (each batch 10-15 files)
- Assign each batch to an independent Lambda function for parallel processing
- My framework automatically tracks each Lambda execution status
- Built-in retry mechanism and exception handling
- Ensures all Lambda functions execute successfully

Through this parallel processing, the ETL pipeline runs **10x+ faster than traditional Glue jobs**, with costs also dropping 10x+. This shows how technical investments pay dividends—I built this framework once, and it keeps delivering value across multiple clients and projects.

### Phase 2: Production-Grade Implementation (Months 3-6)

After PoC success validation, I entered full implementation phase. This stage focused on refactoring prototype code into a production-grade system, including:

#### Code Refactoring & Modularization

I refactored the codebase with clear modular structure:

**Config management system**: Separated environment configuration (Dev/QA/Prod), used parameter store and secrets manager for sensitive data.

**Unit test system**: Wrote unit tests for all transformation operators and core logic, hitting 85%+ coverage.

**Integration test system**: Built end-to-end tests for the complete ETL pipeline, using synthetic data to validate edge cases.

**Operational debugging tools**: Developed internal utilities so developers could easily debug and pinpoint issues:

- Data validation checker (verify transformation correctness)
- Index health monitor (check OpenSearch cluster status)
- Query performance profiler (analyze slow queries)
- Troubleshooting scripts (quickly locate common issues)

These tools weren't just for me—I packaged them as reusable programs for the entire team. Since I'm a contractor who ultimately hands off to internal employees, I needed to ensure they could independently maintain the system.

#### Infrastructure as Code with AWS CDK

The entire infrastructure deployment used AWS CDK (TypeScript), covering:

- OpenSearch cluster provisioning (domain, nodes, security groups)
- IAM roles and policies (least-privilege access control)
- Lambda functions (ETL orchestration)
- S3 buckets (data lake)
- CloudWatch alarms (monitoring and alerting)
- Secrets Manager (credentials management)

Using CDK made infrastructure deployment completely automated, repeatable, and version-controlled. Across Dev/QA/Prod three environments, we could deploy identical stacks just by changing configuration parameters, ensuring environment consistency.

#### Documentation: 70% of Codebase

The final codebase: **8,000+ lines of code, with 70% being documentation**. This reflects my high code quality standards:

- Every function has detailed docstring (parameters, return values, usage examples)
- Every module has README explaining architecture and design decisions
- Comprehensive operational runbook (troubleshooting common issues)
- Architecture diagrams (system component interactions)

This extreme documentation level isn't showing off—it's ensuring successful knowledge transfer. My philosophy: a contractor's value isn't creating dependency, it's raising the bar. I wanted the client's internal team to seamlessly take over and operate independently.

### Phase 3: QA, Improvement & Production Readiness (Months 6-9)

This phase focused on quality assurance, edge cases, and production readiness:

**Comprehensive testing**: Used production-like data in QA environment for thorough testing across all user workflows and boundary conditions.

**Performance tuning**: Optimized query performance based on QA results, adjusted OpenSearch index settings (sharding strategy, replica count, refresh interval).

**Failure scenario testing**: Simulated various failures (node failure, network partition, data corruption) to validate system resilience.

**Knowledge transfer**: Transferred knowledge through code review sessions, architecture walkthroughs, and hands-on workshops. Thanks to high test coverage and extensive automation, the team could smoothly take over the system.

The customer rated my delivery quality and customer obsession **far higher than all contractors they'd seen**. Many contractors deliberately avoid good automation and documentation to create dependency and extend contracts. I'm the opposite—I want to establish long-term partnership through highest value, highest quality delivery. Let customers see my value, and they'll keep giving new projects. No need to create problems to stay employed.

### Phase 4: Production Deployment & Critical Incident (Months 9-10)

#### Seamless Production Cutover (September 2025)

In September, we successfully migrated MSSS to the OpenSearch production environment. Thanks to blue-green deployment via index alias, the cutover process was completely seamless:

1. Ingest new data to production OpenSearch index
2. Run DataStax and OpenSearch in parallel, dual-write to validate data consistency
3. Update alias pointer during low-traffic window
4. Traffic instantly switches to OpenSearch
5. Monitor 24 hours to confirm system stability before decommissioning DataStax

The entire cutover process had zero downtime, completely transparent to 1,900 university clients. This was the result of careful planning and solid engineering.

#### Critical Production Incident & 10-Minute Resolution (October 2025)

Early October (around October 1st)—critical production incident. **The entire MSSS system went down.** If we couldn't recover by Monday, it would directly impact all customer business.

**The situation**: All 15-16 people on the project team went on-call, working through the weekend to troubleshoot. After **an entire day**, they still couldn't find the root cause. System still down. Pressure mounting.

**My involvement**: I got the call and immediately joined troubleshooting. Based on my deep system understanding and experience, I located the root cause in **10 minutes**:

**Root cause**: The upstream data team's S3 bucket folder wasn't cleared of old data as agreed. Our ETL pipeline assumed the S3 folder was a snapshot—old data should be cleared before new data arrives. But an upstream automation failure meant old data stayed, mixing with new data:

- Data volume doubled, exceeding ETL pipeline timeout configuration
- Duplicate records caused data integrity errors
- OpenSearch indexing failed, system unavailable

**Resolution**: After identifying the root cause, I completed full resolution in **1 hour**:

1. Cleared old data from S3 folder
2. Re-ran ETL pipeline with correct data
3. Verified data integrity
4. Restored system to normal

This incident demonstrated exceptional troubleshooting ability—where a full 15-person team spent an entire day without results, I diagnosed the problem in 10 minutes and fully recovered in 1 hour. This massively relieved pressure on management and further cemented my expert status.

**Post-incident**: Since then, MSSS has never had any production issues caused by my code or architecture. Zero failures from my work.

## Result

### Quantifiable Business Impact

#### Cost Reduction: 76% Monthly Operating Cost Savings

**Before (DataStax)**:

- Infrastructure (self-managed EC2 fleet): ~$5,000/month
- License fees: $5,000+/month
- DevOps team (3 FTEs, 1 full-time dedicated): estimated labor cost $15,000+/month
- **Total**: ~$25,000/month (conservative estimate)

**After (OpenSearch Managed Cluster)**:

- Infrastructure (AWS OpenSearch Service): $2,400/month
- License fees: $0 (AWS managed service)
- DevOps team: $0 (completely eliminated dedicated maintenance)
- **Total**: $2,400/month

**Annual savings**: ~$270,000/year (conservative estimate)

Monthly hard costs dropped from ~$10,000 (infrastructure + licensing) to $2,400—cutting **76%**. More importantly, we completely eliminated the 3-person DevOps maintenance burden, freeing valuable engineering resources for higher-value work.

#### Performance Improvement: 5-6X Faster Critical Operations

**Bulk update latency (Update by Query)**:

- DataStax: 1-2 minutes
- OpenSearch: 1-10 seconds (most under 10 seconds, small batches just 1 second)
- **Improvement**: 5-6x performance boost

This performance jump transformed user experience. When users click the update button:

- Before: wait 1-2 minutes before clicking refresh to see new data
- After: refresh within 10 seconds to see new data, small batches just 1 second

This matters because it's a **high-frequency revenue-critical operation**—universities constantly make incremental updates to search student data. That's what MSSS is all about. My optimization here delivered massive business value.

#### Data Freshness: From Monthly to Daily Updates

**Before**: Data updated monthly. DataStax blue-green deployment meant redeploying EC2 clusters and re-running pipelines—complex, risky operations. Updates could only happen infrequently.

**After**: Data updates daily (nightly). Thanks to my blue-green deployment via index alias design, updates became trivial, automated, zero-downtime operations that could safely run every night.

**Business impact**: Data timeliness improved **30x** (monthly → daily). University clients now access much fresher student data, improving recruitment targeting accuracy.

#### System Stability & Operational Excellence

**Zero post-deployment incidents**: Since September production deployment, the system hasn't had any issues from my code or architecture (except that October upstream data quality problem, which I resolved in 1 hour).

**High test coverage**: 85%+ unit test coverage, comprehensive integration tests. System is extremely robust.

**Seamless knowledge transfer**: Thanks to extensive documentation (70% of the codebase) and high automation, the internal team smoothly took over the system. No long-term contractor dependency.

#### Code Quality & Maintainability

**Transformation logic**: Reduced from potentially 1,800+ lines of repetitive code to ~395 lines (through transformation pattern abstraction), code maintainability improved by **orders of magnitude**.

**Codebase size**: 8,000+ lines of code with 70% being documentation, reflecting extremely high code quality standards.

**Reusable components**: The divide-and-conquer ETL framework, index alias management library, and operational debugging tools I developed are all reusable, providing foundation for future projects.

### Strategic Impact

#### Mission-Critical System Modernization

MSSS is **College Board's most core business, bar none**. Successfully migrating this revenue-critical, high-traffic system from legacy architecture to modern cloud-native solution was a major strategic win. This didn't just reduce costs and improve performance—it built a scalable foundation for future innovation (like AI-powered student matching).

#### Handling Millions of Daily Requests

The migrated system handles approximately **1 million query requests daily**. Important to emphasize: these are all large bulk enterprise searches with massive result sets. Each query directly converts to a customer order. The traffic might not look huge, but single query business value is extremely high. The system maintains high availability under strict latency requirements—any downtime directly impacts revenue.

#### Long-Term Partnership & Future Projects

After project completion, College Board clearly expressed satisfaction and committed to 2026 renewal, promising new AI projects. This isn't just recognition of technical delivery—it's recognition of my working style, customer obsession, and long-term partnership mindset.

I earned high praise from technical leadership (Principal Architect, VP) and won appreciation and respect from business stakeholders and frontline engineers. Multiple stakeholders rated my delivery quality and customer obsession **far higher than all contractors they'd seen**.

#### AI-First Engineering Leadership

While MSSS itself wasn't an AI project, I used it to demonstrate what "AI-first engineering" actually means:

- AI fully integrated with JIRA, Confluence, code repositories, AWS deployment tools
- Changed team mindset: every line of code, every JIRA ticket, every document must first be consumable by AI
- Actively promoted AI tools and trained team members

My AI integration far exceeded ordinary engineers. This was actually one of the original reasons College Board brought me in—they needed someone who could lead AI transformation. Through the MSSS project, I proved I'm not just an AI expert but a practitioner who can integrate AI into daily workflows.

### Personal Growth & Recognition

This project reinforced my abilities as the trinity of "architect + hands-on engineer + project manager." I did architecture design, personally wrote core code, and managed a 15-16 person team. Thanks to my self-developed AI-powered project management system, I could excel at all three demanding roles simultaneously.

The October production incident further cemented my expert status—10-minute root cause diagnosis where the full team spent an entire day. My deep expertise and troubleshooting abilities played a critical role.

Most importantly, this project validated my long-term partnership philosophy: build lasting relationships through highest quality delivery, extensive knowledge transfer, and customer obsession rather than creating dependency to extend contracts. This approach not only earned client trust and respect but also brought me new project opportunities for 2026.
  </markdown_content>
</document>
<document>
  <title>2025-09 to 2025-12 Business Intelligence AI Agent - Production-Grade Text-to-SQL System</title>
  <markdown_content>
# 2025-09 to 2025-12 Business Intelligence AI Agent - Production-Grade Text-to-SQL System



## Overview

Between October and December 2025, I built College Board's first production-grade AI application from the ground up. As their Data/AI Architect, I created a Business Intelligence AI Agent that understands natural language questions, dynamically generates SQL across a dual-database architecture (AWS Glue Catalog Athena + Amazon Redshift), and returns interpretable results.

The system is powered by my open-source ohmy-sql MCP engine, an AI-native data dictionary plugin, and Amazon Bedrock Agent's serverless infrastructure. In just three months, I shipped a full-stack application supporting 200+ tables and 20-40 active users. What used to take 3-5 days of manual SQL work now happens in 10-20 seconds. Complex analyses that previously required a minute now complete in roughly 10 seconds.

This project became College Board's flagship AI success story—and directly led to our contract renewal for the following year.

## Background

### About College Board

Founded in 1900, College Board is a nonprofit education organization that serves over 7 million students annually through SAT, AP exams, and the BigFuture platform. With more than 6,000 member institutions, it's a cornerstone of the US higher education ecosystem. Beyond standardized testing, the organization operates critical enrollment management platforms handling massive-scale data operations.

### My Engagement Context

In January 2025, BunnymanTech Data/AI Consulting signed a contract with College Board, and I joined as Data/AI Architect to lead their data modernization and AI transformation initiatives. After successfully delivering the MSSS OpenSearch migration project (primarily focused on search infrastructure), College Board decided to go all-in on AI adoption. I became the technical lead driving this strategic shift.

They brought me in for one simple reason: College Board had zero hands-on experience with Generative AI. My background spans the complete AI application lifecycle—workflow orchestration, RAG applications, AI agents, multi-agent systems, infrastructure, and operations. As an AWS-centric organization, my deep AWS expertise and proven delivery track record made me the natural choice to architect their AI innovation roadmap and lead production-grade implementations.

Here's the reality: **I was the only person in the entire company who understood AI**. All AI-related decisions, implementations, and proof-of-concepts had to flow through me. Other team members could provide input on use cases and user needs, but that was it. This extreme knowledge gap was both challenging and empowering—I had to build College Board's AI capabilities from absolute zero, with no existing infrastructure or accumulated experience to lean on.

## Personal Note

### Strategic Positioning: First Real AI Application

This BI Agent project was strategically critical for my long-term partnership with College Board. While the MSSS project succeeded, it was fundamentally an infrastructure modernization effort (an OpenSearch migration) that didn't showcase AI capabilities. The BI Agent, by contrast, is **College Board's first production AI application solving actual business problems**—not a proof-of-concept, but a tool that real users depend on every single day.

College Board took a pragmatic approach to AI adoption: they needed a pilot to validate AI's value, but it had to deliver genuine business impact, not just look good in a demo. Successfully shipping this project proved three critical things:

1. We can build production-grade AI applications
2. AI can solve real business problems
3. College Board has the capability to scale AI across the organization

The project's success directly secured our second-year contract renewal. Stakeholders saw more than just a working AI application—they saw me as an **end-to-end AI ecosystem expert** with capabilities spanning foundational engine development (ohmy-sql MCP), architecture design (Bedrock Agent + Lambda), full-stack implementation, and production deployment. This kind of comprehensive capability is extremely rare in the market.

### "Hexagonal Warrior" - Full Stack AI Engineer

This project showcased what I call my "hexagonal warrior" skillset—complete mastery across multiple domains:

**Backend**:

- Custom open-source engine (ohmy-sql MCP)
- Complex dual-database orchestration
- Serverless architecture with AWS CDK
- Performance optimization (caching, DuckDB)

**Frontend**:

- Full-stack React + Next.js development
- Chat interface design
- User experience optimization

**AI/ML**:

- Prompt engineering for text-to-SQL
- AI-native data dictionary design
- Agent orchestration with Bedrock

**DevOps/Infrastructure**:

- AWS CDK for IaC
- Lambda deployment with Power Tools
- Monitoring and auditing systems

Most backend engineers and architects can handle AI and system design, but they can't build actual applications. Being able to deliver complete solutions independently eliminates communication overhead and dramatically accelerates time-to-market. While I'm not responsible for long-term maintenance and handoff, covering both frontend and backend in the early stages gives projects far greater velocity than traditional teams achieve.

### Knowledge Transfer Challenge

Like the MSSS project, the biggest challenge was bridging the knowledge gap. As the company's only AI expert, I had to build the system while simultaneously educating the team, establishing best practices, and creating reusable frameworks. My approach:

**Build reusable components**: ohmy-sql MCP is open-source—anyone can use it. The data dictionary is structured so domain experts can contribute without deep technical knowledge.

**AI-native documentation**: I taught the team how to write documentation that AI can actually consume, not just traditional human-only docs. This fundamentally changed how the organization thinks about documentation.

**Auditing-first approach**: Through comprehensive auditing (SQL queries, result sets, user feedback), we built a system that can continuously improve without requiring my permanent presence.

This approach ensures long-term sustainability. The internal team can maintain and evolve the system even after I'm gone.

### Reflection: What This Project Means

The BI Agent project is my second major delivery at College Board, but it represents something fundamentally different:

- MSSS demonstrated infrastructure and data engineering chops
- BI Agent proved AI and product engineering expertise

Together, these projects showcase my full range of capabilities—the foundation for next year's renewal. More importantly, this project opened the door to AI for College Board. Stakeholders witnessed AI's genuine potential, creating a pipeline of future AI initiatives. I'm not just a contractor checking off tasks—I'm a strategic partner helping the organization transform.

## Situation

### Legacy Business Intelligence Process

Before the BI Agent, College Board's business intelligence workflow was entirely manual and painfully slow:

**The Old Way**:

- Business stakeholder (product lead or executive) identifies an analysis need
- Creates a JIRA ticket, assigns it to a data analyst or engineer
- Analyst manually writes SQL queries, connects to Redshift or Glue Catalog Athena
- Delivers a static report 3-5 days later
- If stakeholder has follow-up questions? Create another ticket, wait another 3-5 days

**The Pain Points**:

- **Glacial response time**: 3-5 days from question to answer made real-time decision-making impossible
- **Zero flexibility**: Static reports couldn't support iterative exploration. Every new question meant restarting the entire process
- **High technical barrier**: Stakeholders couldn't self-serve—they depended entirely on technical staff
- **Wasted resources**: Data analysts burned time on repetitive SQL queries instead of generating high-value insights
- **Scattered knowledge**: Business context and domain expertise lived in people's heads with no systematic documentation

### Dual-Database Architecture Complexity

College Board's data architecture added another layer of complexity. Data was split across two fundamentally different systems:

**AWS Glue Catalog + Athena**:

- S3-based serverless query service
- Primarily houses raw and semi-structured data
- Built for large-scale batch analytics
- Supports federated queries

**Amazon Redshift**:

- Traditional data warehouse
- Stores curated and aggregated data
- Optimized for OLAP workloads
- Requires cluster management

**The Challenges**:

- Different data lives in different systems (student data, activity data, user data spanning two databases)
- The two systems speak different SQL dialects (Athena uses Presto SQL, Redshift uses PostgreSQL-based SQL)
- Some business questions require **federated queries**—joining data from S3 (via Athena) and Redshift
- No unified metadata management or data dictionary existed

### Strategic AI Adoption Initiative

This project went beyond just "solving BI pain points"—it was **the first pilot in College Board's AI transformation strategy**:

**Strategic Goal**: College Board needed to prove that AI could deliver real business value, not just impressive demos or proofs-of-concept. This pilot had to be a production application solving genuine business problems with actual users.

**Technology Readiness Gap**:

- Zero AI infrastructure or frameworks
- Zero production AI experience
- Zero AI governance or best practices
- Exactly one AI expert (me)

**High Stakes**: As the first AI application, failure would have seriously undermined the entire AI adoption roadmap. Stakeholders needed tangible results to justify future AI investments.

### Pre-Existing Text-to-SQL Limitations

Text-to-SQL wasn't a new concept—plenty of market solutions existed (Thoughtspot, Tableau Ask Data, Power BI Q&A). But none of them worked for College Board:

**Why Commercial Tools Failed**:

- Didn't support dual-database architecture
- Couldn't understand College Board-specific business context
- Lacked customization for domain-specific terminology
- Black box systems with no explainability or auditability

**Why Previous Internal Attempts Failed**: Team members had tried simple text-to-SQL proofs-of-concept before, but they all fell flat:

- Shallow understanding of LLM prompting techniques
- No systematic metadata management
- No mechanism for injecting business context
- Unresolved performance and reliability issues

These failed attempts left stakeholders skeptical of text-to-SQL. I needed to prove that **with the right architecture and expertise, it actually works**.

## Task

### Primary Objective: First Production AI Application

My core mission: deliver College Board's first production-grade AI application within **3 months** (October-December 2025). Here's what that meant:

**Must-Have Requirements**:

- Understand natural language business questions
- Dynamically generate correct SQL queries across Redshift and Athena
- Return accurate, interpretable analysis results
- Support iterative exploration (users can ask follow-up questions)
- Production-ready (reliable, performant, auditable)
- Usable by real users solving real problems every day

**Success Criteria**:

- **Functional completeness**: Support common BI queries (aggregations, filters, joins, time-series analysis)
- **User adoption**: Real stakeholders willing to use this as their primary BI interface
- **Explainability**: Every answer includes SQL query and data preview for verification
- **Performance**: Fast enough for interactive exploration
- **Auditability**: Complete logging and auditing for compliance and continuous improvement

**Non-Goal**: This wasn't a PoC or research project—it had to be production-quality with measurable business impact.

### Multi-Dimensional Technical Challenges

This project's technical complexity went way beyond simple text-to-SQL:

**Challenge 1: Dual-Database Orchestration**

- How do you make AI intelligently choose between Athena and Redshift?
- How do you handle scenarios requiring federated queries?
- How do you unify two different SQL dialects?

**Challenge 2: Business Context Understanding**

- How do you make AI understand College Board-specific terminology ("SAT score band," "enrollment funnel," "yield rate")?
- How do you inject domain knowledge into AI's reasoning process?
- How do you handle ambiguous business questions?

**Challenge 3: Performance at Scale**

- Managing metadata for 200+ tables
- Complex queries might need 10-20 rounds of LLM calls (chain of thought)
- How do you optimize response time to acceptable levels?

**Challenge 4: Production Readiness**

- **Reliability**: How do you handle query failures and edge cases?
- **Explainability**: How do you make business users trust AI-generated results?
- **Auditability**: How do you track and debug issues?

### My Role: Solo Technical Leader + Full-Stack Engineer

I wore every critical hat in this project:

**AI Architect**: Designed the entire system architecture, chose the technology stack (Bedrock Agent, Lambda, CDK), and defined AI orchestration patterns.

**Backend Engineer**:

- Developed ohmy-sql MCP open-source engine
- Implemented data dictionary plugin
- Built Lambda-based tools ecosystem
- Designed caching and optimization strategies

**Frontend Engineer**: Built the chat interface using React + Next.js. While the team collaborated, I led the entire frontend architecture.

**DevOps Engineer**: AWS CDK for Infrastructure as Code, Lambda deployment with Power Tools, monitoring and logging setup.

**Domain Expert Interviewer**: Collected business knowledge through structured interviews with domain experts, then wrote the AI-native data dictionary.

**Project Manager**: Although the team had 6 people, all AI technical decisions were mine. I handled task breakdown, milestone planning, and stakeholder communication.

This "one-person team" model was demanding but brought incredible execution velocity—zero communication overhead, zero misalignment, and lightning-fast decision-making.

### Key Constraints

**Time pressure**: 3 months from idea to QA-ready was extremely aggressive, especially as College Board's first AI application with zero reusable components.

**Knowledge gap**: The team was completely unfamiliar with AI. All technical decisions and implementations had to come from me while simultaneously educating everyone else.

**Production quality requirements**: This couldn't be a demo. It had to be robust enough for real users to depend on daily—meaning comprehensive error handling, auditability, and performance optimization were non-negotiable.

**Technology constraints**: Had to use AWS services (College Board is AWS-centric) and align with their security and compliance requirements.

## Action

### Phase 1: Foundation & Open-Source Engine (October 2025)

#### Strategic Decision: Build vs Buy

The project's first critical decision: **build a custom solution vs adapt existing text-to-SQL tools**. After careful evaluation, I chose to build:

**Why Build**:

- Commercial tools didn't support dual-database architecture (Athena + Redshift)
- Existing solutions couldn't deeply customize for College Board's business context
- We needed full control over prompting strategies, metadata injection, and performance optimization
- I already had the ohmy-sql MCP open-source foundation to accelerate development

This decision let me deliver a truly tailored solution instead of force-fitting a generic tool.

#### ohmy-sql MCP: Unified Database Abstraction

My open-source project **ohmy-sql MCP** (Model Context Protocol for SQL) became the foundation of the entire system. It solved the core dual-database problem: providing a **unified abstraction layer** that connects different types of SQL databases.

**Supported Database Types**:

- Traditional relational databases: PostgreSQL, SQLite, MS SQL, MySQL, Oracle
- Cloud data warehouses: Amazon Redshift, Snowflake, BigQuery
- Serverless query engines: AWS Glue Catalog + Athena
- Even non-SQL systems with SQL interfaces: DynamoDB, OpenSearch, MongoDB

**Core Capabilities**:

- **Dynamic metadata retrieval**: Programmatically retrieve table schemas, column types, indexes, constraints
- **Unified query interface**: Abstract away differences between SQL dialects, provide consistent API
- **Connection pooling**: Efficiently manage database connections, support concurrent queries
- **Error handling**: Standardized error messages across different database systems

**Why this matters**: Without this unified abstraction layer, I'd need to write separate logic for Athena and Redshift, making the code impossibly complex and hard to maintain. ohmy-sql MCP lets the upper-layer AI agent treat both databases as a single unified data source, dramatically simplifying everything.

This wasn't built specifically for College Board—it was existing work from my technical portfolio. But it perfectly matched this use case, demonstrating how accumulated expertise creates leverage across different clients.

#### AI-Native Data Dictionary: Injecting Business Context

The biggest challenge in text-to-SQL isn't SQL generation—it's **understanding business context**. A generic LLM can write syntactically correct SQL all day long, but it can't write semantically meaningful SQL because it doesn't understand College Board-specific concepts like "enrollment funnel," "yield rate," or "SAT score band."

I designed an innovative **External Database Data Dictionary Plugin** that dynamically injects business knowledge into the AI's context:

**Data Dictionary Structure**:

1. **Business Concepts**: Definitions of high-level domain concepts (like "student lifecycle," "application funnel")
2. **Terminology**: College Board-specific terms with precise definitions
3. **Column Mappings**: Business terms to database column mappings (like "application date" → `applications.submitted_at`)
4. **Aggregation Rules**: Calculation logic for common metrics (like "conversion rate" = `COUNT(enrolled) / COUNT(applied)`)
5. **Few-Shot Examples**: Representative business questions and corresponding SQL queries

**Dynamic Loading Mechanism**:

- When AI processes a business question, the plugin analyzes keywords in the question
- Dynamically retrieves relevant sections from the data dictionary
- Injects this context into the LLM prompt
- LLM generates SQL based on enriched context

**AI-Native Design Philosophy**: Traditional data dictionaries are written for humans—long narratives, free-form text, ambiguous phrasing. My data dictionary is **AI-first**:

- Structured format (JSON/YAML) that's easy to parse
- Concise, precise definitions
- Explicit mappings instead of implicit knowledge
- Executable examples included

**Content Creation Process**: Data dictionary content wasn't created from thin air—it came from **interviewing domain experts**. I developed a systematic interview framework:

1. Identify key stakeholders and domain experts
2. Understand business concepts, terminology, and common questions through structured interviews
3. Encode this knowledge in AI-consumable format
4. Iterate with domain experts to verify accuracy

This approach ensured both quality and completeness while serving as a knowledge transfer process—tacit knowledge from domain experts became explicit, documented, and reusable.

Most people simply can't write documentation in this structured format because they don't understand how AI consumes information. My AI expertise enabled me to design something truly effective.

### Phase 2: Architecture & Orchestration (October-November 2025)

#### Amazon Bedrock Agent: Serverless AI Orchestration

The system's orchestration layer runs on **Amazon Bedrock Agent**, AWS's managed service for building AI agents. Why Bedrock Agent?

**The Advantages**:

- Fully managed—no infrastructure to babysit
- Native integration with AWS Lambda (all my tools are Lambda functions)
- Built-in conversation memory and session management
- Supports multiple LLMs (we use Claude Sonnet 4)
- Comprehensive logging and monitoring via CloudWatch

**The Architecture**:

- Bedrock Agent serves as the orchestration hub
- All capabilities encapsulated as Lambda Function Tools
- Agent interacts with databases through tool calling
- Responses stream back to the frontend

#### Lambda Function Tools: Modular Capabilities

I decomposed the system's capabilities into discrete **Lambda Function Tools**, each handling one specific task:

**Core Tools**:

1. **schema_explorer_tool**: Dynamically retrieves table schemas and metadata (via ohmy-sql MCP)
2. **dictionary_lookup_tool**: Fetches relevant business context from the data dictionary
3. **query_generator_tool**: Generates SQL query based on the business question
4. **query_executor_tool**: Executes SQL query on Athena or Redshift
5. **result_analyzer_tool**: Analyzes query results and generates insights
6. **cache_manager_tool**: Manages query result caching to S3

**Lambda Power Tools Integration**: Used **AWS Lambda Power Tools** to streamline development:

- Auto-generates OpenAPI Spec from Lambda function signatures
- Structured logging with correlation IDs
- Distributed tracing with X-Ray
- Parameter validation and error handling

**Deployment Automation**: Through **AWS CDK** (TypeScript) implemented Infrastructure as Code:

- Lambda functions automatically deployed with proper IAM roles
- OpenAPI specs auto-generated and registered with Bedrock Agent
- Environment-specific configurations (Dev/QA/Prod)
- One-command deployment across environments

This modular architecture makes the system incredibly maintainable—each tool can be independently developed, tested, and deployed without affecting the others.

#### Intelligent Database Selection Logic

The key to handling the dual-database architecture is **intelligent routing**—the AI needs to automatically decide whether to use Athena, Redshift, or a federated query.

I implemented multi-stage decision logic:

**Stage 1: Metadata Analysis**

- Agent first calls `schema_explorer_tool` to get available tables
- Tool returns which tables are in Athena, which in Redshift
- Agent makes initial routing decision based on table availability

**Stage 2: Query Pattern Recognition**

- If the query needs to join tables from both databases → use Athena's federated query capability
- If the query primarily accesses Redshift tables → route to Redshift
- If the query primarily accesses S3 data → route to Athena

**Stage 3: Performance Consideration**

- For large scans, prefer Athena (optimized for S3 data)
- For complex aggregations on curated data, prefer Redshift (optimized for OLAP)

This logic is embedded in the AI's prompting—through carefully crafted prompts, I guide the AI to make the right routing decisions.

### Phase 3: Performance Optimization (November 2025)

#### Challenge: Multi-Round LLM Calls

AI agents typically need a **chain of thought**—a complex business question might require 10-20 rounds of LLM calls and tool invocations:

1. Understand the business question
2. Query relevant schema
3. Query data dictionary
4. Generate initial SQL
5. Execute query and get results
6. Analyze results
7. If further exploration needed, repeat steps 3-6

If each step is slow, the overall user experience becomes painful (potentially several minutes of waiting).

#### Solution 1: Result Set Caching with Parquet + S3

I built an intelligent caching mechanism:

**Caching Strategy**:

- After SQL query execution, results get stored in **Parquet format** on S3
- Parquet is a columnar format—incredibly efficient for analytical queries
- Each cached result includes metadata (query hash, timestamp, schema)
- Cache has TTL (time-to-live) to ensure data freshness

**Cache Hit Path**:

- When the agent needs to do further analysis on a result set, it first checks the cache
- On cache hit, it directly loads the Parquet file from S3 into memory
- Uses **DuckDB** (in-memory analytical database) to analyze the cached data
- DuckDB natively reads Parquet with blazing-fast query speed

**Performance Impact**:

- Traditional path: Call Redshift Data API → 3-5 seconds per query
- Cached path: Load Parquet + DuckDB query → ~1 second
- **For scenarios requiring multiple iterative analyses, response time drops from 1 minute to about 10 seconds**

This optimization dramatically improved user experience for complex business questions.

#### Solution 2: Redshift Data API Optimization

For queries that must hit the database, I optimized the execution path:

**Use Redshift Data API**:

- Asynchronous query execution (doesn't block Lambda)
- Automatic result pagination for large result sets
- Built-in query result caching (managed by AWS)

**Connection Pooling**:

- Lambda functions reuse database connections
- Reduces connection establishment overhead

**Query Optimization Hints**:

- AI-generated SQL includes optimization hints (like appropriate indexes)
- Data dictionary includes query optimization best practices

### Phase 4: Explainability & Auditability (November-December 2025)

#### Explainability: Show Your Work

Business users need to **trust** AI-generated results. I implemented comprehensive explainability features:

**Each AI Response Includes**:

1. **Natural language answer**: Directly answers the business question
2. **SQL query**: Complete generated SQL that users can review
3. **Data preview**: First 10-20 rows of query results for spot-checking
4. **Execution metadata**: Query execution time, rows returned, data source (Athena/Redshift)

**Interactive Verification**:

- Users can click "Show SQL" to see the complete query
- Users can download the complete result set as CSV
- Users can re-run the SQL in BI tools (like Tableau) to verify results

This transparency lets users verify the AI's work and build trust over time.

#### Auditability: Comprehensive Logging

I established extensive auditing infrastructure:

**Logged Information**:

- Every user question and AI response
- All generated SQL queries
- Query execution results (stored as Parquet on S3)
- LLM call traces (prompts, responses, latency)
- User feedback (thumbs up/down)

**Auditing S3 Bucket Structure**:

```
s3://bi-agent-audit/
  sessions/{session_id}/
    questions/{timestamp}.json
    queries/{timestamp}.sql
    results/{timestamp}.parquet
  feedback/{date}/
    negative_feedback.jsonl

```

**Continuous Improvement Workflow**:

- If users think a response is unreliable, they click the "Report Issue" button
- System records complete context to audit logs
- I regularly review negative feedback and identify patterns
- Based on patterns, improve prompts, data dictionary, or tool implementations

This auditing-first approach enables **continuous improvement** even after I hand things off.

### Phase 5: Full-Stack Implementation (November-December 2025)

#### Backend: Lambda Functions + CDK Deployment

**Code Organization**:

- Monorepo structure with clear module separation
- Each Lambda function in its own directory with tests
- Shared utilities (ohmy-sql MCP, data dictionary client) in common lib
- CDK infrastructure code in separate stack

**Deployment Pipeline**:

- AWS CDK for Infrastructure as Code
- One-command deployment: `cdk deploy --all`
- Automatic environment configuration (Dev/QA/Prod)
- Integration with CI/CD (GitHub Actions)

**Testing Strategy**:

- Unit tests for each tool's business logic
- Integration tests using mock databases
- End-to-end tests with real queries in QA environment

#### Frontend: React + Next.js Chat Interface

While there was team collaboration on frontend, I led the entire architecture:

**Technology Stack**:

- **React** for UI components
- **Next.js** for server-side rendering and routing
- **TypeScript** for type safety
- **Tailwind CSS** for styling

**Key Features**:

1. **Chat Interface**:
- Natural language input box
- Streaming responses (results appear in real-time)
- Conversation history
2. **SQL Viewer**:
- Syntax-highlighted SQL display
- Copy-to-clipboard functionality
- "Run in Redshift" link for verification
3. **Data Visualization**:
- Automatic chart generation for numeric results
- Table view with sorting and filtering
- Export to CSV/Excel
4. **User Feedback**:
- Thumbs up/down for each response
- "Report Issue" button with detailed feedback form
- Star rating for overall experience

**Frontend-Backend Integration**:

- RESTful API for invoking Bedrock Agent
- WebSocket for streaming responses
- JWT authentication for user sessions

As a "hexagonal warrior," I could cover both ends, making frontend-backend integration incredibly smooth without the typical miscommunication issues between separate teams.

### Phase 6: QA & Production Readiness (December 2025)

#### User Testing with Real Stakeholders

In December we started intensive user testing:

**Test Users**:

- 2 product teams, 20-40 people total
- Including product leads (non-technical stakeholders)
- Including data analysts (technical users)

**Testing Scenarios**:

- Common business questions (weekly metrics, trend analysis)
- Edge cases (ambiguous questions, complex joins)
- Performance testing (concurrent users, large result sets)

**Feedback Collection**:

- Structured feedback sessions once a week
- Users submit issues through in-app feedback buttons
- I review all negative feedback, prioritize improvements

#### Iterative Improvements

Based on user feedback, I made multiple iterations:

**Prompt Engineering Refinements**:

- Improved AI handling of ambiguous questions
- Added more few-shot examples to prompts
- Optimized error messages for better user guidance

**Data Dictionary Expansion**:

- Added new business concepts based on user questions
- Filled in missing column mappings
- Added more aggregation rules

**Performance Tuning**:

- Optimized slow queries (added indexes, rewrote queries)
- Adjusted caching strategy (more aggressive caching for common queries)
- Increased query timeout limits for complex analysis

**Bug Fixes**:

- Fixed edge cases (like empty result sets, special characters in queries)
- Improved error handling (graceful degradation)
- Added retry logic for transient failures

Through this iterative process, system quality and reliability improved dramatically, making it production-ready.

## Result

### Quantifiable Impact

#### Dramatic Speed Improvement: From Days to Seconds

**The Old Way**:

- Business question → JIRA ticket → Analyst writes SQL → 3-5 days → Static report
- Follow-up question → New ticket → Another 3-5 days

**The New Way**:

- Business question → Type in chat → 10-20 seconds → Interactive answer
- Follow-up question → Continue conversation → Another 10-20 seconds

**Speed improvement**: **From 3-5 days compressed to 10-20 seconds**—a **thousand-fold** improvement. For complex analyses requiring multiple rounds of exploration, the old approach might take weeks. BI Agent completes them in minutes.

#### Response Time Optimization

**Complex Query Performance**:

- **Without optimization**: Multi-round LLM calls + database queries could take 1+ minute
- **With caching + DuckDB**: Response time drops to **about 10 seconds**
- **Simple queries**: Usually under 10 seconds, many under 5 seconds

**Performance Breakdown**:

- Redshift Data API call: Average 3-5 seconds
- Cached Parquet + DuckDB analysis: Average ~1 second
- **Time saved per iteration**: 2-4 seconds
- **For 10-20 rounds of chain of thought**: Total savings of 20-80 seconds

This performance optimization enabled true **interactive exploration** instead of frustratingly long wait times.

#### User Adoption & Scale

**Active Users**:

- 2 product teams, 20-40 users continuously using during QA phase
- Including non-technical stakeholders (product leads, executives)
- Including technical users (data analysts, engineers)

**Database Coverage**:

- Supports **200+ tables** across Athena and Redshift
- Covers College Board's major business domains (student data, activity data, enrollment data, assessment data)

**Query Diversity**:

- Supports common BI queries: aggregations, filters, group by, time-series analysis
- Supports complex scenarios: multi-table joins, nested subqueries, window functions
- Supports federated queries spanning Athena and Redshift

#### Code Quality & Deliverables

**Backend Codebase**:

- Lambda functions with comprehensive error handling
- 70%+ test coverage (unit + integration tests)
- Extensive inline documentation
- Production-grade logging and monitoring

**Open-Source Contribution**:

- ohmy-sql MCP engine publicly available on GitHub
- Other developers can benefit from this unified database abstraction
- Demonstrates my commitment to open-source and knowledge sharing

**Frontend Application**:

- Modern React + Next.js architecture
- Responsive design for desktop and mobile
- Intuitive UX based on user feedback

**Infrastructure as Code**:

- Complete AWS CDK stacks
- One-command deployment
- Environment-specific configurations
- Easy to replicate and scale

### Strategic Business Impact

#### First Successful AI Application Sets Precedent

This BI Agent is **College Board's first production AI application actually solving business problems**, and its success has far-reaching implications:

**From Proof of Concept to Production Reality**:

- Proved that AI isn't just hype—it delivers tangible business value
- Shifted stakeholders from skepticism to enthusiasm by demonstrating AI's real potential
- Built confidence and momentum for future AI projects

**Template for Future AI Applications**:

- Established best practices for AI application development
- Created reusable components (ohmy-sql MCP, data dictionary framework, Bedrock Agent patterns)
- Defined quality standards (explainability, auditability, performance)

**Organizational Learning**:

- Team members learned how to interact with AI systems
- Domain experts learned how to contribute to AI (via data dictionary)
- Stakeholders learned how to evaluate and adopt AI solutions

#### Enabling Self-Service Analytics

BI Agent fundamentally changed College Board's business intelligence model:

**From Dependence to Self-Service**:

- Product leads and executives no longer wait for analysts
- They can explore data in real-time and make informed decisions
- Analysts are freed from repetitive queries to focus on high-value insights

**Democratized Access to Data**:

- Business users don't need to learn SQL
- Lowered the barrier to data analysis
- Promoted a data-driven culture

#### Foundation for Renewal & Future AI Projects

BI Agent's success directly secured our second-year contract renewal:

**What Stakeholders Saw**: Not just a working AI application, but me as an **end-to-end AI ecosystem expert** with capabilities spanning:

- Foundational engine development (ohmy-sql MCP)
- Architecture design (Bedrock Agent orchestration)
- Frontend and backend implementation (full-stack capability)
- Production deployment and operations

**Pipeline of Future AI Projects**:

- College Board now has confidence to pursue more AI initiatives
- I've already built the AI infrastructure and frameworks
- Future projects can leverage these foundations

**Strategic Partnership**:

- From "contractor completing tasks" to "strategic partner driving transformation"
- College Board recognizes my long-term value
- Renewal isn't contingent on specific projects, but based on overall AI capability building

### Technical Excellence & Innovation

#### Novel Solutions to Hard Problems

This project contained multiple technical innovations:

1. **Unified Database Abstraction (ohmy-sql MCP)**:

- Solved the fundamental challenge of dual-database complexity
- Provided reusable framework for future multi-database scenarios
- Open-sourced for broader community benefit

2. **AI-Native Data Dictionary**:

- Innovative approach to inject business context into AI
- Structured and machine-readable format
- Dynamic loading based on query context
- Systematic knowledge capture from domain experts

3. **Intelligent Caching with Parquet + DuckDB**:

- Non-traditional caching approach
- Leveraged columnar format and in-memory analytics
- Dramatic performance improvement for iterative exploration

4. **Comprehensive Auditability**:

- Every interaction logged for debugging and improvement
- S3-based audit trail for compliance
- User feedback loop for continuous enhancement

#### "Hexagonal Warrior" Demonstrated

This project perfectly showcases me as a "hexagonal warrior":

**Multi-Domain Expertise**:

- Backend: Lambda, CDK, Python
- Frontend: React, Next.js, TypeScript
- AI/ML: LLM prompting, agent orchestration, RAG patterns
- Data Engineering: SQL, database optimization, caching strategies
- DevOps: Infrastructure as Code, monitoring, logging

**End-to-End Ownership**:

- From requirements gathering (interviewing domain experts)
- To architecture design
- To implementation (both backend and frontend)
- To deployment and operations
- To user training and feedback collection

**Speed of Execution**:

- 3 months from idea to QA-ready
- Zero prior AI infrastructure at College Board
- Production-quality deliverables
- Only full-stack AI engineers can achieve this velocity

### Personal Growth & Recognition

#### Solidifying AI Leadership Position

This project further cemented my position at College Board:

**From Infrastructure Expert (MSSS) to AI Expert (BI Agent)**:

- MSSS demonstrated data engineering and infrastructure capabilities
- BI Agent proved AI and product engineering expertise
- Together they showcase my **full spectrum of technical leadership**

**Unique Value Proposition**:

- I'm the only AI expert in the entire company
- I can not only build AI but also ship complete applications
- I can not only implement but also architect and lead

**Trust from Stakeholders**:

- Product leads trust that I understand business needs
- Technical teams trust my architecture decisions
- Executives trust that I can deliver business value

#### Knowledge Transfer & Organizational Impact

Although I was the solo technical leader, I intentionally designed for knowledge transfer:

**Reusable Components**:

- ohmy-sql MCP is open-source, anyone can use it
- Data dictionary framework is documented and structured
- AI prompting patterns are shared with team

**AI-Native Documentation**:

- I taught the team how to write documentation that AI can easily consume
- Changed the organization's perception of documentation
- Established AI-first best practices

**Mentoring**:

- Team members learned AI concepts through participating in this project
- Domain experts learned how to contribute to AI systems
- Left College Board with sustainable AI capability

#### Setting Stage for Long-Term Partnership

BI Agent's success laid the groundwork for my long-term partnership with College Board:

**Renewal Secured**:

- Stakeholders clearly expressed their intent to continue our collaboration
- There's a pipeline of AI projects waiting for me to lead
- We've moved from project-based engagement to strategic partnership

**Expanding Scope**:

- No longer limited to specific projects
- Viewed as the overall leader of AI transformation
- Influence expanding beyond just implementation to strategy

**Market Differentiation**:

- In a competitive contractor market, my unique value is crystal clear
- Full-stack AI capability is extremely rare
- Proven track record of delivering production systems

This project proved my philosophy: **build long-term relationships through highest-quality delivery and genuine value creation**, not by creating dependency to extend contracts. This approach not only wins client trust but also drives sustainable business growth.
  </markdown_content>
</document>
<document>
  <title>2022-10 to 2023-01 - AWS Glue ETL Job for Fraud Detection Feature Engineering</title>
  <markdown_content>
# 2022-10 to 2023-01 - AWS Glue ETL Job for Fraud Detection Feature Engineering

> As a Data Architect / Engineer Consultant at Accenture Federal Service, I led the design and implementation of critical data infrastructure for the Federal Retirement Thrift Investment Board's pension asset management system, which serves all federal government employees. My work spanned the entire data engineering lifecycle, encompassing **data architecture**, **ETL pipeline development**, **workflow orchestration**, **data analytics**, and **PySpark job optimization**.

## Overview

I refactored the ETL pipeline for the Federal Retirement Thrift Investment Board's fraud detection system, replacing a 500+ line SQL monstrosity with clean, modular PySpark code. The old approach used deeply nested subqueries that nobody wanted to touch. I broke everything down into intermediate DataFrames and built a proper upsert mechanism with staging tables. This made the code actually maintainable, which meant the team could keep iterating on feature engineering as fraud detection requirements evolved.

## Background

The Federal Retirement Thrift Investment Board (FRTIB) needed a fraud detection system to protect user accounts. The system relied on a Feature Store in Redshift that fed behavioral data into ML models for fraud identification.

Raw data lived in S3, while the Feature Set sat in Redshift. The existing ETL process used an absolutely massive SQL script - over 500 lines packed with nested subqueries and CTEs (common table expressions). This script would read from S3, join everything together, clean the data, transform it, enrich it, and dump it into the Redshift Feature Store table.

The single giant SQL script was a maintenance nightmare. Nobody really understood how it worked, changing anything was risky, debugging was painful, and fraud detection requirements kept changing. The team needed to update feature engineering logic constantly, but the existing setup made that nearly impossible.

## Personal Note

I led this project from October 2022 through January 2023 when I left the company. My main strength here was deep PySpark expertise - I could take gnarly SQL logic and turn it into structured, readable Spark code.

The Glue Job code I wrote had real structure. Each transformation step became its own PySpark DataFrame operation with clear variable names and comments. All intermediate results got cached in-memory as DataFrames, which made modifications straightforward. Instead of editing one enormous SQL file, you could just update the relevant code block.

This clean architecture let the team keep iterating on feature engineering logic, both while I was there and after I left. The project really showcased my ability to handle large-scale data processing, code refactoring, and engineering quality.

## Situation

Here's what we were dealing with:

- **Unmaintainable code**: The ETL logic was a 500+ line SQL script full of nested subqueries and CTEs. Nobody wanted to touch it.
- **Constant iteration needed**: Fraud detection feature engineering had to evolve quickly, but the architecture couldn't support that pace.
- **Debugging hell**: When something broke in that massive SQL script, good luck finding the problem.
- **Complex data flow**: We needed to read raw user activity from S3, join it with the Feature Set in Redshift, clean everything, transform it, and enrich it.
- **Upsert challenges**: Redshift doesn't support primary key constraints, so we needed a reliable way to update the Feature Store table without creating duplicates.

## Task

Refactor the entire ETL pipeline into an AWS Glue Job with these requirements:

- Rewrite 500+ lines of SQL into PySpark while maintaining functional equivalence
- Make the code readable and maintainable so the team could iterate quickly
- Build out the complete data pipeline from S3 to Redshift (join, clean, transform, enrich)
- Design a solid upsert mechanism for updating the Redshift Feature Store table
- Keep everything debuggable and extensible

I owned the entire refactoring effort from October 2022 to January 2023.

## Action

### SQL to PySpark Refactoring Strategy

My approach was simple: break down every nested subquery into its own intermediate PySpark DataFrame. Each CTE and subquery in the original SQL became a clearly-named DataFrame variable stored in memory for later steps.

This made each transformation step independent and testable. You could actually understand what each DataFrame represented and what happened to it, instead of tracing through nested SQL chaos.

### Modular Code Structure

I split the ETL pipeline into distinct stages, each with its own DataFrame operations:

1. **Data Loading**: Pull raw user activity data from S3
2. **Feature Set Join**: Combine with the Feature Set from Redshift
3. **Data Cleaning**: Handle missing values and outliers
4. **Column Transformation**: Rename columns and convert data types
5. **Default Value Handling**: Fill nulls with sensible defaults
6. **Feature Enrichment**: Calculate derived features
7. **Final Preparation**: Get data ready for Redshift

Each stage got its own code block with clear comments. Easy to understand, easy to modify.

### In-Memory DataFrame Caching

I cached all intermediate transformation results as in-memory PySpark DataFrames to avoid recomputation. This improved performance, but more importantly, it made code changes painless. Need to adjust some transformation logic? Just modify that DataFrame operation without touching anything else.

### Staging Table Upsert Mechanism

Since Redshift doesn't support primary key constraints, I built an upsert process using a staging table:

1. **Create Staging Table**: Write new data to a temporary staging table first
2. **Identify Conflicts**: Use a join to find records in the original table that conflict with new data
3. **Delete Old Records**: Remove all conflicting records from the original table
4. **Insert New Records**: Move everything from the staging table into the original table
5. **Clean Up**: Drop the staging table

This kept data consistent and prevented duplicate records.

### Data Quality Enhancement

I added several data quality improvements during transformation:

- **Column Renaming**: Standardized messy column names
- **Missing Value Handling**: Filled nulls with reasonable defaults so downstream systems wouldn't break
- **Data Type Conversion**: Made sure every field matched the Feature Store schema

### Iterative Development Support

The refactored code structure naturally supported iteration. When fraud detection requirements changed and we needed to add or modify features, developers could:

1. Find the relevant transformation code block
2. Update the DataFrame operation
3. Validate intermediate results
4. Deploy the change

No more editing a giant SQL script and hoping nothing broke. Much lower risk, much easier debugging.

## Result

Between October 2022 and January 2023, I completely refactored the ETL pipeline, turning 500+ lines of tangled SQL into clean, modular PySpark code. The new AWS Glue Job made the code actually readable and maintainable, which let the team quickly respond to changing fraud detection requirements and iterate on feature engineering.

The intermediate DataFrame approach with in-memory caching made modifications safe and straightforward. The staging table upsert mechanism kept data consistent in Redshift.

This project demonstrated strong capabilities in large-scale data processing, code refactoring, and engineering quality. The clean architecture I built gave the team a solid foundation for continued iteration after I left.
  </markdown_content>
</document>
<document>
  <title>2023-01 to 2024-03 Third-Party Data Ingestion Platform</title>
  <markdown_content>
# 2023-01 to 2024-03 Third-Party Data Ingestion Platform



## Overview

We were dealing with music rights data from 10+ vendors, and it was a mess. Every vendor had their own format, their own way of sending data, their own schedule. I came in as Data Architect and Data Engineer after the previous architect left, and the team was basically rebuilding the wheel for every single data source. I built an extensible ingestion platform around push/pull patterns and the Command design pattern that standardized everything - SFTP drops, REST APIs, CSV, JSON, XML, you name it. Once we had that framework locked in, I led 6-7 engineers through 15 months of work, knocking out 10 data sources. Without the new architecture, we'd probably still be working on it two years later. Everything lands in a centralized data lake house on S3 and Redshift.

## Background

SoundExchange is a U.S. music rights org that collects usage data from streaming services like Spotify and Viva Music - that's how they calculate and distribute royalties. The data sources were completely inconsistent: some pushed files over SFTP, others exposed custom APIs, formats ranged from structured CSVs to semi-structured XML to completely unstructured text dumps.

I joined in January 2023 through an implementation partner as both architect and engineer. The previous architect had just walked, and they needed someone to own the technical direction immediately. They went with a contractor setup - faster than hiring, cleaner politically. My background building data platforms at AWS made me a good fit.

## Personal Note

There was apparently some friction between the previous architect and leadership, which is why they left. The company didn't have time to grow someone internally, but they also couldn't let the project sit. Bringing me in as a contractor solved both problems.

For me, this project was about proving I could design architecture for messy, real-world scenarios with lots of moving parts. No dramatic team dynamics or anything, but leading 6-7 engineers as an external contractor and getting them to adopt a framework I built - that's credibility.

The part I'm proudest of is how clean the solution turned out. Two patterns covered everything, and the Command pattern gave us a consistent interface. Nothing groundbreaking, but that's not the point. The point is finding the right level of abstraction so your team can actually ship.

## Situation

### Business Context

SoundExchange collects music usage data from 10+ vendors to calculate royalties. Every vendor did things differently:

- **How they sent data**: SFTP pushes, API pulls, shared S3 buckets
- **Formats**: CSV, JSON, XML, sometimes just text
- **Cadence**: Daily, weekly, monthly - completely inconsistent
- **Size**: Anywhere from a few megs to several gigs

### Core Challenges

Some data sources were already running when I got there, but they were held together with duct tape - ad-hoc scripts and Airflow jobs with no real architecture. The problems were obvious:

1. **Every New Source Was Starting From Scratch** Engineers built everything from the ground up every time: data fetching, state tracking, error handling, validation. Zero reuse. At that rate, getting 10 sources online would take 2-3 years easy.

2. **Airflow Wasn't Cutting It** The existing setup leaned hard on Airflow and manual scripts, but Airflow wasn't built for this:

- Event-driven scenarios (like "process this file when it lands on SFTP") were awkward to implement
- Lambda times out at 15 minutes, which killed us on large files, and Airflow workers had their own resource limits
- No good way to track state for incremental pulls - things like API pagination or resuming after a failure

3. **Code Was All Over the Place** Everyone wrote in their own style. No standard error handling, no logging conventions, no CI/CD. Every new source meant reinventing the wheel, and tech debt kept piling up.

4. **No Real Data Quality Checks** These vendors aren't exactly reliable - schemas change, data gets corrupted. AWS Glue Data Quality wasn't mature enough yet, and we didn't have any systematic validation. By the time we caught issues, they were already downstream causing problems.

### What Made This Hard

The individual sources weren't that complicated. The hard part was:

- **Variety**: 10+ sources, all with different tech and business logic
- **Scale**: More sources coming all the time, so the architecture had to grow with us
- **Coordination**: 6-7 people working in parallel meant we needed clear structure and ownership

## Task

### Project Goals

Coming in to replace the previous architect, here's what I needed to deliver:

1. **Build a unified architecture**: Create a platform that could handle all these different source types without turning into spaghetti code
2. **Set up a code framework**: Make it easy for engineers to add new sources - reusable components, clear patterns, no reinventing the wheel
3. **Lead the team**: Guide 6-7 engineers through implementation, make sure everyone followed the same standards
4. **Ship it**: Get all 10 sources into production within 15 months (January 2023 to March 2024)

### My Role

**I wore two hats: Data Architect and Data Engineer**

- Architecture side: Overall technical design, picking the tech stack, writing docs
- Engineering side: Built two of the most complex sources myself (Spotify and Viva Music) so the team had working examples to follow

**Tech Lead**

- Set coding standards and dev processes
- Reviewed everyone's code
- Made the hard technical calls

### Constraints

- **Timeline**: 15 months for 10 sources - roughly 6 weeks per source
- **Team**: 6-7 people with different skill levels, mix of data engineers and devops
- **Legacy code**: Had to play nice with existing Airflow jobs, couldn't just blow everything up
- **Production stability**: Some sources were live, couldn't break them while we rebuilt

## Action

I broke this into three phases: design the architecture, build the core framework, then enable the team to scale it out.

### Phase 1: Architecture Design and Tech Stack

#### Core Idea: Keep It Layered

I split the whole ingestion flow into two stages:

- **Stage 1 (Raw Ingestion)**: Get data from vendors into S3 exactly as-is. No transformation, no cleanup, just pure raw data
- **Stage 2 (Normalization)**: Take that raw mess and transform it into a clean, unified schema in our curated layer

Why split it like this?

- Keeps acquisition and transformation separate - easier to reason about
- Raw layer gives us perfect audit trails and makes debugging way easier
- Each layer can be optimized independently

#### Push vs Pull: The Two Patterns That Matter

After looking at all 10+ sources, I realized they all fell into two buckets:

**Push Pattern (They Send Us Data)**

- Classic example: Vendor drops files on SFTP
- Event-driven - we react when data shows up
- Example: Viva Music

**Pull Pattern (We Fetch Data)**

- Classic example: We call their API on a schedule
- Needs state tracking - where did we leave off? What's the next page?
- Example: Spotify

Once I saw this, the architecture got simple:

- Push scenarios → Event-driven (EventBridge + Lambda/Batch)
- Pull scenarios → State tracking (DynamoDB + scheduled jobs)

#### AWS Services

**Stage 1 (Raw Ingestion):**

- **EventBridge**: Catches S3 events, SFTP arrivals, whatever
- **Lambda**: Handles small files (<6GB) and quick jobs
- **AWS Batch + ECS Fargate**: Handles big files and long-running stuff, no limits
- **DynamoDB**: Tracks what we've ingested and what's pending
- **S3**: Raw storage

**Stage 2 (Normalization):**

- **AWS Glue**: Runs the ETL jobs
- **Airflow (MWAA)**: Orchestrates the complex workflows, keeps legacy stuff working
- **Step Functions**: For new workflows - cleaner than Airflow
- **Redshift**: Final warehouse for analytics

#### Data Validation

AWS Glue Data Quality wasn't ready for prime time yet, so I built validation around Pydantic:

1. Define a schema for each source using Pydantic models
2. After Glue transforms the data, sample it (can't validate billions of rows)
3. Run samples through Pydantic validators
4. Log failures and alert

Why Pydantic? Everyone knows it, it's solid, and the team didn't need to learn something new.

### Phase 2: Building the Code Framework

This was the make-or-break part. I needed a framework that let engineers with different skill levels ship sources quickly and consistently.

#### Command Pattern as the Foundation

I used the Command pattern to keep things clean:

**The basic idea:**

- Each source's ingestion logic lives in its own Command object
- Every Command exposes the same interface: `execute(source_uri: str)` - you pass in where the data is, it returns where it landed in S3
- All the stateful stuff (SFTP clients, HTTP clients, S3 clients) gets injected when you initialize the Command

**Code structure (simplified):**

```python
class IngestionCommand(ABC):
    def __init__(self, config: Dict):
        # Set up clients and config
        self.config = config
        self.s3_client = boto3.client('s3')
        
    @abstractmethod
    def fetch(self, source_uri: str) -> bytes:
        """Get data from the source"""
        pass
    
    @abstractmethod
    def track_state(self, metadata: Dict):
        """Save state to DynamoDB"""
        pass
    
    def execute(self, source_uri: str) -> str:
        """Main entry point"""
        # 1. fetch data
        data = self.fetch(source_uri)
        # 2. upload to S3 raw layer
        s3_path = self.upload_to_s3(data)
        # 3. record state
        self.track_state({'source': source_uri, 's3_path': s3_path})
        return s3_path

```

#### How Each Pattern Breaks Down

**Pull Pattern (Spotify example):**

1. **Client Layer**: Wraps the HTTP client - handles auth, retries, pagination
2. **State Tracker**: DynamoDB table remembers what we've already pulled
3. **Sync Layer**: Looks at state, figures out what to pull next, updates state when done

**Push Pattern (Viva Music SFTP example):**

1. **Client Layer**: Wraps SFTP client - connection, auth, file listing
2. **Event Handler**: Lambda listens for EventBridge events, parses file info
3. **Sync Layer**: Looks at file size, routes to Lambda or Batch, downloads to S3

#### Reference Implementations

I built two sources myself as examples for the team:

- **Spotify (Pull)**: HTTP client hitting their API, DynamoDB tracking pagination, Lambda running on a schedule
- **Viva Music (Push)**: SFTP upload triggers EventBridge, routes to Lambda or Batch based on file size

These two covered push and pull completely. Engineers could:

1. Copy the right template
2. Swap out the client (change HTTP to whatever)
3. Tweak the business logic (API params, file paths, etc.)

That's it. No starting from scratch.

#### CI/CD

Used Jenkins for automation:

- Tests run automatically on commit
- If tests pass, package Lambda or build Docker image
- Deploy to dev, test it, manual approval for prod

### Phase 3: Scaling Through the Team

As tech lead, here's how I made sure everyone could execute:

1. **Docs and Training**

- Wrote up architecture docs explaining when to use push vs pull
- Ran code walkthroughs on Spotify and Viva Music so everyone understood the patterns
- Created a code review checklist so we stayed consistent

2. **Smart Task Assignment**

- Gave people sources that matched their skill level
- Knocked out the easy ones first to prove the framework worked
- I took the gnarly technical problems

3. **Keep Improving**

- Listened to feedback and refined the framework as we went
- Pulled common patterns into utility functions
- Regular syncs to unblock people

4. **Quality Gates**

- I reviewed all code before merge
- Required unit tests for core logic
- Everything got tested in dev before prod

## Result

### What We Shipped

**Hit all the goals:**

- Got 10 data sources into production in 15 months (Jan 2023 - Mar 2024)
- Everything flowing through the layered S3 architecture: Raw → Curated → Redshift
- Left behind a framework that made adding new sources way faster

### Impact

1. **Speed**

- Cut what would've been 2-3 years of work down to 15 months
- Once the framework was solid, new sources went from weeks of work to days
- Engineers stopped rebuilding the same stuff over and over

2. **Maintainability**

- Consistent code structure made everything easier to understand and fix
- Command pattern kept each source isolated - debugging got way simpler
- CI/CD caught issues before they hit prod

3. **Scalability**

- Push/pull patterns covered basically everything, so the architecture has staying power
- New engineers could ramp up fast using Spotify and Viva Music as examples
- Built it to handle future stuff like streaming data

4. **Cleaned Up Tech Debt**

- Replaced a bunch of hacky scripts and fragile Airflow jobs
- Standardized logging, monitoring, alerts
- Data validation framework caught issues early instead of downstream

### Technical Highlights

**Architecture:**

- Distilled all the complexity into two patterns (push/pull)
- Layered design (Raw → Curated) kept acquisition and transformation separate
- Mixed event-driven and state-tracking to handle both real-time and batch

**Code:**

- Command pattern gave us a consistent interface across all sources
- Modular design made everything reusable
- Automated CI/CD kept quality high

### Personal Win

Coming in as an external contractor, I:

- Stepped into the previous architect's shoes and got the team's buy-in
- Drove the whole platform architecture and built a framework people actually used
- Led 6-7 engineers working in parallel and shipped on time
- Built the two hardest sources myself - proved I could go from design to code

This project showed I can handle complex architecture, lead a team, and actually ship. Push/pull and Command pattern aren't new ideas, but finding the right abstraction level for a messy real-world problem and making it easy for a team to execute on - that's the hard part, and that's what matters.
  </markdown_content>
</document>
<document>
  <title>2021-10-04 - Build Lab - $1 billion global management consulting firm</title>
  <markdown_content>
# 2021-10-04 - Build Lab - $1 billion global management consulting firm

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

I was assigned to a $1 billion global management consulting firm specializing in healthcare, higher education, and data analytics services. Their internal Insights and Analytics team was building a centralized analytics platform to serve all business units across the enterprise. The team received daily data feeds from multiple sources (Salesforce, Workday, public datasets) stored in Amazon S3, but their existing ETL pipeline suffered from critical performance and maintainability issues blocking scalability. The legacy architecture used an anti-pattern of event-driven middleware (AWS CloudWatch, SQS, and Lambda) to trigger batch AWS Glue ETL jobs, which caused initialization bottlenecks when processing concurrent events. Additionally, their data access control model using Glue Catalog Resource Policy couldn't scale to manage 300+ rapidly growing tables and lacked column-level and row-level security capabilities. The team of three data engineers and one data architect had limited experience with AWS Glue ETL development and lacked a clear strategy for error handling and access management.

## Task

I was responsible for architecting and building a production-ready, scalable data lake solution with comprehensive security controls during a five-day intensive Build Lab engagement. My deliverables included designing and implementing a three-step ETL pipeline using AWS Glue to replace the problematic Lambda-based architecture, migrating configuration and runtime data from PostgreSQL to DynamoDB, establishing a functioning data lake with properly formatted data (snappy-compressed Parquet), implementing AWS Lake Formation with tag-based access control for database/table/column/row-level security across four personas, and orchestrating the entire workflow using AWS Step Functions with robust error handling. The unique challenge of this Build Lab required me to work on-site with the customer's engineers, building the POC solution collaboratively while simultaneously training them to replicate and extend the architecture independently in their own AWS environment.

## Action

I designed and implemented a comprehensive data lake architecture consisting of three distinct processing stages with separate S3 buckets for landing, staging, data lake, and purpose-built zones.

**ETL Pipeline Development:** I built three sequential AWS Glue ETL jobs in Python using PySpark—the first job converted mixed-format source data (CSV, JSON, Excel) to Parquet while validating schemas against DynamoDB metadata and dropping invalid records; the second performed data cleansing, merging, unnesting, and null-filling operations; and the third denormalized data into single-view fact tables for specific business use cases. I introduced the team to the PynamoDB Python library and guided them through migrating 30+ PostgreSQL configuration tables to DynamoDB, eliminating the database connection management complexity.

**Security Implementation:** I provided comprehensive training on AWS Lake Formation and implemented tag-based access control (TBAC) methodology, configuring database, table, column, and row-level permissions for four distinct personas (developers, security admins, business analysts, data scientists), then validated the access controls through extensive testing in Amazon Athena and Amazon QuickSight.

**Workflow Orchestration:** I taught the team AWS Step Functions concepts and best practices, then helped them develop a state machine using Amazon States Language to orchestrate the three-step ETL pipeline with conditional logic, job sequencing, and sophisticated error handling for different failure scenarios.

**Technical Problem-Solving:** Throughout the lab, I debugged critical issues including missing IAM policies preventing Glue Dev Endpoint notebook library imports, non-functional Glue bookmark mechanisms due to non-distinct transformation contexts, and QuickSight permissions requiring specific IAM policies beyond standard Athena access. I also integrated AWS CloudWatch Events with SNS for ETL job performance monitoring and notifications. I conducted daily checkpoint sessions, coordinated subject matter expert consultations with AWS Glue and DynamoDB specialists, and ensured the customer team gained hands-on experience with every component, leaving them confident to extend the solution independently.

## Results

Within five days, the customer team successfully delivered a fully functional, production-ready data lake with 300+ datasets in optimized Parquet format, a scalable three-step ETL pipeline processing data from landing to purpose-built zones, comprehensive Lake Formation-based security controls validated across four personas, and a serverless Step Functions orchestration engine managing the complete workflow. Performance benchmarking confirmed the architecture could support data traffic growth for the next five years, and the team was released from complex operational maintenance efforts to focus on higher-value business analytics work.
  </markdown_content>
</document>
<document>
  <title>2023-04-10 - Build Lab - data analytics and risk assessment firm</title>
  <markdown_content>
# 2023-04-10 - Build Lab - data analytics and risk assessment firm

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

I was assigned to a multinational data analytics and risk assessment firm specializing in insurance industry solutions, including fraud detection, claims processing, and actuarial services. This was a follow-up Build Lab engagement focused on improving an existing document deduplication pipeline initially developed in a previous lab. The customer faced a critical challenge processing multi-page medical documents (100-1,000 pages) containing potential duplicate pages from concatenated scans. Their manual review process was time-consuming and error-prone, while the existing automated solution had achieved a 66.6% false positive rate. The unique complexity involved distinguishing true duplicates from near-duplicates where minor differences in critical entities (patient names, service dates, balance amounts) determined whether pages should be preserved or discarded, requiring deep domain expertise to make accurate determinations.

### Task

I was responsible for enhancing the existing three-step deduplication pipeline to dramatically reduce false positives while maintaining high capture rates. Working on-site with the customer's development team and a medical document subject matter expert, I needed to deliver two key improvements: first, optimize the document similarity threshold using a larger labeled dataset to balance capture rate against false positive rate; and second, train and integrate a custom entity recognition model using Amazon Comprehend to identify business-critical entities like service dates that would inform duplication decisions. The deliverable was a working proof-of-concept system with a framework enabling the customer team to independently tune parameters, train additional models, and continue improvements post-lab.

### Action

**Threshold Optimization:** I collaborated with the medical document SME to create a labeled dataset of 550 page pairs from three documents, then designed a weighted scoring system balancing capture rate (weight of 1) and false positive rate (weight of -5, as false positives caused greater business impact). I developed Python code to parse human-generated Excel annotations, extract similarity scores and duplication labels, and calculate optimal thresholds across the confusion matrix. I built the code with extensible functions allowing the customer to easily modify business logic for header/footer handling and adjust the weighted score formula as requirements evolved.

**Custom Entity Recognition:** I implemented a training pipeline using Amazon Comprehend's custom entity recognizer, starting with "Service Date" as a pilot entity. I developed a Python program to convert SME annotations from Excel into plain-text annotation format with position information, addressing the challenge where entity values had multiple occurrences requiring manual disambiguation with the SME. Using 128 labeled examples split 65/35 for training/testing, I trained the model via the Comprehend API, achieving an F1 score of 69.72. I then integrated this model into the deduplication pipeline, creating decision logic where pages with similarity between upper and lower thresholds would be sent to entity recognition for final determination.

**Implementation Infrastructure:** I leveraged AWS Cloud9 on t3.xlarge EC2 instances (16GB memory to support the in-memory similarity model), AWS CodeCommit for version control, and Amazon S3 for data storage, building on infrastructure preserved from the previous lab. I conducted two validation experiments—first on the known 128-sample dataset achieving 100% accuracy, then on an unseen 300+ page-pair document achieving 87% accuracy (26/30 correct predictions), identifying that date format variations ("09/16/2022" vs "September 16, 2022") caused the four false negatives. Throughout the engagement, I trained customer engineers on the solution architecture, code structure, and how to independently conduct future experiments and model retraining.

### Results

I successfully delivered a dramatically improved deduplication system achieving a 97.4% capture rate with 0% false positive rate on new test data, compared to the previous lab's aggressive approach (98.2% capture rate, 66% false positive rate) and conservative approach (43% capture rate, 0% false positive rate). The customer team gained hands-on experience optimizing thresholds and training custom ML models, with a scalable framework enabling continuous improvement through additional labeled data without requiring architecture changes. I provided architectural guidance for production deployment options (real-time endpoints vs. asynchronous batch processing) and facilitated a consultation with an Amazon Comprehend SME who recommended strategies for handling the 25-entity-per-model limitation and implementing message queue buffering for production scalability.

  </markdown_content>
</document>
<document>
  <title>2023-03 to 2023-12 Dynamic KPI Engine - Data Architecture & Engineering</title>
  <markdown_content>
# 2023-03 to 2023-12 Dynamic KPI Engine - Data Architecture & Engineering



## Overview

I joined EDF Energy (a French power company) as Data Architect and Tech Lead to fix a major performance problem: their KPI calculation system was grinding to a halt. Every time they needed one metric, the system recalculated everything from scratch - completely unsustainable as they scaled.

I built a lazy-loading engine that figures out exactly which calculations it needs to run and nothing more. Using dynamic Step Function workflows and automatic dependency resolution, we got KPI calculations running 10x faster. Just as important, I refactored their messy codebase and mentored the team until they could own the whole system without me.

## Background

EDF Energy runs wind and solar farms across multiple sites. Each site is packed with IoT sensors tracking everything - power output, efficiency metrics, AC converter health, you name it. All that telemetry flows through Kafka into S3, which serves as the source of truth.

Nobody can sit around watching dashboards 24/7, so everything runs on automation. They also need to slice and dice this data for operational decisions. Our team focused on the data processing and KPI engine - the IoT collection side was someone else's problem.

BunnymanTech (the consulting firm I was with) landed this project, and they brought me in to untangle the architecture.

## Personal Note

This was one of those consulting gigs where someone had good ideas but needed someone to actually build it. They picked me because of my AWS data pipeline chops and CDK background - I'd even built a CDK alternative years earlier.

Here's the thing: there's no playbook for this kind of problem. You're inventing the solution from scratch. And you can't just code your way out - you have to get a whole team to believe in an approach they've never seen before.

The win came when Adam (one of the engineers) started leading new KPI engine work on his own after a few months of working together. That's when you know the knowledge actually stuck.

Biggest lesson: great architecture means nothing if you can't explain it clearly enough for your team to run with it.

## Situation

### Legacy System Crisis

Over four or five years, they'd piled up 20+ KPIs with all kinds of different aggregation rules - hourly, daily, weekly. Different plant types needed different metrics. The business logic was a tangled mess.

Here's what killed them: calculating *one* KPI meant running *every* related Glue Job and reprocessing *all* the historical data. As they added more KPIs, this approach just stopped working. Jobs took forever, results arrived late, and they were hemorrhaging money on unnecessary compute.

### Technical Challenges

The architecture had some serious issues:

1. **Zero dependency tracking**: Those 20+ KPIs depended on each other in complex ways (some needed outputs from other KPIs), but nothing tracked these relationships. Jobs just ran in whatever order.
2. **Step Functions can't do dynamic workflows**: They needed to orchestrate Glue Jobs with Step Functions, but the workflow had to change based on which KPIs were requested. Step Functions want everything defined upfront - you can't build workflows on the fly.
3. **When things broke, you were blind**: Glue Jobs run async. When one failed, good luck figuring out where. And there was no clean way to stop the whole workflow.
4. **Codebase was a disaster**: Infrastructure code mixed with business logic, configs all over the place.
5. **Team was green on Hudi**: They'd just started using Apache Hudi for upserts and didn't have best practices or even basic unit tests.

## Task

They brought me in as Data Architect and Tech Lead to:

1. **Redesign the engine**: Build a lazy-loading system that only runs necessary Glue Jobs instead of brute-forcing everything.
2. **Map the dependencies**: Figure out how all these KPIs relate and automate dependency resolution.
3. **Make dynamic workflows work**: Build dynamic orchestration within Step Function's static limitations.
4. **Fix observability**: Add proper error handling and logging so you can actually debug problems.
5. **Clean up the codebase**: Refactor the CDK infrastructure into something maintainable.
6. **Level up the team**: Get everyone comfortable with Hudi and modern DataOps practices.

Had to get this done between March and December 2023 without taking the system down.

## Action

### KPI Dependency Resolution System

First problem: we needed to know which jobs to run. I mapped out every KPI dependency, building a directed acyclic graph (DAG) of the relationships. Some Glue Jobs calculated multiple KPIs, and some KPIs needed results from other KPIs first.

Here's what I built:

**Single source of truth**: Threw all the KPI-to-Glue-Job dependencies into an Excel spreadsheet. Sounds low-tech, but it meant business folks could update dependencies without touching code.

**Dependency Resolver**: Lambda function using Python's toposort library. Give it a target KPI, and it calculates the minimal set of jobs you need to run, in the right order. True lazy loading - nothing gets computed unless it's actually needed.

### Dynamic Step Function Workflow with Event Loop Pattern

Now for the tricky part: Step Functions don't support dynamic workflows. You can't just generate a workflow on the fly based on what the Resolver spits out.

My solution was inspired by React's event loop:

**Core loop**: Instead of trying to build a dynamic graph, I built a single loop that does this:

- Check the execution plan for the next job
- If there's nothing left, we're done
- If there's a job to run, kick it off and wait
- Loop back

**Three Lambda functions make this work**:

- **Plan Iterator**: Tracks where we are in the execution plan
- **Glue Job Starter**: Launches jobs
- **Glue Job Ender**: Updates state when jobs finish

**Safety valve**: Max dependency depth of 10 levels, just in case something goes haywire.

This approach sidesteps Step Function's limitations while keeping all the built-in monitoring and visualization.

### State Management with DynamoDB

Tracking async Glue Job state was its own problem. I built a state machine on top of DynamoDB:

**Workflow records**: When a workflow kicks off, we create a DynamoDB record for each job that needs to run. Records link together like a linked list (Record1 → Record2 → Record3...).

**Locking**: Mark a record "Locked" when its job starts running. Release when it finishes. Now you can see exactly what's running at any moment.

**Pinpoint failures**: Job fails? Look up the Workflow ID in DynamoDB and you know immediately which job died. The whole workflow stops cleanly instead of limping along.

**Built-in audit trail**: The DynamoDB records are your execution logs. Debugging and auditing become straightforward.

This fixed the Event Loop's biggest weakness - without this, you'd have no idea what was happening inside the loop.

### Infrastructure as Code Refactoring

Their CDK code was a mess. I spent years working with CDK (even built an alternative back in the day), so I knew how to fix this:

**Modular architecture**:

- Split infrastructure from business logic
- Centralized config management
- Made stacks reusable and independent

**CI/CD cleanup**: Rebuilt the CodeBuild and CodePipeline configs for reliable deployments.

This wasn't just fixing today's problems - it set them up to scale this thing properly.

### Apache Hudi Best Practices & Team Mentoring

The team was shaky on Apache Hudi. I'd used Glue + Hudi and Delta Lake extensively at AWS, so I ran a structured training program:

**Built working examples**: Created several proof-of-concept projects showing Hudi in action across different scenarios.

**Introduced local testing**: Brought in techniques from my AWS DataOps workshop - running Glue Jobs locally in Docker containers, unit testing everything including Hudi logic. Pretty advanced stuff at the time.

**Taught the "why"**: Didn't just show them what to do. Explained the reasoning behind architectural decisions so they'd make good choices on their own.

**Mentored Adam intensively**: Worked closely with one engineer for three months. By October, he was leading new KPI engine development independently. That's when I knew the knowledge transfer worked.

### Team Leadership & Solution Buy-in

Getting a team to trust a completely novel approach isn't trivial. They'd never seen anything like this, and there were no references to point to.

Here's what worked:

- Drew clear diagrams and built working prototypes to make abstract ideas concrete
- Explained not just "what" but "why" for every design choice
- Wrote proof-of-concept code to show it wasn't just theory
- Kept discussions open and incorporated their feedback

Eventually everyone was on board and execution went smoothly.

## Result

### Performance Improvements

**10x faster calculations**: Went from "recompute everything every time" to "only compute what you need." KPI calculation time dropped from "can't finish" to predictable completion in hours.

**60-80% cost reduction**: No more reprocessing all historical data. AWS Glue bills dropped dramatically (exact savings depend on which KPIs get requested).

**System scales properly now**: Adding new KPIs doesn't drag performance down anymore. The Dependency Resolver handles more KPIs without breaking a sweat.

### Architectural Wins

**Dynamic workflows actually work**: Built fully dynamic orchestration inside Step Functions' static box. This pattern became the team's go-to for similar problems.

**State tracking solved error handling**: The DynamoDB approach works incredibly well. Finding failures went from "nearly impossible" to "instant via Workflow ID."

**Maintainable codebase**: Clean CDK structure made future work way easier.

### Team Growth

**Knowledge transfer succeeded**: By October, Adam was leading new development independently. Mission accomplished.

**Team leveled up**: Everyone got solid with Hudi and DataOps, including local unit testing - pretty advanced for 2023.

**Self-sufficient team**: They went from needing external help to designing and building complex data pipelines on their own.

### Project Delivery

**Shipped on time**: Started in March, core features done by October, wrapped in December. No downtime during the upgrade.

**Kept delivering value**: After I left, the system kept supporting new KPIs. Architecture scaled like it was supposed to.

**Client was happy**: Solved their technical problems and built team capability. Got great feedback from the client and Principal Architect.

### Personal Takeaways

This project showed what I bring as an architect:

- **Invent solutions**: Designed original approaches where nothing existed
- **Deep AWS knowledge**: Used CDK, Step Functions, DynamoDB, and Glue in creative ways
- **Lead teams**: Guided people through difficult technical work and made them self-sufficient
- **Communicate clearly**: Made complex ideas understandable and got everyone aligned

This experience set me up well for my next role as Enterprise Architect at Travelers Insurance, proving I can handle tough architectural challenges at scale.
  </markdown_content>
</document>
<document>
  <title>2022-09-19 - Design Lab - digital marketing and automation platform company</title>
  <markdown_content>
# 2022-09-19 - Design Lab - digital marketing and automation platform company

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

I was assigned to a leading digital marketing and automation platform company serving small businesses and nonprofits through email marketing, social media management, and online marketing tools. The customer's data and ML teams were facing critical challenges in scaling their machine learning operations: they had a working gender prediction model in their experimentation environment but lacked the infrastructure to efficiently productionize ML models. Their existing system struggled with managing hundreds of AWS Glue jobs for per-customer datasets, had concerns about Amazon SageMaker's ability to process large-scale batch inference workloads, and relied on manual, sequential ML task execution that couldn't scale across multiple models and customer accounts.

## Task

As the Data Lab Architect, I was responsible for designing comprehensive architectural solutions for both the customer's data pipeline and ML pipeline that would enable repeatable, scalable ML model development and deployment. I needed to resolve three immediate technical challenges around ETL job management, batch transformation scalability, and ML task orchestration, while also creating reference architectures for data preparation workflows that could handle 200GB of MySQL CDC data with 5 billion rows. Additionally, I was tasked with establishing a roadmap that would move the customer from manual ML experimentation to standardized, production-ready ML operations across hundreds of customer accounts.

## Action

I designed a comprehensive two-part architecture addressing both data and ML pipeline requirements.

**For the data pipeline**, I architected a solution using AWS DMS to capture CDC events from MySQL, AWS Glue for ETL processing with PySpark, and Amazon S3 as the data lake, implementing an incremental snapshot strategy that created point-in-time snapshots and periodically merged incremental changes to maintain data freshness. I solved the ETL job management challenge by decomposing workflows into "Transformation" jobs for complex business logic and highly parameterized "Filtering" jobs that could process hundreds of customer accounts concurrently using dynamic JSON configurations.

**For orchestration**, I evaluated AWS Step Functions against Apache Airflow and created a detailed comparison demonstrating Step Functions' advantages for the customer's data engineering and data science team composition—providing serverless operation, drag-and-drop workflow creation, native AWS service integration, and similar flow control capabilities without cluster management overhead. I demonstrated this through a live AWS Step Functions workflow showcasing Chain, Parallel, and Map patterns across four ETL jobs.

**For the ML pipeline**, I designed an end-to-end architecture leveraging Amazon SageMaker Studio as the experimentation environment, Amazon SageMaker Pipelines for workflow automation, and Amazon SageMaker Model Registry for model versioning. I addressed the batch transformation scalability concern by creating a Jupyter Notebook demonstration processing 250MB of data using SageMaker Batch Transform with 100MB micro-batches, proving the service could handle arbitrarily large datasets. I introduced pipeline step caching to enable reusing expensive training results and recommended migrating from native SageMaker notebooks to SageMaker Studio for better pipeline visualization and collaboration.

**For productionalization**, I mapped the customer's ML ops maturity using AWS best practices, identified them at the early "Repeatable" phase, and provided a roadmap covering standardized code repositories, automated pipelines, testing/monitoring practices, and multi-account deployment strategies. I suggested two integration patterns for connecting data and ML pipelines: SNS-based event triggering and nested AWS Step Functions orchestration.

## Results

I successfully delivered comprehensive architectural designs addressing all objectives within the two-day Design Lab engagement. The customer team expressed high confidence in the proposed solutions and confirmed they received everything expected. They gained clear direction on using AWS Step Functions for orchestration, Amazon SageMaker Pipelines for ML automation, and a proven approach to scale from their single gender prediction model to hundreds of account-specific models. The engagement concluded with a defined next step: building a POC SageMaker pipeline based on the design, with follow-up SME sessions scheduled for AWS Step Functions and Amazon SageMaker deep dives to accelerate their journey toward repeatable ML operations.
  </markdown_content>
</document>
<document>
  <title>2022-07 to 2022-10 - Mutual Fund Asset Management System Login Log Auditing Data Pipeline</title>
  <markdown_content>
# 2022-07 to 2022-10 - Mutual Fund Asset Management System Login Log Auditing Data Pipeline

> As a Data Architect / Engineer Consultant at Accenture Federal Service, I led the design and implementation of critical data infrastructure for the Federal Retirement Thrift Investment Board's pension asset management system, which serves all federal government employees. My work spanned the entire data engineering lifecycle, encompassing **data architecture**, **ETL pipeline development**, **workflow orchestration**, **data analytics**, and **PySpark job optimization**.

## Overview

Built a login audit pipeline for the Federal Retirement Thrift Investment Board's pension system. Rather than shell out tens of thousands monthly for Informatica's Okta Connector, I wrote a custom crawler from scratch that pulled 10 million login records into S3 for compliance and fraud detection. The crawler ate through 60 days of backlog in a few hours using time-sliced ingestion, then settled into a steady daily rhythm - saving the company serious money every month.

## Background

FRTIB runs retirement accounts for federal employees who authenticate through Okta. The user base skews older - think career government workers who still write passwords on sticky notes. MFA was already mandatory, but that wasn't enough.

The business wanted deeper visibility into login patterns to catch suspicious activity. Our ML team would build the anomaly detection models, but they needed clean data first. That's where I came in.

Informatica sells a prebuilt Okta connector, but the pricing was ridiculous - tens of thousands per month for what's essentially an ETL job. We decided to roll our own instead.

## Personal Note

Three-month project, July through October 2022. I had the most experience with crawlers on the team, so I owned it end-to-end - architecture, implementation, everything. Basically rebuilt Informatica's connector myself.

The hard part? Okta purges logs after 60 days. I had to backfill fast while building a system that wouldn't miss new data. The solution - time-slicing with DynamoDB state tracking - was elegant. The crawler ran 15x faster than real-time (processed 15 minutes of logs per minute), so catching up took hours, not days.

Thing ran flawlessly until I left. Saved a bunch of money and proved we didn't need enterprise software for everything.

## Situation

What we needed to solve:

- **Compliance auditing**: Capture every login attempt with full context - IPs, devices, browsers, the works
- **Feed the ML pipeline**: Data had to be clean and continuous for anomaly detection
- **Security-challenged users**: Lots of older feds who need extra protection from themselves
- **Budget reality**: Can't justify enterprise pricing for a point solution
- **Scale**: 10 million historical records, then ~1GB daily going forward
- **Ticking clock**: 60-day retention window meant we couldn't dawdle
- **Always-on**: Had to run automatically every day without babysitting

## Task

Build our own Okta log scraper. Requirements:

- Extract login events as JSON with all the metadata - IP, device, browser, everything
- Backfill 60 days before Okta deletes it
- Keep running incrementally without gaps or duplicates
- Store it smart so downstream analytics don't crawl
- Do it cheaper than buying software

Being the crawler guy, I designed and built the whole thing.

## Action

### API Analysis and Batch Download Design

Okta's API is pretty straightforward - batch endpoint returns JSON logs with pagination. You specify start/end timestamps and get back everything in that window. Perfect for time-based chunking.

### DynamoDB State Tracker

Used DynamoDB to bookmark progress. Simple table: current start time, current end time. Every successful crawl advances the bookmark. If Lambda crashes or times out, next run picks up exactly where it left off. No manual intervention, no lost data, no reruns.

### Time-Sliced Crawling Strategy

Settled on 15-minute chunks. Why?

- Small enough to process in-memory without blowing up Lambda
- Big enough to be efficient (not hammering the API every 30 seconds)
- Easy to reason about when debugging

Lambda wakes up, checks DynamoDB for the current position, pulls the next 15 minutes, writes to S3, bumps the pointer forward. Rinse and repeat.

### Fast Catch-Up Mechanism

Here's where it gets good. The crawler runs faster than time passes - 15 minutes of data per minute of runtime. So the bookmark races forward. Leave it running overnight and you'll chew through two months of backlog in hours, not weeks.

Once you hit "now", the crawler's caught up. From there it just runs daily to grab yesterday's logs. Never falls behind again.

### Time-Based Partitioning on S3

Partitioned by event timestamp (not crawl time - important distinction) into hourly buckets: `hour=2022070812`, `hour=2022070813`, etc.

Downstream Glue jobs love this. Want logins from 2-4pm on July 8th? Read two folders. No full table scans, no wasted I/O.

### Automated Incremental Updates

Set it and forget it. Lambda on a schedule (EventBridge trigger), wakes up daily, checks DynamoDB, grabs whatever's new, writes it out. Zero maintenance once it's running.

## Result

Shipped a working Okta scraper that replaced Informatica completely. Caught up on 10 million historical records in a few hours, then ran on autopilot daily. Saved tens of thousands per month, basically forever.

Code worked first try - no bugs in production. Partitioned S3 data made analytics fast. The DynamoDB state pattern became our go-to for future scrapers.
  </markdown_content>
</document>
<document>
  <title>2022-01-31 - Build Lab - Foreign exchange services company</title>
  <markdown_content>
# 2022-01-31 - Build Lab - Foreign exchange services company

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

I was assigned to a Canada-based financial technology company specializing in foreign exchange services and international money transfers, serving millions of customers globally. The company used an Entity Resolution (ER) model to create unified customer profiles across multiple products for Financial Crime Risk scoring, but their existing batch processing system had critical performance limitations: a 30-minute cold start time and 1.5-2 seconds to process each transaction. With a ground truth dataset of 125 million customer records stored in an on-premises relational database, the system's requirement to load the entire dataset into memory before each execution created a major bottleneck. The company needed to achieve sub-second response times to support real-time fraud detection and enable one million daily transactions, but faced constraints including sensitive PII data requiring dummy data for the Build Lab and an isolated development AWS account environment.

## Task

As the assigned Data Lab Architect, I was responsible for designing and building a proof-of-concept solution that would dramatically improve ER matching performance from 1.5-2 seconds to under one second while maintaining high accuracy (≥95%) and supporting peak traffic of 70 transactions per second. My task involved the unique Build Lab challenge of working on-site with the customer's data scientists and software developers in an AWS Cloud9 environment, building the complete solution collaboratively while simultaneously teaching them to replicate and deploy it independently. I needed to deliver a production-ready microservice API with monitoring, generate 125 million records of production-like dummy data, develop the matching algorithm, and conduct a migration practice to demonstrate deployment automation—all within four days.

## Action

I designed and implemented an AWS OpenSearch-based Entity Resolution system with comprehensive data pipeline and microservice architecture.

**Data Generation and ETL:** I introduced the Faker Python library to generate 125 million production-like dummy records, then optimized the process using the mpire multiprocessing library to reduce generation time from 21 hours to 1.75 hours across 1,250 CSV files. I implemented AWS DynamoDB with the pynamodb ORM framework to track loading status for each data file, enabling surgical retry logic for failed bulk inserts rather than full repeats.

**Algorithm Innovation:** I developed a novel ER matching methodology that classified data fields into "determinative fields" (lastname, firstname, date of birth for filtering) and "non-determinative fields" (address, city, state for relativity ranking). I implemented compound OpenSearch queries combining term search, fuzzy search with weighted scoring, and built a data preprocessing layer to handle typos, missing values, and non-standard formats before querying. I added a 60-second cache layer to optimize throughput for micro-batch traffic patterns.

**Microservice Architecture:** I built the matching algorithm into an AWS Lambda function integrated with API Gateway for secure REST API access, and provided comprehensive hands-on training covering unit testing, integration testing, and high-concurrency load testing—development practices the customer team had been missing. I configured AWS CloudWatch alarms monitoring API latency and OpenSearch metrics with SNS notifications, and delivered a mini Subject Matter Expert session on CloudWatch methodology.

**Production Readiness:** On day four, I led an impromptu "deployment automation and migration practice" session, creating a deployment runbook and successfully reproducing the entire architecture in a different namespace within three hours, demonstrating production migration readiness. I coordinated with AWS OpenSearch and API Gateway specialists to optimize performance bottlenecks and standardize data formats using the phonenumbers library.

## Results

I successfully delivered a real-time ER matching microservice that achieved 300ms average latency with 97% accuracy (exceeding the ≤1000ms, ≥95% accuracy targets) and supported 100 transactions per second without caching—significantly surpassing the 70 TPS requirement. The customer left the Build Lab with a fully functional, production-ready API and the knowledge to independently deploy it, completing a successful migration practice that gave them confidence to reach production within 2-3 weeks.
  </markdown_content>
</document>
<document>
  <title>2023-02-06 - Build-Lab - data analytics and risk assessment firm</title>
  <markdown_content>
# 2023-02-06 - Build-Lab - data analytics and risk assessment firm

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

A multinational data analytics and risk assessment firm specializing in insurance, natural resources, and financial services faced a critical operational challenge in their insurance claims processing division. The organization received concatenated multi-page PDF documents (100-1,000 pages each) containing medical claims and billing information that frequently contained duplicate pages. Manual review to identify duplicates was nearly impossible at scale, and the business definition of "duplication" was nuanced—two pages might appear 99.99% identical but represent different patients or service dates, requiring deep domain understanding to distinguish. The company needed an automated solution to detect and discard duplicate pages while avoiding false positives that could result in lost critical documentation.

## Task

I was assigned as the Data Lab Architect to lead a four-day Build Lab engagement where I would design, implement, and validate a proof-of-concept document deduplication solution. My responsibility was to build a working prototype that demonstrated technical feasibility, establish performance baselines using F1 score metrics, and train the customer's engineering team to replicate the solution in their AWS environment. The engagement required implementing the complete business logic in code while working on-site with the customer's developers, simultaneously building the solution on my laptop and teaching the implementation approach, all within the constraints of their sandbox environment and limited data access.

## Action

I architected and implemented an end-to-end intelligent document processing pipeline using AWS services and advanced NLP techniques.

**Document Preprocessing Pipeline:** I built a serverless data pipeline using Amazon S3, AWS Lambda, and Amazon Textract that automatically split multi-page PDFs into single-page PNG images, extracted both unstructured text and structured key-value pairs from documents, and organized the processed data for downstream analysis. I implemented MD5-based file naming to prevent overwrites and designed a modular Python codebase with a lightweight configuration management system for easy deployment across AWS accounts.

**Document Similarity Model:** I developed a TF-IDF-based document similarity model using the gensim Python library that programmatically identified similar page pairs with 80%+ similarity scores. This model reduced computational complexity from O(N²) to O(N log N), achieving 50x faster performance and enabling analysis of 421-page documents that returned 821 candidate pairs for review.

**Difference Analyzer:** Inspired by Git's file comparison algorithm, I implemented detailed text sequence analysis using Python's difflib library combined with fuzzy string matching (fuzzywuzzy library) to provide word-by-word comparisons with syntax highlighting. I also built fuzzy matching capabilities for structured key-value data to handle inconsistent field names caused by variable scan quality.

**Duplication Detector:** I created a decision tree classifier that applied business rules based on similarity thresholds and fuzzy match scores to predict duplicates, then validated the model against 300 manually-labeled page pairs. The solution achieved 94.33% accuracy and 97.02% F1 score (compared to a 94% baseline), successfully detecting 98.2% of duplicates with only 12 false positives.

**Human Review Workflow (Stretch Goal):** I deployed an Amazon Augmented AI (A2I) human-in-the-loop workflow using Amazon SageMaker Ground Truth for generating labeled training datasets, creating a private labeling workforce and custom UI template for future model improvements. Throughout the four-day engagement, I taught the customer team AWS development best practices including local Lambda function testing, deterministic dependency management with Python Poetry, and event-driven architecture patterns.

## Results

Successfully delivered a working prototype that exceeded all defined goals and stretch objectives. The solution automatically identified 98.2% of duplicate pages with minimal false positives, dramatically reducing manual review effort for documents containing hundreds of pages. The customer's engineering team gained hands-on experience with the complete codebase and deployment approach, positioning them to integrate the solution into production. The reusable document preprocessing pipeline created additional value for other intelligent document processing use cases across the organization.
  </markdown_content>
</document>
<document>
  <title>2023-03-27 to 2023-12-29 at EDF Energy</title>
  <markdown_content>
# 2023-03-27 to 2023-12-29 at EDF Energy

In March 2023, I secured a contract with **EDF Energy** for the following reasons:

- They are an **AWS customer**, and I am a **Senior AWS Data/AI Architect** (and a former AWS employee).
- Their core **Data Lakehouse** is built on **S3 and Apache Hudi**, with **ETL jobs running on AWS Glue** and **workflows orchestrated through Step Functions**—areas in which I have extensive hands-on experience.
- My previous work at **Earth Networks**, a leading weather and energy big data company, gave me deep domain knowledge of the **energy sector**.

In this project, I served as the **Data Architect and Technical Lead**, leading a team of software engineers in designing the company’s **data lake and data pipelines**, and reported directly to the **Principal Architect**.
  </markdown_content>
</document>
<document>
  <title>2022-05-12 - Design Lab - automotive services and technology provider</title>
  <markdown_content>
# 2022-05-12 - Design Lab - automotive services and technology provider

> AWS Data Lab is an accelerated engineering program where AWS architects partner directly with customers to solve complex data challenges through intensive monthly engagements. Unlike AWS Professional Services which focuses on broader consulting and implementation projects, Data Lab delivers rapid, hands-on proof-of-concepts built collaboratively with customer engineers using their actual data in their AWS environments. Each engagement begins with determining whether to pursue a Design Lab (architectural recommendations and design documents) or a Build Lab (2-5 day intensive hands-on development). The Build Lab is particularly challenging as architects work with completely unknown customer environments and datasets, requiring adaptability and deep technical expertise to validate solutions in real-time. The ultimate goal is to deliver production-ready architectures and prototypes that customers can confidently deploy, accelerating their data modernization initiatives.

I was assigned to the world's largest automotive services and technology provider, a global enterprise with operations across automotive retail, dealer management, and vehicle lifecycle solutions. The customer operated a "Common Customer" system that matched and merged customer demographic records across multiple automotive software platforms to create "golden records" providing unified customer views for dealerships. However, their existing architecture relied on DynamoDB with Global Secondary Indexes, which couldn't support dynamically changing search rules required by different dealer enterprises (such as country-specific matching) and lacked fuzzy search capabilities to handle typos and misspellings in customer data. With approximately two billion customer records and complex multi-tenant data ownership hierarchies spanning dealer groups, individual dealerships, and various software solutions, the system needed architectural redesign to support both real-time and batch matching operations while ensuring strict data isolation when dealership ownership changed.

## Task

As the lead Data Lab Architect for this two-day Design Lab engagement, I was responsible for architecting a scalable, cost-effective solution that would enable flexible "Search," "Match," and "Merge" operations across multiple tiers of golden records (dealership-level, region-level, and enterprise-level) while maintaining data security and compliance. The deliverable was a comprehensive architecture design with detailed data models, OpenSearch indexing strategies, and query patterns that would support both real-time transactional matching and bulk historical data onboarding. I needed to address the critical business requirement that golden records owned by one dealership must never be exposed to another dealership when ownership transfers occur, and provide recommendations for cost optimization given the customer's concerns about managing two billion records in OpenSearch.

## Action

I designed a hybrid architecture leveraging AWS DynamoDB, OpenSearch, Lambda, ECS Fargate, and API Gateway to enable dynamic search patterns with efficient cost structure.

**Architecture Design:** I created a Change Data Capture (CDC) pipeline using DynamoDB Streams to feed customer data into AWS Lambda, which processes events in batches and loads them into AWS OpenSearch for advanced search capabilities including fuzzy matching and full-text search. I deployed the core "Search/Match/Merge" microservice logic on ECS Fargate, which receives requests through API Gateway, executes OpenSearch queries to find matched records, creates golden records based on survivorship rules, and stores results.

**Data Model Innovation:** I invented a novel "Golden Tier" concept using a prefix-tree string data structure to represent hierarchical ownership (e.g., "global/dg(dealer-group)/dealer-id"), enabling constant-time query performance and logical isolation of golden records across different enterprises. The OpenSearch index schema indexed customer fields (firstname, lastname, phone, email, address) as both "keyword" and "text" types to support exact matching and fuzzy search simultaneously, while metadata fields (dealer_group_id, dealer_id, solution_id, golden_tier) provided multi-tenant data isolation and search scope control. I designed the golden record model with a "ccid_list" array field to maintain relationships between golden records and source customer records, enabling efficient bidirectional queries.

**Technical Implementation:** I created Python-based data models using pynamodb for DynamoDB and attrs for OpenSearch, along with comprehensive OpenSearch mapping configurations and sample query patterns demonstrating how to use filter clauses for tenant isolation, must clauses for required field matches, and should clauses for weighted similarity scoring.

**Cost Optimization:** I conducted a preliminary cost analysis demonstrating that the proposed OpenSearch index strategy could reduce monthly costs from $17,000 to approximately $800-$1,000 while managing two billion records, and recommended follow-up activities including load testing, OpenSearch cluster optimization, and SME sessions for Elasticsearch comparison.

## Results

I successfully delivered a complete architectural design that met all business requirements for real-time and batch customer matching across multiple hierarchy levels while ensuring data security during ownership transfers. The proposed "Golden Tier" data structure innovation solved the critical compliance requirement by providing automatic query-time isolation, preventing access to stale golden records even before relationship cleanup completes. The customer team validated the data model design and confirmed it could serve as the foundation for production implementation while remaining extensible for future requirements.
  </markdown_content>
</document>
</knowledge-base>